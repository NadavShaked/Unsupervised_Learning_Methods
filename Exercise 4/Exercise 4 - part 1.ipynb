{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cqdhRTw4CVqa"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 004 - Part I\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 0.1.000 | 11/06/2023 | Royi Avital | First version                                                      |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8lpgX2okCVqf"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0004Part001.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n1qbo58TCVqf"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cDeiW96bCVqg"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
        " - Answer all questions / tasks within the Jupyter Notebook.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Verify the rendering on VS Code.\n",
        " - Submission in groups (Single submission per group).\n",
        " - You may and _should_ use the forums for questions.\n",
        " - Good Luck!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6ixXlF2DCVqg"
      },
      "source": [
        "## Team Members\n",
        "- Nadav_Talmon_203663950\n",
        "- Nadav_Shaked_312494925\n",
        "- Adi_Rosenthal_316550797"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cKXMAuYUCVqh"
      },
      "source": [
        "## 1. Classic Multi Dimensional Scaling (MDS)\n",
        "\n",
        " * Given a function $ \\phi \\left( \\cdot \\right) : \\mathbb{R}^{D} \\to \\mathbb{R}^{M} $.\n",
        " * Consider the following inner product: ${\\left \\langle \\boldsymbol{x}, \\boldsymbol{y} \\right \\rangle}_{\\phi} = \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle$.\n",
        " * Yields the induced norm: $ {\\left\\| \\boldsymbol{x} \\right\\|}_{\\phi} = \\sqrt{ \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{x} \\right) \\right \\rangle } $.\n",
        " * Yields the induced metric: ${d}_{\\phi} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\| \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{\\phi}$.\n",
        "\n",
        "### 1.1. Question\n",
        "\n",
        "Consider the data (Training set) $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ and let $\\boldsymbol{D}_{\\phi} \\left[ i, j \\right] = {d}_{\\phi}^{2} \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$.\n",
        "\n",
        "Show that $- \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$ where:\n",
        "\n",
        " * $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$ - The centering matrix.\n",
        " * $\\boldsymbol{K}_{\\phi} = \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}$ where: $\\boldsymbol{\\Phi} = \\begin{bmatrix} \\mid & \\mid &  & \\mid \\\\ \\phi \\left( \\boldsymbol{x}_{1} \\right) & \\phi \\left( \\boldsymbol{x}_{2} \\right) & \\dots & \\phi \\left( \\boldsymbol{x}_{N} \\right) \\\\ \\mid & \\mid & & \\mid \\end{bmatrix} \\in \\mathbb{R}^{M \\times N} = \\boldsymbol{\\Phi} = \\begin{bmatrix} \\mid & \\mid &  & \\mid \\\\ \\boldsymbol{\\phi}_{1} & \\boldsymbol{\\phi}_{2} & \\dots & \\boldsymbol{\\phi}_{N} \\\\ \\mid & \\mid & & \\mid \\end{bmatrix} \\in \\mathbb{R}^{M \\times N}$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> Hints:\n",
        "    * Show that the transformation $\\phi \\left( \\cdot \\right)$ must be linear: $\\phi \\left( \\alpha \\boldsymbol{x}, \\beta \\boldsymbol{y} \\right) = \\alpha \\phi \\left( \\boldsymbol{x} \\right) + \\beta \\phi \\left( \\boldsymbol{y} \\right)$.  \n",
        "      You may use $\\left \\langle \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y}, \\boldsymbol{z} \\right \\rangle$ as a starting point.\n",
        "    * Show that ${d}_{\\phi}^{2} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) - \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2} = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) \\right\\|}_{2}^{2} - 2 \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle + {\\left\\| \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2}$\n",
        "    * Use the lecture notes to conclude $- \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lhP3wWvKCVqh"
      },
      "source": [
        "### 1.1. Solution\n",
        "\n",
        "Lets calculate each cell in matrix $D_{\\phi}$\n",
        "\n",
        "$D_\\phi[i, j] = d^2_\\phi (x_i, x_j) = ||x_i - x_j||^2_{\\phi} = <\\phi(x_i - x_j), \\phi(x_i - x_j)> = <\\phi(x_i) - \\phi(x_j), \\phi(x_i) - \\phi(x_j)> = \\phi(x_i)^T \\phi(x_i) - \\phi(x_i)^T \\phi(x_j) - \\phi(x_j)^T \\phi(x_i) + \\phi(x_j)^T \\phi(x_j) = \\lVert \\phi(x_i) \\rVert_2^2 - 2 \\phi(x_i)^T \\phi(x_j) + \\lVert\\phi(x_j)\\rVert_2^2$\n",
        "\n",
        "After rows centering of matrix $D_{\\phi}$, we will get\n",
        "\n",
        "$- \\frac{1}{2} D_{\\phi} J [i, j] = - \\frac{1}{2} \\lVert\\phi(x_i)\\rVert_2^2 + \\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\lVert\\phi(x_j)\\rVert_2^2 - \\frac{1}{N} \\Sigma^N_{j = 1} (- \\frac{1}{2} \\lVert\\phi(x_i)\\rVert_2^2 + \\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\phi(x_j\\rVert_2)^2) = - \\frac{1}{2} \\lVert\\phi(x_i\\rVert_2)^2 + \\frac{1}{2} \\phi(x_i)\\rVert_2^2 + \\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\lVert\\phi(x_j)\\rVert_2^2 - \\frac{1}{N} \\Sigma^N_{j= 1} (\\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\lVert\\phi(x_j)\\rVert_2^2) = \\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\lVert\\phi(x_j)\\rVert_2^2 - \\frac{1}{N} \\Sigma^N_{j= 1} (\\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\lVert\\phi(x_j)\\rVert_2^2)$\n",
        "\n",
        "And then after columns centering of matrix $- \\frac{1}{2} D_{\\phi} J$, we will get\n",
        "\n",
        "$- \\frac{1}{2} J D_{\\phi} J [i, j] = \\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\lVert\\phi(x_j)\\rVert_2^2 - \\frac{1}{N} \\Sigma^N_{j = 1}(\\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\lVert\\phi(x_j)^2) - \\frac{1}{N} \\Sigma^N_{i = 1}(\\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\lVert\\phi(x_j)\\rVert_2^2 - \\frac{1}{N} \\Sigma^N_{j = 1}(\\phi(x_i)^T \\phi(x_j) - \\frac{1}{2} \\lVert\\phi(x_j)\\rVert_2^2))$\n",
        "\n",
        "$= \\phi(x_i)^T \\phi(x_j) - \\frac{1}{N} \\Sigma^N_{j = 1}(\\phi(x_i)^T \\phi(x_j)) + \\frac{1}{2 \\cdot N} \\Sigma^N_{j = 1}(\\lVert\\phi(x_j)\\rVert_2^2) - \\frac{1}{N} \\Sigma^N_{i = 1}(\\phi(x_i)^T \\phi(x_j)) + \\frac{1}{N^2} \\Sigma^N_{i, j = 1}(\\phi(x_i)^T \\phi(x_j)) - \\frac{1}{2 \\cdot N} \\Sigma^N_{j = 1}(\\lVert\\phi(x_j)\\rVert_2^2)$\n",
        "\n",
        "$= \\phi(x_i)^T \\phi(x_j) - \\frac{1}{N} \\Sigma^N_{j = 1}(\\phi(x_i)^T \\phi(x_j)) - \\frac{1}{N} \\Sigma^N_{i = 1}(\\phi(x_i)^T \\phi(x_j)) + \\frac{1}{N^2} \\Sigma^N_{i, j = 1}(\\phi(x_i)^T \\phi(x_j))$\n",
        "\n",
        "Now lets calculate each cell in the matrix $J K_{\\phi} J [i, j]$\n",
        "\n",
        "$J K_{\\phi} J [i, j] = \\phi(x_i)^T \\phi(x_j) - \\frac{1}{N} \\Sigma^N_{j = 1}(\\phi(x_i)^T \\phi(x_j)) - \\frac{1}{N} \\Sigma^N_{i = 1}(\\phi(x_i)^T \\phi(x_j) - \\frac{1}{N} \\Sigma^N_{j = 1}(\\phi(x_j)^T \\phi(x_i)))$\n",
        "\n",
        "$= \\phi(x_i)^T \\phi(x_j) - \\frac{1}{N} \\Sigma^N_{j = 1}(\\phi(x_i)^T \\phi(x_j)) - \\frac{1}{N} \\Sigma^N_{i = 1}(\\phi(x_i)^T \\phi(x_j)) + \\frac{1}{N^2} \\Sigma^N_{i, j = 1}(\\phi(x_i)^T \\phi(x_j))$\n",
        "\n",
        "Every cell in the matrices $- \\frac{1}{2} J D J$ and $J K_{\\phi} J$ are equals,\n",
        "\n",
        "thus the equations $- \\frac{1}{2} J D J$ and $J K_{\\phi} J$ are equivalent.\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J3YQpnL4CVqi"
      },
      "source": [
        "Consider the data (Training set) $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ and let $\\boldsymbol{D} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2}$.\n",
        "\n",
        "### 1.2. Question\n",
        "\n",
        "Show that $\\boldsymbol{v}^{T} \\boldsymbol{D} \\boldsymbol{v} \\leq 0$ for any $\\boldsymbol{v}$ such that $\\left \\langle \\boldsymbol{v}, \\boldsymbol{1} \\right \\rangle = 0$.  \n",
        "What does it imply on _distance matrices_?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FqdwTreJCVqi"
      },
      "source": [
        "### 1.2. Solution\n",
        "\n",
        "Notice that by definition\n",
        "\n",
        "$\\forall i, j, D[i, j] \\ge 0$\n",
        "\n",
        "$<v, 1> = \\Sigma^N_{i = 1} v_i = 0$\n",
        "\n",
        "$v^T D v = \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i D_{i j} v_j = \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j (x_i - x_j)^T (x_i - x_j) = \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} (x_i^T x_i - x_i^T x_j - x_j^T x_i + x_j^T x_j) = \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_i^T x_i - \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_i^T x_j - \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_j^T x_i + \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_j^T x_j$\n",
        "\n",
        "Notice that\n",
        "\n",
        "$\\Sigma^N_{i = 1} v_i  x_i^T x_i \\Sigma^N_{j = 1} v_j = \\Sigma^N_{i = 1} v_i  x_i^T x_i \\cdot 0 = 0$\n",
        "\n",
        "$\\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_j^T x_j = \\Sigma^N_{i = 1} v_i \\Sigma^N_{j = 1} v_j x_j^T x_j = (\\Sigma^N_{i = 1} v_i) \\cdot (\\Sigma^N_{j = 1} v_j x_j^T x_j) = 0 \\cdot (\\Sigma^N_{j = 1} v_j x_j^T x_j) = 0$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$\\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_i^T x_i - \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_i^T x_j - \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_j^T x_i + \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_j^T x_j = - \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_i^T x_j - \\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_j^T x_i = -2 \\cdot (\\Sigma^N_{i = 1} \\Sigma^N_{j = 1} v_i v_j x_i^T x_j) = -2 \\cdot (\\Sigma^N_{i = 1} v_i x_i^T (\\Sigma^N_{j = 1} v_j x_j))$\n",
        "\n",
        "Denote $u = \\Sigma^N_{i = 1} v_i x_i$\n",
        "\n",
        "$v_i$ is scalar, thus\n",
        "\n",
        "$-2 \\cdot (\\Sigma^N_{i = 1} v_i x_i^T (\\Sigma^N_{j = 1} v_j x_j)) = -2 \\cdot (\\Sigma^N_{i = 1} v_i^T x_i^T (\\Sigma^N_{j = 1} v_j x_j)) = -2 \\cdot (\\Sigma^N_{i = 1} (v_i x_i)^T (\\Sigma^N_{j = 1} v_j x_j)) = -2 \\cdot u^T u$\n",
        "\n",
        "Notice that $u^T u \\ge 0$, so\n",
        "\n",
        "$-2 \\cdot u^T u \\le 0$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$\\forall v s.t. <v, 1> = 0$,\n",
        "\n",
        "$v^T D v \\le 0$\n",
        "\n",
        "it is imply on the distance matrices that they are not PSD, so the problem is not convex\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe35MCpXCVqj"
      },
      "source": [
        "## 3. Metric Multi Dimensional Scaling (MDS)\n",
        "\n",
        "The metric MDS objective is given by:\n",
        "\n",
        "$$ \\arg \\min_{\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}} {\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} $$\n",
        "\n",
        "Where:\n",
        "\n",
        " * $\\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] = d \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ - The given distance matrix.\n",
        " * $\\boldsymbol{D}_{z} = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}$.\n",
        "\n",
        "Consider the surrogate function:\n",
        "\n",
        "$$ g \\left( \\boldsymbol{Z}, \\tilde{\\boldsymbol{Z}} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} + 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right) - 4 \\left \\langle \\boldsymbol{Z}^{T} \\tilde{\\boldsymbol{Z}}, \\boldsymbol{B} \\right \\rangle $$\n",
        "\n",
        "Where:\n",
        "\n",
        " * $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$ - The centering matrix.\n",
        " * $\\tilde{\\boldsymbol{D}}_{\\tilde{z}} \\left[ i, j \\right] = {\\left\\| \\tilde{\\boldsymbol{z}}_{i} - \\tilde{\\boldsymbol{z}}_{j} \\right\\|}_{2}$.\n",
        " * $\\boldsymbol{C} \\left[ i, j \\right] = \\begin{cases} 0 & \\text{ if } i = j \\\\ - \\frac{ \\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] }{ \\tilde{\\boldsymbol{D}}_{z} \\left[ i, j \\right] } & \\text{ if } i \\neq j \\end{cases}$.\n",
        " * $\\boldsymbol{B} = \\boldsymbol{C} - \\operatorname{Diag} \\left( \\boldsymbol{C} \\boldsymbol{1} \\right)$.\n",
        "\n",
        "### 3.1. Question\n",
        "\n",
        "Prove that $ \\boldsymbol{B} \\boldsymbol{J} = \\boldsymbol{B} $."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "co7hbHNxCVqj"
      },
      "source": [
        "### 3.1. Solution\n",
        "\n",
        "$B J = [C - Diag(C \\boldsymbol{1})] [I - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^T] = C - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^T - Diag(C \\boldsymbol{1}) + \\frac{1}{N} Diag(C \\boldsymbol{1}) \\boldsymbol{1} \\boldsymbol{1}^T$\n",
        "\n",
        "What we need to show it that $C \\boldsymbol{1} \\boldsymbol{1}^T = Diag(C \\boldsymbol{1}) \\boldsymbol{1} \\boldsymbol{1}^T$\n",
        "\n",
        "$$\n",
        "C =\n",
        "\\begin{bmatrix}\n",
        "0 & ... & - \\frac{\\Delta_x[1, N]}{\\widetilde{D}_z[1, N]}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "- \\frac{\\Delta_x[N, 1]}{\\widetilde{D}_z[N, 1]} & ... & 0\\\\\n",
        "\\end{bmatrix}_{N x N}\n",
        "\\quad\n",
        "\\boldsymbol{1} \\boldsymbol{1}^T =\n",
        "\\begin{bmatrix}\n",
        "1 & ... & 1\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "1 & ... & 1\\\\\n",
        "\\end{bmatrix}_{N x N}\n",
        "$$\n",
        "\n",
        "$$\n",
        "C \\boldsymbol{1} \\boldsymbol{1}^T =\n",
        "\\begin{bmatrix}\n",
        "\\Sigma^N_{j \\ne 1} - \\frac{\\Delta_x[1, j]}{\\widetilde{D}_z[1, j]} & ... & \\Sigma^N_{j \\ne 1} - \\frac{\\Delta_x[1, j]}{\\widetilde{D}_z[1, j]}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\Sigma^N_{j \\ne N} - \\frac{\\Delta_x[N, j]}{\\widetilde{D}_z[N, j]} & ... & \\Sigma^N_{j \\ne N} - \\frac{\\Delta_x[N, j]}{\\widetilde{D}_z[N, j]}\\\\\n",
        "\\end{bmatrix}_{N x N}\n",
        "$$\n",
        "\n",
        "$$\n",
        "C \\boldsymbol{1} = [\\Sigma^N_{j \\ne 1} - \\frac{\\Delta_x[1, j]}{\\widetilde{D}_z[1, j]}, ..., \\Sigma^N_{j \\ne 1} - \\frac{\\Delta_x[N, j]}{\\widetilde{D}_z[N, j]}]\n",
        "$$\n",
        "\n",
        "$$\n",
        "Diag(C \\boldsymbol{1}) =\n",
        "\\begin{bmatrix}\n",
        "\\Sigma^N_{j \\ne 1} - \\frac{\\Delta_x[1, j]}{\\widetilde{D}_z[1, j]} & ... & 0\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "0 & ... & \\Sigma^N_{j \\ne N} - \\frac{\\Delta_x[N, j]}{\\widetilde{D}_z[N, j]}\\\\\n",
        "\\end{bmatrix}_{N x N}\n",
        "$$\n",
        "\n",
        "$$\n",
        "Diag(C \\boldsymbol{1}) \\boldsymbol{1} \\boldsymbol{1}^T =\n",
        "\\begin{bmatrix}\n",
        "\\Sigma^N_{j \\ne 1} - \\frac{\\Delta_x[1, j]}{\\widetilde{D}_z[1, j]} & ... & \\Sigma^N_{j \\ne 1} - \\frac{\\Delta_x[1, j]}{\\widetilde{D}_z[1, j]}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\Sigma^N_{j \\ne N} - \\frac{\\Delta_x[N, j]}{\\widetilde{D}_z[N, j]} & ... & \\Sigma^N_{j \\ne N} - \\frac{\\Delta_x[N, j]}{\\widetilde{D}_z[N, j]}\\\\\n",
        "\\end{bmatrix}_{N x N}\n",
        "$$\n",
        "\n",
        "Thus, $Diag(C \\boldsymbol{1}) \\boldsymbol{1} \\boldsymbol{1}^T = c \\boldsymbol{1} \\boldsymbol{1}^T$\n",
        "\n",
        "$C - \\frac{1}{N} C \\boldsymbol{1} \\boldsymbol{1}^T - Diag(C \\boldsymbol{1}) + \\frac{1}{N} Diag(C \\boldsymbol{1}) \\boldsymbol{1} \\boldsymbol{1}^T = C - Diag(C \\boldsymbol{1}) = B$\n",
        "\n",
        "Therefore $B J = B$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgzlGLuCVqk"
      },
      "source": [
        "### 3.2. Question\n",
        "\n",
        "Show that $g \\left( \\boldsymbol{Z}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$.\n",
        "\n",
        "\n",
        " * <font color='brown'>(**#**)</font> Hints (See _lecture notes_):\n",
        "     * ${\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} - 2 \\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle + {\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$.\n",
        "     * ${\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right)$.\n",
        "     * $\\boldsymbol{D}^{\\circ 2}_{z} \\left[ i, j \\right] = \\boldsymbol{p} \\boldsymbol{1}^{T} - 2 \\boldsymbol{Z}^{T} \\boldsymbol{Z} + 1 \\boldsymbol{p}^{T}, \\; \\boldsymbol{p} = \\begin{bmatrix} {\\left\\| \\boldsymbol{z}_{1} \\right\\|}_{2}^{2} \\\\ {\\left\\| \\boldsymbol{z}_{2} \\right\\|}_{2}^{2} \\\\ \\vdots {\\left\\| \\boldsymbol{z}_{N} \\right\\|}_{2}^{2} \\end{bmatrix}$.\n",
        "     * For $\\tilde{\\boldsymbol{Z}} = \\boldsymbol{Z}$ we have $\\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle = - \\left \\langle \\boldsymbol{C}, \\boldsymbol{D}^{\\circ 2}_{z} \\right \\rangle$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ebEREY-GCVqk"
      },
      "source": [
        "### 3.2. Solution\n",
        "\n",
        "Lets denote\n",
        "\n",
        "$B = C - Diag(C \\boldsymbol{1})$\n",
        "\n",
        "$g(Z, Z) = ||\\Delta_X||^2_F + 2 \\cdot N Tr(Z J Z^T) - 4 \\cdot <Z^T Z, B>$\n",
        "\n",
        "$||\\Delta_X - D_Z||^2_F = ||\\Delta_X||^2_F + ||D_Z||^2_F - 2 \\cdot <\\Delta_X, D_Z> = ||\\Delta_X||^2_F + N Tr(Z J Z^T) + 2 \\cdot <C, D^{\\circ 2}_Z>$\n",
        "\n",
        "Thus to show equality of $g(Z, Z) = ||\\Delta_X - D_Z||^2_F$ its enough to show that\n",
        "\n",
        "$- 4 \\cdot <Z^T Z, B> = 2 \\cdot <C, D^{\\circ 2}_Z>$\n",
        "\n",
        "$- 2 <Z^T Z, B> = <C, D^{\\circ 2}_Z>$\n",
        "\n",
        "Lets evalute each of the equations\n",
        "\n",
        "The first equation\n",
        "\n",
        "$- 2 \\cdot <Z^T Z, B> = - 2 \\cdot <Z^T Z, C - Diag(C \\boldsymbol{1})> = - 2 \\cdot <Z^T Z, C> + 2 \\cdot <Z^T Z, Diag(C \\boldsymbol{1})> = - 2 \\cdot <Z^T Z, C> + 2 \\cdot <diag(Z^T Z), C \\boldsymbol{1}>$\n",
        "\n",
        "And the second equation\n",
        "\n",
        "$<C, D^{\\circ 2}_Z> = <C, P - 2 Z^T Z + P^T> = <C, P> - 2 \\cdot <C, Z^T Z> + <C, P^T>$\n",
        "\n",
        "$C$ is symmetric so $C = C^T$, thus\n",
        "\n",
        "$<C, P> - 2 \\cdot <C, Z^T Z> + <C, P^T> = <C, P> - 2 \\cdot <C, Z^T Z> + <C^T, P> = <C, P> - 2 \\cdot <C, Z^T Z> + <C, P> = 2 \\cdot <C, P> - 2 \\cdot <C, Z^T Z> = - 2 \\cdot <C, Z^T Z> + 2 \\cdot <C, P>$\n",
        "\n",
        "Thus to show equality of $- 2 <Z^T Z, B> = <C, D^{\\circ 2}_Z>$ its enough to show that\n",
        "\n",
        "$2 \\cdot <diag(Z^T Z), C \\boldsymbol{1}> = 2 \\cdot <C, P>$\n",
        "\n",
        "$<diag(Z^T Z), C \\boldsymbol{1}> = <C, P>$\n",
        "\n",
        "The second equation\n",
        "\n",
        "$<C, P> = <C, p \\boldsymbol{1}^T> = <C \\boldsymbol{1}, p>$\n",
        "\n",
        "So if we will show that $p = Diag(Z^T Z)$ we prove that the equations are equals\n",
        "\n",
        "Lets take a look on $Z^T Z$\n",
        "\n",
        "$\\forall i, j | Z^T Z [i, j] = z_i^T z_j$\n",
        "\n",
        "So\n",
        "\n",
        "$\\forall i | Z^T Z [i, i] = z_i^T z_i = ||z_i||^2_2$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$Diag(Z^T Z) = (||z_1||^2_2, ||z_2||^2_2, ..., ||z_N||^2_2) = p$\n",
        "\n",
        "We show that\n",
        "\n",
        "$Diag(Z^T Z) = p$\n",
        "\n",
        "So\n",
        "\n",
        "$<diag(Z^T Z), C \\boldsymbol{1}> = <C, P>$\n",
        "\n",
        "So\n",
        "\n",
        "$- 2 <Z^T Z, B> = <C, D^{\\circ 2}_Z>$\n",
        "\n",
        "And\n",
        "\n",
        "$- 2 <Z^T Z, B> = <C, D^{\\circ 2}_Z>$\n",
        "\n",
        "So enentually\n",
        "\n",
        "$g(Z, Z) = ||\\Delta_X - D_Z||^2_F$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QQn_pxEtCVql"
      },
      "source": [
        "## 4. IsoMap\n",
        "\n",
        "Let $G = \\left( V, E, W \\right) $ be a simple, undirected and weighted graph with no negative weights / edges.  \n",
        "Let $\\boldsymbol{D} \\in \\mathbb{R}^{N \\times N}$ be the shortest path distance matrix where $ \\left| V \\right| = N$.\n",
        "\n",
        "\n",
        "### 4.1. Question\n",
        "\n",
        "Prove or disprove: There is an embedding ${\\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{d} \\right\\}}_{i = 1}^{N}$ for some $d \\in \\mathbb{N}$ such that:\n",
        "\n",
        "$$ \\forall i, j \\; \\boldsymbol{D} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} $$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ejmUwUTfCVql"
      },
      "source": [
        "### 4.1. Solution\n",
        "\n",
        "Disproof:\n",
        "\n",
        "For the graph $G$ below:        <br>\n",
        "&emsp;&emsp;&emsp;&emsp;$B_2$   <br>\n",
        "&emsp;&emsp;&emsp;&emsp;|       <br>\n",
        "&emsp;&emsp;&emsp;&emsp;|       <br>\n",
        "$\\,\\,\\,\\,\\,B_1$ ---  $A$  --- $B_3$       <br>\n",
        "&emsp;&emsp;&emsp;&emsp;|       <br>\n",
        "&emsp;&emsp;&emsp;&emsp;|       <br>\n",
        "&emsp;&emsp;&emsp;&emsp;$B_4$   <br>\n",
        "\n",
        "Lets set the weight for each edge:  <br>\n",
        "$W[A, B_1] = 1$&emsp;&emsp;$W[A, B_2] = 1$&emsp;&emsp;$W[A, B_3] = 1$&emsp;&emsp;$W[A, B_4] = 1$\n",
        "\n",
        "And the distance matrix $D$ is:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "& A & B_1 & B_2 & B_3 & B_4\\\\\n",
        "A & 0 & 1&  1 & 1 & 1\\\\\n",
        "B_1 & 1 & 0 & 2 & 2 & 2\\\\\n",
        "B_2 & 1 & 2 & 0 & 2 & 2\\\\\n",
        "B_3 & 1 & 2 & 2 & 0 & 2\\\\\n",
        "B_4 & 1 & 2 & 2 & 2 & 0\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "For $d \\in \\mathbb{R}$ assume in contradict that there is exist $z_{B_1}, z_{B_2}, z_{B_3}, z_{B_4}, z_{A}$ embedded samples\n",
        "\n",
        "such that the following constraints are satisfied\n",
        "\n",
        "$\\forall i, j \\in {1, 2, 3, 4} \\quad D[i, j] = ||z_{B_i} - z_{B_j}||_2 = 2$\n",
        "\n",
        "$\\forall i \\in {1, 2, 3, 4} \\quad \\quad D[1, i] = ||z_{A} - z_{B_i}||_2 = 1$\n",
        "\n",
        "By the triangle equation and because $||z_{B_3} - z_{B_1}||_2 = ||z_{B_3} - z_{A}||_2 - ||z_{B_1} - z_{A}||_2$, the point $z_{A}$ is the middle between $z_{B_1}$ and $z_{B_3}$.\n",
        "\n",
        "In addition, by the triangle equation and because $||z_{B_2} - z_{B_1}||_2 = ||z_{B_2} - z_{A}||_2 - ||z_{B_1} - z_{A}||_2$, the point $z_{A}$ is the middle between $z_{B_1}$ and $z_{B_2}$.\n",
        "\n",
        "By that $z_{A}$ is the middle of the couples $z_{B_1}$ and $z_{B_2}$, $z_{B_1}$ and $z_{B_3}$.\n",
        "\n",
        "We show that $z_{B_2} = z_{B_3}$ so $||z_{B_2} - z_{B_3}||_2 = 0$ in contradiction to the SP matrix D, specifically $D[2, 3] = ||z_{B_2} - z_{B_3}||_2 = 2$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5e_5KN6aCVql"
      },
      "source": [
        "### 4.2. Question\n",
        "\n",
        " * Let $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ be the training set.\n",
        " * Let $\\mathcal{Z} = \\left\\{ \\boldsymbol{z}_{i} \\right\\}_{i = 1}^{N}$ be the representation obtained by IsoMap (Encoded data).\n",
        " * Consider a new point $\\boldsymbol{x}^{\\ast}$ where $\\boldsymbol{x}^{\\ast} = \\boldsymbol{x}_{k}, \\; k \\in \\left\\{ 1, 2, \\ldots, N \\right\\}$.\n",
        " * Let $\\boldsymbol{z}^{\\ast}$ be the out of sample encoding applied to $\\boldsymbol{x}^{\\ast}$.\n",
        "\n",
        "Prove or disprove: $\\boldsymbol{z}^{\\ast} = \\boldsymbol{z}_{k}$.\n",
        "\n",
        " * <font color='brown'>(**#**)</font> The out of sample encoding is as shown in _lecture notes_.\n",
        " * <font color='brown'>(**#**)</font> The question is basically if the out of sample extension is equivalent to having the point in the training set for such case.\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p2WYHzWkCVql"
      },
      "source": [
        "### 4.2. Solution\n",
        "\n",
        "It is true, proof:\n",
        "\n",
        "Notice that\n",
        "\n",
        "$\\stackrel{\\sim}{K}_{XX} = Z^T Z = V_d \\Sigma_d^2 V_d^T$\n",
        "<br><br>\n",
        "So\n",
        "\n",
        "$Z^T = V_d \\Sigma_d^T = V_d \\Sigma_d$\n",
        "<br><br>\n",
        "Since $x^*$ is the k data point from the sample set, we know that\n",
        "\n",
        "$\\stackrel{\\sim}{K}_{Xx^*} = Z^T z_k $\n",
        "<br><br>\n",
        "As we learned in class\n",
        "\n",
        "$z^* = \\Sigma_d^{-1} V_d^T \\stackrel{\\sim}{K}_{Xx^*} = \\Sigma_d^{-1} V_d^T Z^T z_k = \\Sigma_d^{-1} \\overbrace{V_d^T V_d}^{I_d} \\Sigma_d z_k = \\overbrace{\\Sigma_d^{-1} \\Sigma_d}^{I_d} z_k = z_k$\n",
        "\n",
        "Thus\n",
        "<br><br>\n",
        "$z^* = z_k$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rM8sLW7PCVql"
      },
      "source": [
        "## 5. Laplacian Eigenmaps\n",
        "\n",
        " * Let $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$.\n",
        " * Let $G = \\left( V, E, W \\right)$ be a weighted graph with with $V = \\mathcal{X}$.\n",
        " * Define $\\boldsymbol{W} \\left[ i, j \\right] = \\begin{cases} \\exp \\left( - \\frac{ {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} }{2 {\\sigma}^{2}} \\right) & \\text{ if } \\boldsymbol{x}_{i} \\in \\mathcal{N}_{j} \\text{ or } \\boldsymbol{x}_{j} \\in \\mathcal{N}_{i} \\\\ 0 & \\text{ else } \\end{cases}$.\n",
        " * Then ${e}_{ij} \\in E$ if $\\boldsymbol{W} \\left[ i, j \\right] \\neq 0$.\n",
        " * The _Graph Laplacian_ $\\boldsymbol{L} = \\boldsymbol{D} - \\boldsymbol{W}$.\n",
        " * The _Degree Matrix_ $\\boldsymbol{D} = \\operatorname{Diag} \\left( \\boldsymbol{W} \\boldsymbol{1} \\right)$.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_CVimVP4CVql"
      },
      "source": [
        "Assume that $G$ has 2 connected components, that is $V = {V}_{1} \\cup {V}_{2}$ such that $\\left\\{ {e}_{ij} \\mid i \\in {V}_{1}, j \\in {V}_{2} \\right\\} = \\emptyset$.\n",
        "\n",
        "### 5.2. Question\n",
        "\n",
        "Show that the _Graph Laplacian_ $\\boldsymbol{L}$ has two **orthogonal** eigenvectors corresponding to the zero eigenvalue.  \n",
        "That is, there are $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\in \\mathbb{R}^{N}$ such that:\n",
        "\n",
        " * $\\boldsymbol{L} \\boldsymbol{u}_{1} = \\boldsymbol{L} \\boldsymbol{u}_{2} = \\boldsymbol{0}$.\n",
        " * $\\left \\langle \\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\right \\rangle = 0$.\n",
        "\n",
        "Explain the meaning of the result, specifically address:\n",
        "\n",
        " * How to do spectral clustering in such case.\n",
        " * How to do dimensionality reduction in such case."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "si6pjH_xCVqm"
      },
      "source": [
        "### 5.2. Solution\n",
        "\n",
        "For graph $G$ with exactly 2 independent components\n",
        "\n",
        "denote the graph vertices\n",
        "\n",
        "$\\forall i \\quad v_{1, i} \\in V_1 \\quad$ and $|V_1| = N$\n",
        "\n",
        "$\\forall j \\quad v_{2, j} \\in V_2 \\quad$ and $|V_2| = M$\n",
        "\n",
        "lets denote the weight matrix\n",
        "\n",
        "$$\n",
        "W =\n",
        "\\begin{bmatrix}\n",
        "& v_{1, 1} & v_{1, 2} & ... & v_{1, N} & v_{2, 1} & ... & v_{2, M - 1} & v_{2, M}\\\\\n",
        "v_{1, 1} & 0 & W[v_{1, 1}, v_{1, 2}] & ... & W[v_{1, 1}, v_{1, N}] & 0 & ... & 0 & 0\\\\\n",
        "v_{1, 2} & W[v_{1, 2}, v_{1, 1}] & 0 & ... & W[v_{1, 2}, v_{1, N}] & 0 & ... & 0 & 0\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots &\\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
        "v_{1, N} & W[v_{1, N}, v_{1, 1}] & W[v_{1, N}, v_{1, 2}] & ... & 0 & 0 & ... & 0 & 0\\\\\n",
        "v_{2, 1} & 0 & 0 & ... & 0 & 0 & ... & W[v_{2, 1}, v_{2, M - 1}] & W[v_{2, 1}, v_{2, M}]\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
        "v_{2, M - 1} & 0 & 0 & ... & 0 & W[v_{2, 1}, v_{2, M - 1}] & ... & 0 & W[v_{2, M - 1}, v_{2, M}]\\\\\n",
        "v_{2, M} & 0 & 0 & ... & 0 & W[v_{2, 1}, v_{2, M}] & ... & W[v_{2, M - 1}, v_{2, M}] & 0\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Lets calculate $D$\n",
        "\n",
        "Every value outside of the diagonal is $0$.\n",
        "\n",
        "Denote every value in the upper left block ($V_1$ vertices)\n",
        "\n",
        "$X_{1, i} = \\Sigma_{v_j \\in V_1} W[v_{1, i}, v_j]$\n",
        "\n",
        "Denote every value in the bottom right block ($V_2$ vertices)\n",
        "\n",
        "$Y_{2, i} = \\Sigma_{v_j \\in V_2} W[v_{2, i}, v_j]$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$\n",
        "D =\n",
        "\\begin{bmatrix}\n",
        "& v_{1, 1} & v_{1, 2} & ... & v_{1, N} & v_{2, 1} & ... & v_{2, M - 1} & v_{2, M}\\\\\n",
        "v_{1, 1} & X_{1, 1} & 0 & ... & 0 & 0 & ... & 0 & 0\\\\\n",
        "v_{1, 2} & 0 & X_{1, 2} & ... & 0 & 0 & ... & 0 & 0\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots &\\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
        "v_{1, N} & 0 & 0 & ... & X_{1, N} & 0 & ... & 0 & 0\\\\\n",
        "v_{2, 1} & 0 & 0 & ... & 0 & Y_{1, 1} & ... & 0 & 0\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
        "v_{2, M - 1} & 0 & 0 & ... & 0 & 0 & ... & Y_{1, M - 1} & 0\\\\\n",
        "v_{2, M} & 0 & 0 & ... & 0 & 0 & ... & 0 & Y_{1, M}\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "So $L = D - W$, lets write $L$\n",
        "\n",
        "$$\n",
        "L =\n",
        "\\begin{bmatrix}\n",
        "& v_{1, 1} & v_{1, 2} & ... & v_{1, N} & v_{2, 1} & ... & v_{2, M - 1} & v_{2, M}\\\\\n",
        "v_{1, 1} & X_{1, 1} & - W[v_{1, 1}, v_{1, 2}] & ... & - W[v_{1, 1}, v_{1, N}] & 0 & ... & 0 & 0\\\\\n",
        "v_{1, 2} & - W[v_{1, 2}, v_{1, 1}] & X_{1, 2} & ... & - W[v_{1, 2}, v_{1, N}] & 0 & ... & 0 & 0\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots &\\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
        "v_{1, N} & - W[v_{1, N}, v_{1, 1}] & - W[v_{1, N}, v_{1, 2}] & ... & X_{1, N} & 0 & ... & 0 & 0\\\\\n",
        "v_{2, 1} & 0 & 0 & ... & 0 & Y_{2, 1} & ... & - W[v_{2, 1}, v_{2, M - 1}] & - W[v_{2, 1}, v_{2, M}]\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
        "v_{2, M - 1} & 0 & 0 & ... & 0 & - W[v_{2, 1}, v_{2, M - 1}] & ... & Y_{2, M - 1} & - W[v_{2, M - 1}, v_{2, M}]\\\\\n",
        "v_{2, M} & 0 & 0 & ... & 0 & - W[v_{2, 1}, v_{2, M}] & ... & - W[v_{2, M - 1}, v_{2, M}] & Y_{2, M}\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Lets denote,\n",
        "\n",
        "$u_1 = (\\overbrace{1, ..., 1}^{V_1}, \\overbrace{0, ...,0}^{V2})$\n",
        "\n",
        "$u_2 = (\\overbrace{0, ..., 0}^{V_1}, \\overbrace{1, ...,1}^{V2})$\n",
        "\n",
        "So for $L u_1$ and $L u_2$ we will get\n",
        "\n",
        "$$\n",
        "L u_1 =\n",
        "\\begin{bmatrix}\n",
        "X_{1, 1} - \\Sigma_{v_j \\in V_1} W[v_{1, 1}, v_j]\\\\\n",
        "\\vdots\\\\\n",
        "X_{1, N} - \\Sigma_{v_j \\in V_1} W[v_{1, N}, v_j]\\\\\n",
        "0\\\\\n",
        "\\vdots\\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        " =\n",
        "\\begin{bmatrix}\n",
        "\\Sigma_{v_j \\in V_1} W[v_{1, 1}, v_j] - \\Sigma_{v_j \\in V_1} W[v_{1, 1}, v_j]\\\\\n",
        "\\vdots\\\\\n",
        "\\Sigma_{v_j \\in V_1} W[v_{1, N}, v_j] - \\Sigma_{v_j \\in V_1} W[v_{1, N}, v_j]\\\\\n",
        "0\\\\\n",
        "\\vdots\\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        " =\n",
        "\\boldsymbol{0}\n",
        "$$\n",
        "\n",
        "$$\n",
        "L u_2 =\n",
        "\\begin{bmatrix}\n",
        "0\\\\\n",
        "\\vdots\\\\\n",
        "0\\\\\n",
        "Y_{2, 1} - \\Sigma_{v_j \\in V_2} W[v_{2, 1}, v_j]\\\\\n",
        "\\vdots\\\\\n",
        "Y_{2, N} - \\Sigma_{v_j \\in V_2} W[v_{2, N}, v_j]\n",
        "\\end{bmatrix}\n",
        " =\n",
        "\\begin{bmatrix}\n",
        "0\\\\\n",
        "\\vdots\\\\\n",
        "0\\\\\n",
        "\\Sigma_{v_j \\in V_1} W[v_{2, 1}, v_j] - \\Sigma_{v_j \\in V_1} W[v_{2, 1}, v_j]\\\\\n",
        "\\vdots\\\\\n",
        "\\Sigma_{v_j \\in V_1} W[v_{2, N}, v_j] - \\Sigma_{v_j \\in V_1} W[v_{2, N}, v_j]\n",
        "\\end{bmatrix}\n",
        " =\n",
        "\\boldsymbol{0}\n",
        "$$\n",
        "\n",
        "Lets show that $<u_1, u_2> = 0$\n",
        "\n",
        "$<u_1, u_2> = <(\\overbrace{1, ..., 1}^{V_1}, \\overbrace{0, ...,0}^{V2}), (\\overbrace{0, ..., 0}^{V_1}, \\overbrace{1, ...,1}^{V2})> = 0$\n",
        "\n",
        "Thus there are $u_1, u_2 \\in \\mathbb{R}$ such that\n",
        "\n",
        "$L u_1 = 0$\n",
        "\n",
        "$L u_2 = 0$\n",
        "\n",
        "$<u_1, u_2> = 0$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U447cnZp1oHf"
      },
      "source": [
        "Explain the meaning of the result, specifically address:\n",
        "\n",
        " * How to do spectral clustering in such case.\n",
        " * How to do dimensionality reduction in such case.\n",
        "<br><br>\n",
        "\n",
        "For both question if we requiered to split the original graph to two components, we dont have to use any algorithm, we can assume that each non-zero block is a component and return them as components.\n",
        "<br><br>\n",
        "\n",
        "For more than to components\n",
        "\n",
        "### How to do spectral clustering in such case\n",
        "\n",
        "In this case we know that there are two components, so we can take sub metrix (one of the non-zeros blocks in $L$ matrix), which represent component, and conduct spectral clustering on this component. if we required for more than 3 components, we can conduct it over and over.\n",
        "\n",
        "### How to do dimensionality reduction in such case\n",
        "\n",
        "As we explained before, in this case we know that there are two components, so we can take sub metrix (one of the non-zeros blocks in $L$ matrix), which represent component, and conduct dimensionality reduction on this component. if we required for more than 3 components, we can conduct it over and over.\n",
        "\n",
        "#### Note:\n",
        "there is no reason run the algorithm on the whole graph when its not connected, the computation time is bigger, and we can learn less from each component\n",
        "\n",
        "> Royi: Great answer!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "71PlKUkSCVqm"
      },
      "source": [
        "## 6. t-SNE (Bonus 4 Points)\n",
        "\n",
        "The t-SNE objective is given by:\n",
        "\n",
        "$$ \\arg \\min_{ \\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N} } f \\left( \\boldsymbol{Z} \\right) = \\arg \\min_{ \\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N} } {D}_{KL} \\left( \\boldsymbol{P} \\mid \\mid \\boldsymbol{Q} \\right) = \\arg \\min_{ \\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N} } \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} {p}_{ij} \\log \\left( \\frac{{p}_{ij}}{{q}_{ij}} \\right) $$\n",
        "\n",
        "Look for the definitions of $\\boldsymbol{P}$ and $\\boldsymbol{Q}$ **in the context of t-SNE** in lecture notes.\n",
        "\n",
        "### 6.1. The t-SNE Objective Gradient\n",
        "\n",
        "In the following sub questions the gradient of the objective function $\\nabla f \\left( \\boldsymbol{Z} \\right)$ will be derived in multiple steps.\n",
        "\n",
        "#### 6.1.1. Question\n",
        "\n",
        "Show that $f \\left( \\boldsymbol{Z} \\right) = {D}_{KL} \\left( \\boldsymbol{P} \\mid \\mid \\boldsymbol{Q} \\right)$ can be written as $f \\left( \\boldsymbol{Z} \\right) = C - \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{Q} \\right) \\right \\rangle$ for some constant $c$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The constant $C$ above is actually the _Entropy_ of $\\boldsymbol{P}$.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N2sDcsfNCVqm"
      },
      "source": [
        "#### 6.1.1. Solution\n",
        "\n",
        "$$\n",
        "f(Z)=D_{K L}(P \\| Q)=\\sum_{i=1}^N \\Sigma_{j=1}^N p_{i j} \\log \\left(\\frac{p_{i j}}{q i j}\\right)=\\sum_{i=1}^N \\Sigma_{j=1}^N p_{i j} \\log \\left(p_{i j}\\right)-\\sum_{i=1}^N \\Sigma_{j=1}^N p_{i j} \\log \\left(q_{i j}\\right)\n",
        "$$\n",
        "When optimizing with respect to $Z, p_{i j}$ is not dependent on $Z$ and therefore the first term can be treated as a constant. <br> Let $\\mathrm{C}=\\sum_{i=1}^N \\Sigma_{j=1}^N p_{i j} \\log \\left(p_{i j}\\right)$ thus, $f(Z)=C-\\sum_{i=1}^N \\Sigma_{j=1}^Np_{i j} \\log \\left(q_{i j}\\right)$<br>\n",
        "$\\sum_{i=1}^N \\Sigma_{j=1}^N p_{i j} \\log \\left(q_{i j}\\right)$ is dependent on $Z$ and can be written as $\\operatorname{Tr}\\left\\{P^T \\log (Q)\\right\\}=<P, \\log (Q)>$\n",
        "Therefore, $f(Z)=C-<P, \\log (Q)>$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ps2M2QbbCVqm"
      },
      "source": [
        " * Reminder $\\boldsymbol{Q} \\left[ i, j \\right] = \\frac{1}{B} \\begin{cases} 0 & \\text{ if } i = j \\\\ {\\left( 1 + {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}^{2} \\right)}^{-1} & \\text{ if } i \\neq j \\end{cases}$.\n",
        " * Let $\\boldsymbol{D}_{z} \\in \\mathbb{R}^{N \\times N}$ where $\\boldsymbol{D}_{z} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}^{2}$.  \n",
        " * Let $\\boldsymbol{S} = {\\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right)}^{\\circ -1} \\in \\mathbb{R}^{N \\times N}$ that is $\\boldsymbol{S} \\left[ i, j \\right] = {\\left( 1 + \\boldsymbol{D}_{z} \\left[ i, j \\right] \\right)}^{-1}$.\n",
        "\n",
        "\n",
        "#### 6.1.2. Question\n",
        "\n",
        "Show that $B = \\boldsymbol{1}^{T} \\left( \\boldsymbol{S} - \\boldsymbol{I} \\right) \\boldsymbol{1} \\in \\mathbb{R}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C592C7J6CVqm"
      },
      "source": [
        "#### 6.1.2. Solution\n",
        "\n",
        "\\begin{aligned}\n",
        "& S-I=\\left[\\begin{array}{ccc}\n",
        "\\frac{1}{1+D_Z[1,1]} & \\cdots & \\frac{1}{1+D_Z[1, N]} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{1}{1+D_Z[N, 1]} & \\cdots & \\frac{1}{1+D_Z[N, N]}\n",
        "\\end{array}\\right]-\\left[\\begin{array}{ccc}\n",
        "1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "0 & \\cdots & 1\n",
        "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
        "\\frac{-D_Z[1,1]}{1+D_Z[1,1]} & \\cdots & \\frac{1}{1+D_Z[1, N]} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{1}{1+D_Z[N, 1]} & \\cdots & \\frac{-D_Z[N, N]}{1+D_Z[N, N]}\n",
        "\\end{array}\\right]_{N x N} \\\\\n",
        "& 1^T(S-I)=\\left[\\begin{array}{llll}\n",
        "1 & 1 & \\ldots & 1\n",
        "\\end{array}\\right]_{1 \\times N}\\left[\\begin{array}{ccc}\n",
        "\\frac{-D_Z[1,1]}{1+D_Z[1,1]} & \\cdots & \\frac{1}{1+D_Z[1, N]} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{1}{1+D_Z[N, 1]} & \\cdots & \\frac{-D_Z[N, N]}{1+D_Z[N, N]}\n",
        "\\end{array}\\right]_{N \\times N}=\\left[\\frac{-D_Z[1,1]}{1+D_Z[1,1]}+\\right. \\\\\n",
        "& \\left.\\sum_{i=2}^N\\left(\\frac{1}{1+D_Z[i, 1]}\\right), \\ldots, \\frac{-D_Z[N, N]}{1+D_Z[N, N]}+\\sum_{i=1}^{N-1}\\left(\\frac{1}{1+D_Z[i, N]}\\right)\\right]_{1 x N} \\\\\n",
        "& 1^T(S-I) 1=\\left[\\frac{-D_Z[1,1]}{1+D_Z[1,1]}+\\sum_{i=2}^N\\left(\\frac{1}{1+D_Z[i, 1]}\\right), \\ldots, \\frac{-D_Z[N, N]}{1+D_Z[N, N]}+\\sum_{i=1}^{N-1}\\left(\\frac{1}{1+D_Z[i, N]}\\right)\\right]_{1 x N}\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "\\vdots \\\\\n",
        "1\n",
        "\\end{array}\\right]_{N \\times 1}= \\\\\n",
        "& \\Sigma_{\\mathrm{i}=1}^{\\mathrm{N}} \\Sigma_{i \\neq j}\\left(\\frac{1}{1+D_Z[i, j]}\\right)-\\Sigma_{\\mathrm{i}=1}^{\\mathrm{N}} \\Sigma_{i=j} \\frac{D_Z[i, j]}{1+D_Z[i, j]} \\text { clearly when } i=j \\rightarrow\\left\\|z_i-z_j\\right\\|_2^2=0 \\rightarrow 1^T(S-I) 1= \\\\\n",
        "& \\sum_{\\mathrm{i}=1}^{\\mathrm{N}} \\Sigma_{i \\neq j}\\left(\\frac{1}{1+D_z[i, j]}\\right)=\\sum_{\\mathrm{i}=1}^{\\mathrm{N}} \\Sigma_{i \\neq j}\\left(\\frac{1}{1+\\left\\|z_i-z_j\\right\\|_2^2}\\right)=\\sum_{\\mathrm{i}=1}^{\\mathrm{N}} \\Sigma_{i \\neq j}\\left(1+\\left\\|z_i-z_j\\right\\|_2^2\\right)^{-1}=B \\\\\n",
        "&\n",
        "\\end{aligned}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "neJT9KdCCVqm"
      },
      "source": [
        "#### 6.1.3. Question\n",
        "\n",
        "Show that $\\boldsymbol{Q} = \\frac{1}{B} \\left( \\boldsymbol{S} - \\boldsymbol{I} \\right)$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "li6hFB18CVqn"
      },
      "source": [
        "#### 6.1.3. Solution\n",
        "\n",
        "\\begin{aligned}\n",
        "& S-I=\\left[\\begin{array}{ccc}\n",
        "0 & \\cdots & \\frac{1}{1+D_Z[1, N]} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{1}{1+D_Z[N, 1]} & \\cdots & 0\n",
        "\\end{array}\\right]_{N x N} \\rightarrow \\frac{1}{B}(S-I)=\\left[\\begin{array}{ccc}\n",
        "0 & \\cdots & \\frac{1}{B\\left(1+D_Z[1, N]\\right)} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{1}{B\\left(1+D_Z[N, 1]\\right)} & \\cdots & 0\n",
        "\\end{array}\\right]_{N x N} \\\\\n",
        "& Q[i, j]=\\frac{1}{B}\\left\\{\\begin{array}{l}\n",
        "0 \\\\\n",
        "\\left(1+\\left\\|z_i-z_j\\right\\|_2^2\\right)^{-1} i \\neq j\n",
        "\\end{array}=\\frac{1}{B}(S-I)_{i j} \\rightarrow Q=\\frac{1}{B}(S-I)\\right.\n",
        "\\end{aligned}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GgWtH7ljCVqn"
      },
      "source": [
        "#### 6.1.4. Question\n",
        "\n",
        "Show that $- \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{Q} \\right) \\right \\rangle = \\log \\left( B \\right) + \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right) \\right \\rangle$.\n",
        "\n",
        " * <font color='brown'>(**#**)</font> Think of the value of $\\boldsymbol{P} \\left[ i, i \\right]$ and $\\boldsymbol{1}^{T} \\boldsymbol{P} \\boldsymbol{1}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy_V5vVpCVqn"
      },
      "source": [
        "#### 6.1.4. Solution\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& Q=\\frac{S-I}{B} \\rightarrow \\log (Q)=\\log (S-I)-\\log (B) \\rightarrow-<P, \\log (Q)>=-<P, \\log (S-I)>+ \\\\\n",
        "& <P, \\log (B)> \\\\\n",
        "& <P, \\log (B)>=\\Sigma_{i j} p_{i j} \\log (B)=\\log (B) \\Sigma_{i j} p_{i j} \\text { since B is a scalar. } \\\\\n",
        "& \\Sigma_{i j} p_{i j}=1 \\text { since it is the sum of probabilities. } \\\\\n",
        "& \\text { Therefore, }<P, \\log (B)>=\\log (B) \\text {. } \\\\\n",
        "& \\text { We need to show } \\log (S-I)=-\\log \\left(11^T+D_z\\right) \\text { : } \\\\\n",
        "& \\log \\left(11^T+D_z\\right)=\\log \\left(\\left[\\begin{array}{ccc}\n",
        "1 & \\cdots & 1+D_{z_{1 j}} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "1+D_{z_{j 1}} & \\cdots & 1\n",
        "\\end{array}\\right]\\right)=\\left[\\begin{array}{ccc}\n",
        "0 & \\cdots & \\log \\left(1+D_{z_{1 j}}\\right) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\log \\left(1+D_{z_{j 1}}\\right) & \\cdots & 0\n",
        "\\end{array}\\right] \\\\\n",
        "& \\log (S-I)=\\log \\left(\\left[\\begin{array}{ccc}\n",
        "0 & \\cdots & \\left(1+D_{z_{1 j}}\\right)^{-1} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\left(1+D_{z_{j 1}}\\right)^{-1} & \\cdots & 0\n",
        "\\end{array}\\right]\\right)= \\\\\n",
        "& {\\left[\\begin{array}{ccc}\n",
        "-\\infty & \\cdots & -\\log \\left(1+D_{z_{1 j}}\\right) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "-\\log \\left(1+D_{z_{j 1}}\\right) & \\cdots & -\\infty\n",
        "\\end{array}\\right]=-\\left[\\begin{array}{ccc}\n",
        "\\infty & \\cdots & \\log \\left(1+D_{z_{1 j}}\\right) \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\log \\left(1+D_{z_{j 1}}\\right) & \\cdots & \\infty\n",
        "\\end{array}\\right]=} \\\\\n",
        "& -\\log \\left(11^T+D_z\\right) \\forall i \\neq j \\\\\n",
        "&\n",
        "\\end{aligned}\n",
        "$$\n",
        "For our purposes since $P[i, i]=0$ the $\\infty$ values in the $S-I$ matrix are don't cares.\n",
        "Therefore $\\forall i \\neq j \\log (S-I)=-\\log \\left(11^T+D_z\\right)$\n",
        "Thus, $-<P, \\log (Q)>=\\log (B)+<P, \\log \\left(11^T+D_z\\right)>$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qHA659BwCVqn"
      },
      "source": [
        "Let $f \\left( \\boldsymbol{Z} \\right) = C + \\log \\left( B \\right) + \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right) \\right \\rangle$\n",
        "\n",
        "#### 6.1.5. Question\n",
        "\n",
        "Show that ${\\nabla}_{z} \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right) \\right \\rangle \\left[ \\boldsymbol{H} \\right] = \\left \\langle \\boldsymbol{S} \\circ \\boldsymbol{P}, {\\nabla}_{z} \\left[ \\boldsymbol{H} \\right] \\right \\rangle$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> You may use $\\nabla \\boldsymbol{S} \\left[ \\boldsymbol{H} \\right] = \\nabla {\\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right)}^{\\circ -1} \\left[ \\boldsymbol{H} \\right] = - {\\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right)}^{\\circ -2} \\circ \\nabla \\left( \\boldsymbol{D}_{z} \\right) \\left[ \\boldsymbol{H} \\right] = - \\boldsymbol{S} \\circ \\boldsymbol{S} \\circ \\nabla \\left( \\boldsymbol{D}_{z} \\right) \\left[ \\boldsymbol{H} \\right]$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6_M3JmF7CVqn"
      },
      "source": [
        "#### 6.1.5. Solution\n",
        "\n",
        "We just showed that $\\forall i \\neq j \\rightarrow \\log \\left(11^T+D_z\\right)=-\\log (S-I)$.\n",
        "Therefore, $\\nabla_z<P,-\\log (\\mathrm{S}-\\mathrm{I})>[\\mathrm{H}]=\\left\\langle\\nabla_{\\mathrm{z}} P[H],-\\log (S-I)>+\\left\\langle P, \\nabla_z-\\log (S-I)[H]\\right\\rangle\\right.$ $P$ is not dependent on $z$ and therefore its derivative is zero.\n",
        "Hence, $\\nabla_z<P,-\\log (\\mathrm{S}-\\mathrm{I})>[\\mathrm{H}]=<P, \\nabla_z-\\log (S-I)[H]>$\n",
        "$\\nabla_z-\\log (S-I)[H]=\\frac{1}{s-I} \\nabla_z(S-I)[H]=\\frac{1}{S-I} \\nabla_z S[H]-\\frac{1}{S-I} \\nabla_z I[H]=\\frac{1}{S-I} \\nabla_z S=-\\frac{1}{S-I} S \\circ S \\circ$ $\\nabla\\left(D_z\\right)[H]$\n",
        "$\\forall i \\neq j \\rightarrow S-I=S$, we can treat it this way since diagonal elements don't matter in thiscase.\n",
        "Thus, $\\nabla_z-\\log (S-I)[H]=-S \\circ \\nabla\\left(D_z\\right)[H]$\n",
        "Hence, $\\nabla_z<P,-\\log (\\mathrm{S}-\\mathrm{I})>[\\mathrm{H}]=\\left\\langle P, \\nabla_z-S \\circ \\nabla\\left(D_z\\right)[H]>=\\left\\langle\\mathrm{S}^{\\mathrm{T}} \\circ P,-\\nabla\\left(D_z\\right)[H]>\\right.\\right.$\n",
        "Since $S$ is a symmetric matrix we can say $S=S^T$.\n",
        "Therefore, $\\nabla_z<P,-\\log (\\mathrm{S}-\\mathrm{I})>[\\mathrm{H}]=-<\\mathrm{S} \\circ P,-\\nabla\\left(D_z\\right)[H]>$\n",
        "Since $\\log (S-I)=-\\log \\left(11^T+D_z\\right) \\rightarrow \\nabla_z<P, \\log \\left(11^{\\mathrm{T}}+\\mathrm{D}_z\\right)>[\\mathrm{H}]=-\\nabla_z<P,-\\log (\\mathrm{S}-\\mathrm{I})>$ $[\\mathrm{H}]=\\left\\langle\\mathrm{S} \\circ P,\\nabla\\left(D_z\\right[H])\\right\\rangle$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7n_OC3t2CVqn"
      },
      "source": [
        "\n",
        "#### 6.1.6. Question\n",
        "\n",
        "Show that ${\\nabla}_{z} \\log \\left( B \\right) = \\left \\langle \\boldsymbol{S} \\circ \\boldsymbol{Q}, \\nabla \\left( \\boldsymbol{D}_{z} \\right) \\left[ \\boldsymbol{H} \\right] \\right \\rangle$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> You may use $\\boldsymbol{Q} = \\frac{1}{B} \\left( \\boldsymbol{S} - \\boldsymbol{I} \\right)$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Royi: <font color='green'> +2 Points (Bonus)</font>. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hBQh_MwNCVqn"
      },
      "source": [
        "#### 6.1.6. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GBLdWu4fCVqn"
      },
      "source": [
        "\n",
        "#### 6.1.7. Question\n",
        "\n",
        "Combine all previous and writhe the gradient of the objective $\\nabla f \\left( \\boldsymbol{Z} \\right)$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> You may use $\\boldsymbol{A} = \\left( \\boldsymbol{P} - \\boldsymbol{Q} \\right) \\circ \\boldsymbol{S}$ to simplify the answer."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z-sll45UCVqn"
      },
      "source": [
        "#### 6.1.7. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2VJmXuKjCVqo"
      },
      "source": [
        "\n",
        "#### 6.1.8. Question\n",
        "\n",
        "What can you say about the gradient of $\\nabla f \\left( \\boldsymbol{Z} \\right)$ when $\\boldsymbol{P} = \\boldsymbol{Q}$?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6v2oEvAOCVqo"
      },
      "source": [
        "#### 6.1.8. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
