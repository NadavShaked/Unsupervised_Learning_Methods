{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J3l8tGVRq4sC"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 004 - Part IV\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 0.1.000 | 17/06/2023 | Royi Avital | First version                                                      |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O67I6MsTq4sH"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0004Part004.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj8Tx-85u8Ym",
        "outputId": "4934cb05-3609-4d47-dcf9-67f686676a67"
      },
      "outputs": [],
      "source": [
        "# !pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T09:30:06.492269Z",
          "start_time": "2022-02-02T09:30:06.220934Z"
        },
        "id": "hj6nc6Gvq4sI"
      },
      "outputs": [],
      "source": [
        "# Import Packages\n",
        "\n",
        "# General Tools\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import Isomap, MDS, TSNE\n",
        "from sklearn.metrics import pairwise_distances\n",
        "# from umap import UMAP\n",
        "from umap.umap_ import UMAP\n",
        "\n",
        "# Computer Vision\n",
        "\n",
        "# Miscellaneous\n",
        "import os\n",
        "import math\n",
        "from platform import python_version\n",
        "import random\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "# Typing\n",
        "from typing import Callable, List, Tuple, Union\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Jupyter\n",
        "from IPython import get_ipython\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pERy-fkIq4sK"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGgM2VDhqATj"
      },
      "outputs": [],
      "source": [
        "# Import time module\n",
        "import time\n",
        "\n",
        "# record start time\n",
        "start = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9sgAtHeq4sK"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# %matplotlib inline\n",
        "\n",
        "seedNum = 512\n",
        "np.random.seed(seedNum)\n",
        "random.seed(seedNum)\n",
        "\n",
        "# sns.set_theme() #>! Apply SeaBorn theme\n",
        "\n",
        "runInGoogleColab = 'google.colab' in str(get_ipython())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6H4dVbgq4sL"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "DATA_FILE_URL   = r'https://drive.google.com/uc?export=download&confirm=9iBg&id=1UXLdZgXwClgwZVszRq88UaaN2nvgMFiC'\n",
        "DATA_FILE_NAME  = r'BciData.npz'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LeJrHLmq4sL"
      },
      "outputs": [],
      "source": [
        "# Auxiliary Functions\n",
        "\n",
        "def Plot2DScatter(mX: np.ndarray, hA: plt.Axes, vC: np.ndarray = None) -> None:\n",
        "    m = mX.min()\n",
        "    M = mX.max()\n",
        "    if vC is not None:\n",
        "        hA.scatter(*mX.T, s = 50,  c = vC, edgecolor = 'k', alpha = 1)\n",
        "    else:\n",
        "        hA.scatter(*mX.T, s = 50,  c = 'lime', edgecolor = 'k', alpha = 1)\n",
        "    hA.set_xlim([m, M])\n",
        "    hA.set_ylim([m, M])\n",
        "    hA.set_xlabel('$x_1$')\n",
        "    hA.set_ylabel('$x_2$')\n",
        "\n",
        "\n",
        "def PlotLabelsHistogram(vY: np.ndarray, hA = None, lClass = None, xLabelRot: int = None) -> plt.Axes:\n",
        "\n",
        "    if hA is None:\n",
        "        hF, hA = plt.subplots(figsize = (8, 6))\n",
        "\n",
        "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
        "\n",
        "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
        "    hA.set_title('Histogram of Classes / Labels')\n",
        "    hA.set_xlabel('Class')\n",
        "    hA.set_ylabel('Number of Samples')\n",
        "    hA.set_xticks(vLabels)\n",
        "    if lClass is not None:\n",
        "        hA.set_xticklabels(lClass)\n",
        "\n",
        "    if xLabelRot is not None:\n",
        "        for xLabel in hA.get_xticklabels():\n",
        "            xLabel.set_rotation(xLabelRot)\n",
        "\n",
        "    return hA\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tz-TP8tq4sM"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
        " - Answer all questions / tasks within the Jupyter Notebook.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Verify the rendering on VS Code.\n",
        " - Submission in groups (Single submission per group).\n",
        " - You may and _should_ use the forums for questions.\n",
        " - Good Luck!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dRvf8Bqgq4sM"
      },
      "source": [
        "* <font color='brown'>(**#**)</font> The `Import Packages` section above imports most needed tools to apply the work. Please use it.\n",
        "* <font color='brown'>(**#**)</font> You may replace the suggested functions to use with functions from other packages.\n",
        "* <font color='brown'>(**#**)</font> Whatever not said explicitly to implement maybe used by a 3rd party packages.\n",
        "* <font color='brown'>(**#**)</font> The total run time of this notebook must be **lower than 90 [Sec]**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1okC38rNq4sM"
      },
      "source": [
        "## Team Members\n",
        "\n",
        "- Nadav_Talmon_203663950\n",
        "- Nadav_Shaked_312494925\n",
        "- Adi_Rosenthal_316550797"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V_2XBJCZq4sN"
      },
      "source": [
        "## Generate / Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KbzxxI2q4sN"
      },
      "outputs": [],
      "source": [
        "# Download Data\n",
        "# This section downloads data from the given URL if needed.\n",
        "\n",
        "if (DATA_FILE_NAME != 'None') and (not os.path.exists(DATA_FILE_NAME)):\n",
        "    urllib.request.urlretrieve(DATA_FILE_URL, DATA_FILE_NAME)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "onNa1OUHq4sN"
      },
      "source": [
        "## 9. T-SNE and UMAP\n",
        "\n",
        "In this section we'll use the _t-SNE_ and _UMAP_ algorithms to analyze EEG Signals in teh context of _Brain Computer Interface_ (BCI).  \n",
        "\n",
        "### The BCI Data Set\n",
        "\n",
        "The Brain Computer Interfaces (BCI) Data from [BCI Competition IV](https://www.bbci.de/competition/iv/).  \n",
        "Specifically we'll use [data set 2a](https://www.bbci.de/competition/iv/#dataset2a) provided by the Institute for Knowledge Discovery (Laboratory of Brain Computer Interfaces), Graz University of Technology, (Clemens Brunner, Robert Leeb, Gernot Müller-Putz, Alois Schlögl, Gert Pfurtscheller).  \n",
        "This is a recording of EEG signals while the subject is doing a cued movement of the left hand, right hand, feet or tongue.\n",
        "\n",
        "The data is composed of:\n",
        "\n",
        " * 22 EEG channels (0.5-100Hz; notch filtered).\n",
        " * Sampling Rate of 250 [Hz] for 4 [Sec] for total of 1000 samples.\n",
        " * 4 classes: _left hand_, _right hand_, _feet_, _tongue_.\n",
        " * 9 subjects.\n",
        " * 287 measurements.\n",
        "\n",
        "</br>\n",
        "\n",
        "* <font color='brown'>(**#**)</font> You should install [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html) on your system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0Ac5ze1q4sN",
        "outputId": "6300bc16-b51a-493c-8ac6-3d38aa95ad52"
      },
      "outputs": [],
      "source": [
        "# Generate Data\n",
        "\n",
        "dData = np.load(DATA_FILE_NAME)\n",
        "mX    = dData['mX1']\n",
        "vY    = dData['vY1']\n",
        "\n",
        "# Sample: An observation of a subject.\n",
        "# Measurement: Sample in time of the EEG signal.\n",
        "# Channel: EEG Channel.\n",
        "numSamples, numMeasurements, numChannels = mX.shape\n",
        "lLabel = ['Left Hand', 'Right Hand', 'Foot', 'Tongue'] #<! The labels\n",
        "\n",
        "print(f'The data shape: {mX.shape}')\n",
        "print(f'The number of samples: {mX.shape[0]}')\n",
        "print(f'The number of measurements per sample: {mX.shape[1]}')\n",
        "print(f'The number of EEG channels: {mX.shape[2]}')\n",
        "print(f'The classes labels: {np.unique(vY)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W9qx6m4gq4sO"
      },
      "source": [
        "* <font color='brown'>(**#**)</font> In the above `sample` is used per object detected.\n",
        "* <font color='brown'>(**#**)</font> In the above `measurement` is used to describe the time sample (Like in Signal Processing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "-oTN5KOPq4sO",
        "outputId": "699901d8-976e-4c17-f865-2dac47a9213c"
      },
      "outputs": [],
      "source": [
        "# Plot the Data\n",
        "\n",
        "numRowsPlot = 3\n",
        "numColsPlot = 2\n",
        "\n",
        "numPlots = numRowsPlot * numColsPlot\n",
        "\n",
        "hF, hAs = plt.subplots(nrows = numRowsPlot, ncols = numColsPlot, figsize = (18, 10))\n",
        "hAs = hAs.flat\n",
        "\n",
        "vIdx = np.random.choice(numSamples, numPlots, replace = False)\n",
        "\n",
        "for sampleIdx, hA in zip(vIdx, hAs):\n",
        "    mXX = mX[sampleIdx, :, :]\n",
        "    hA.plot(mXX)\n",
        "    hA.set_title(f'EEG Signals of Sample {sampleIdx:04d} with Label {lLabel[vY[sampleIdx]]}')\n",
        "    hA.set_xlabel('Measurement Index')\n",
        "    hA.set_ylabel('Measurement Value')\n",
        "\n",
        "hF.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "XgjrnK4gq4sO",
        "outputId": "d2813dc4-f3b5-4c9e-ec58-94e7718fce8e"
      },
      "outputs": [],
      "source": [
        "# Plot the Data\n",
        "# Show a shifted and scaled channels at a single plot.\n",
        "# You may run it several times to observe different measurements.\n",
        "\n",
        "numSamples, numMeasurements, numChannels = mX.shape #<! Samples, Time Samples, Channels\n",
        "idx     = np.random.randint(numSamples)\n",
        "mXi     = mX[idx, :, :].copy()\n",
        "yi      = vY[idx]\n",
        "\n",
        "# Normalizing and Vertically Shifting\n",
        "mXi -= mXi.mean(0)\n",
        "mXi /= 20\n",
        "mXi += np.arange(numChannels)[None, :]\n",
        "vT   = np.linspace(0, 4, numMeasurements, endpoint = False)\n",
        "\n",
        "hF, hA = plt.subplots(figsize = (10, 6))\n",
        "hA.plot(vT, mXi)\n",
        "hA.set_title (f'Raw EEG Data\\nTrue Label = {lLabel[yi]}')\n",
        "hA.set_xlabel('Time [Sec]')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "pyHQiM4Sq4sO",
        "outputId": "18928a0d-5919-465a-caab-3e256af8c45f"
      },
      "outputs": [],
      "source": [
        "# Histogram of Labels\n",
        "\n",
        "hA = PlotLabelsHistogram(vY, lClass = lLabel)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7G1QhvaVq4sP"
      },
      "source": [
        "* <font color='brown'>(**#**)</font> The data is balanced."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E-urfYmJq4sP"
      },
      "source": [
        "### 9.1. Processing RAW Data\n",
        "\n",
        "In this section we'll compare 4 dimensionality reduction methods:\n",
        "\n",
        " - IsoMap.\n",
        " - MDS.\n",
        " - t-SNE.\n",
        " - UMAP.\n",
        "\n",
        "We'll reduce the data dimension to `d = 2` and use the reference labels to display result.\n",
        "\n",
        "The steps:\n",
        "\n",
        "1. Data Pre Processing  \n",
        "     * Reshape data into `mXRaw` such that each measurement is a row.  \n",
        "       Namely the data should have dimensions of `287 x 22_000`.  \n",
        "     * Apply dimensionality reduction using PCA to keep most of the data energy.\n",
        "2. Set Parameters  \n",
        "   Set parameters of each method to get results.  \n",
        "   Set the amount of energy preserved using the PCA pre processing.\n",
        "   **No need for grid search, just try and error of few values**.\n",
        "3. Apply the Transform  \n",
        "   Apply the transform on the data.  \n",
        "   The low dimensional data should be in 2D space.  \n",
        "4. Display Results  \n",
        "   Create a subplot per method showing teh results and the used parameters.  \n",
        "\n",
        "* <font color='brown'>(**#**)</font> No need to self implement any of the methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "W6LlD931q4sP",
        "outputId": "dae7bc99-deaa-4a74-bbbd-972af1a36d4d"
      },
      "outputs": [],
      "source": [
        "#===========================Fill This===========================#\n",
        "# 1. Pre Process data:\n",
        "#    * Reshape to (287 x 22_000).\n",
        "#    * Apply PCA to preserve `relEnergy` of the energy.\n",
        "# 2. Apply Dimensionality Reduction (d = 2) using IsoMap, MDS, t-SNE and UMAP.\n",
        "#    Choose a reasonable parameters for each.\n",
        "# 3. Display the Low Dimensionality data.\n",
        "#    Include the parameters in the title of each method.\n",
        "# !! The output should be a figure of 1x4 axes (2D Scatter and 3D Scatter per method).\n",
        "# !! You may use `Plot2DScatter()` for displaying the the data.\n",
        "\n",
        "d = 2\n",
        "relEnergy   = 0.95\n",
        "\n",
        "mX_reshaped = mX.reshape(mX.shape[0], -1)\n",
        "\n",
        "pca = PCA(n_components=relEnergy)\n",
        "pca_mX = pca.fit_transform(mX_reshaped)\n",
        "print(\"Number of dimensions to preserve desired relative energy:\", pca_mX.shape[1])\n",
        "\n",
        "algorithms = {\n",
        "      'MDS': lambda : MDS(n_components=d, normalized_stress='auto'),\n",
        "      'ISO map': lambda : Isomap(n_components=d, n_neighbors=5),\n",
        "      't-SNE': lambda : TSNE(n_components=d),\n",
        "      'UMAP': lambda : UMAP(n_components=d, n_neighbors=5)\n",
        "}\n",
        "\n",
        "fig, ax  = plt.subplots(1,4, figsize = (20, 6))\n",
        "\n",
        "for i, (algorithm_name, algorithm) in enumerate(algorithms.items()):\n",
        "    reduction = algorithm()\n",
        "    mZ = reduction.fit_transform(pca_mX)\n",
        "    Plot2DScatter(mZ, ax[i], vC = vY)\n",
        "    ax[i].set_title(f\"Method - {algorithm_name}, d = {d}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#===============================================================#"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UNtQvYzWq4sP"
      },
      "source": [
        "### 9.1.1 Question\n",
        "\n",
        "1. What's the purpose of the PCA pre process?\n",
        "2. Explain the results. Think in the context of being able to classify the body part in action."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ifhRhVc3q4sP"
      },
      "source": [
        "### 9.1.1 Solution\n",
        "\n",
        "1. What's the purpose of the PCA pre process?\n",
        "\n",
        "  The purpose of the PCA pre-processing step in this context is to reduce the dimensionality of the data while preserving most of its energy or variance.\n",
        "\n",
        "  The PCA pre-processing can help to reduce computational complexity by reducing the dimensionality of the input data. It can remove noise or less relevant features from the data, and allowing to focus on the most important variations in the data.\n",
        "\n",
        "  Note:\n",
        "  PCA pre-processing not guaranteed good dimentionality reduction, and sometimes won't bring to a effective results, the results may vary depending on the dataset.\n",
        "\n",
        "2. Explain the results. Think in the context of being able to classify the body part in action.\n",
        "\n",
        "  By each of the dimentionality reduction method we can learn something for the pre-precessed PCA and the dimentionality reduction after it.\n",
        "\n",
        "  By MDS technique we can learn that there is no similarity for each data label clasification group.\n",
        "\n",
        "  By ISO-map technique we can learn that the data is not represented ny manifold\n",
        "\n",
        "  By t-SNE technique we can learn that the local area for each data point is not related to the same classifcation\n",
        "\n",
        "  Note:\n",
        "  All of this assumption are related to results above (PCA pre-processing and then dimensionality reduction of each technique)\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "29LMkTa5q4sP"
      },
      "source": [
        "### 9.2. Feature Engineering\n",
        "\n",
        "Form the above it is clear the RAW measurements are not a good representation of the data.  \n",
        "Since each sample is basically $\\boldsymbol{X}_{i}\\in\\mathbb{R}^{1000 \\times 22}$ we can work on the time axis of the data.  \n",
        "\n",
        "The strategy is as following:\n",
        "\n",
        " * Use the Covariance Matrix is a feature of the sample.\n",
        " * Use a metric optimized to covariance matrices.\n",
        "\n",
        "Do the following steps:\n",
        "\n",
        "1. Implements `GenCovMat()`  \n",
        "   For each sample $\\boldsymbol{X}_{i}\\in\\mathbb{R}^{1000 \\times 22}$, compute the covariance matrix $\\boldsymbol{C}_{i}\\in\\mathbb{R}^{22\\times22}$.  \n",
        "   You may use [`np.cov()`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html).\n",
        "2. Implement `SpdMetric()`  \n",
        "   Given 2 SPD matrices in the form of vectors computes the geodesic distance between them.\n",
        "3. Apply the Feature Engineering  \n",
        "   The end of this process should be a matrix `mZ` with dimensions (287 x 22^2)\n",
        "4. Generate the Distance Matrix  \n",
        "   Using `SpdMetric()` generate the distance matrix of the data.  \n",
        "   The output should be a matrix `mD` with dimensions (287 x 287).\n",
        "\n",
        "#### The SPD Matrices Metric\n",
        "\n",
        "An optimized metric for the manifold of SPD Matrices, is given by the SPD Metric (`SpdMetric`):\n",
        "\n",
        "$$d\\left(\\boldsymbol{C}_{i},\\boldsymbol{C}_{j}\\right)=\\sqrt{\\sum_{i=1}^{d}\\log^{2}\\left(\\lambda_{i}\\left(\\boldsymbol{C}_{i}^{-1}\\boldsymbol{C}_{j}\\right)\\right)}$$\n",
        "\n",
        "Where ${\\lambda}_{i} \\left( \\cdot \\right)$ extract the $i$ -th eigen value of the matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wODnOsj5q4sQ"
      },
      "outputs": [],
      "source": [
        "def GenCovMat(mX: np.ndarray) -> np.ndarray:\n",
        "    '''\n",
        "    Calculates the covariance matrices of the input data.\n",
        "    Args:\n",
        "        mX - Input data with shape (N x T x C)\n",
        "    Output:\n",
        "        mC - N covariance matrices with shape (N x C x C)\n",
        "    '''\n",
        "\n",
        "    N , T, C = mX.shape\n",
        "    mC = np.zeros((N,C,C))\n",
        "\n",
        "    for i in range(N):\n",
        "        mC[i] = np.cov(mX[i].T)\n",
        "\n",
        "    return mC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zPpCWrLq4sQ"
      },
      "outputs": [],
      "source": [
        "def SpdMetric(vP: np.ndarray, vQ: np.ndarray) -> float:\n",
        "    '''\n",
        "    Calculates the AIRM geodesic distance between two SPD matrices.\n",
        "    Args:\n",
        "        mP    - An SPD matrix in the form of a vector with shape (d^2, )\n",
        "        mQ    - An SPD matrix in the form of a vector with shape (d^2, )\n",
        "    Output:\n",
        "        geoDist - The geodesic distance, dist ≥ 0\n",
        "    '''\n",
        "\n",
        "    #===========================Fill This===========================#\n",
        "    # 1. Convert the vectors into a matrix.\n",
        "    # 2. Calculate the geodesic distance of SPD matrices.\n",
        "    # !! Do not use any loop!\n",
        "    # !! You may find `scipy.linalg.eigvalsh()` useful.\n",
        "\n",
        "    d = int(np.sqrt(len(vP)))\n",
        "    mC_i = vP.reshape((d, d))\n",
        "    mC_j = vQ.reshape((d, d))\n",
        "\n",
        "    eigenvalues = sp.linalg.eigvalsh(mC_j, mC_i)\n",
        "    geoDist = np.sqrt(np.sum(np.log(eigenvalues) ** 2))\n",
        "\n",
        "    #===============================================================#\n",
        "\n",
        "    return geoDist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhnwaH7lq4sQ"
      },
      "outputs": [],
      "source": [
        "# Process Data\n",
        "\n",
        "#===========================Fill This===========================#\n",
        "# 1. Generate the covariance matrices.\n",
        "# 2. Reshape data into the samples matrix.\n",
        "# 3. Calculate the distance matrix.\n",
        "# !! Don't use any loop!\n",
        "# !! You may use `pairwise_distances()` in order to avoid loops.\n",
        "\n",
        "mC = GenCovMat(mX)\n",
        "\n",
        "mC = mC.reshape(mC.shape[0], -1)\n",
        "mD = pairwise_distances(mC, metric=SpdMetric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "zXU3DMyEq4sQ",
        "outputId": "67b44d10-e262-42df-8f5f-e651114a6dec"
      },
      "outputs": [],
      "source": [
        "# Plot the Low Dimensional Data\n",
        "# This shows the result using IsoMap with SpdMetric.\n",
        "\n",
        "oIsoMapDr = Isomap(n_neighbors = 3, n_components = d, metric = 'precomputed')\n",
        "\n",
        "hF = plt.figure(figsize = (12, 6))\n",
        "\n",
        "mZi = oIsoMapDr.fit_transform(mD)\n",
        "hA = hF.add_subplot()\n",
        "Plot2DScatter(mZi, hA, vY)\n",
        "hA.set_title(f'IsoMap (SPD Metric) with d = {d}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FwKZeMcsq4sQ"
      },
      "source": [
        "#### 9.2.1. Question\n",
        "\n",
        "In the above the _IsoMap_ is used.  \n",
        "If one would like to use SciKit Learn's _Spectral Embedding_ (See [`SpectralEmbedding`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html)) one need to supply _Affinity Matrix_ (See `affinity` parameter).  \n",
        "Describe how would you generate such matrix based on `SpdMetric()`.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> SciKit Learn's _Spectral Embedding_ is equivalent to _Laplacian Eigenmaps_ on lecture notes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tQyiPU5cq4sQ"
      },
      "source": [
        "#### 9.2.1. Solution\n",
        "\n",
        "To generate the affinity matrix for SciKit Learn's Spectral Embedding based on the SpdMetric(), you can follow these steps:\n",
        "\n",
        "Calculate the distance matrix `mD` based on the SpdMetric() for your dataset. The distance matrix represents the pairwise geodesic distances between the covariance matrices.\n",
        "\n",
        "Convert the distance matrix `mD` into the affinity matrix `m_aff` by applying the exponential function element-wise:\n",
        "\n",
        "`m_aff[i, j] = np.exp(-mD[i, j])`.\n",
        "\n",
        "The resulting affinity matrix `m_aff` is now suitable for use as the input affinity parameter in SciKit Learn's Spectral Embedding. It captures the pairwise similarities or dissimilarities between the covariance matrices based on the geodesic distances.\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CCzVsqHLq4sR"
      },
      "source": [
        "### 9.3. Dimensionality Reduction and Classification on Engineered Data\n",
        "\n",
        "In this section we'll focus on `t-SNE` and `UMAP`.  \n",
        "\n",
        "1. Apply Dimensionality Reduction  \n",
        "   Apply `t-SNE` and / or `UMAP` on the engineered data.  \n",
        "   Make sure to use the appropriate options to utilize the metric.  \n",
        "   Apply this on the whole data.\n",
        "2. Apply Classification  \n",
        "   Apply a classifier (From SciKit Learn without Neural Based) on the low dimensional data.\n",
        "3. Measure Performance of the Classifier\n",
        "   Apply leave one out cross validation with score based on accuracy.  \n",
        "   You should use [`cross_val_predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html).  \n",
        "   This will generate a vector of 287 predictions.\n",
        "4. Display Results  \n",
        "   Display the confusion matrix and the total accuracy.\n",
        "\n",
        "**This is a competition**:\n",
        "\n",
        " * The group with the 1st highest score will get 4 bonus points.\n",
        " * The group with the 2nd highest score will get 2 bonus points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "qutb2u_cq4sR",
        "outputId": "d081533b-89c4-4f73-968c-43d3aee547d1"
      },
      "outputs": [],
      "source": [
        "#===========================Fill This===========================#\n",
        "# 1. Use t-SNE and/or UMAP for Dimensionality Reduction.\n",
        "# 2. On the output of (1) apply a classifier from SciKit Learn.\n",
        "# 3. To measure the performance use `cross_val_predict()` with accuracy as score.\n",
        "# 4. Display the confusion matrix of the result of the `cross_val_predict()`.\n",
        "# !! Use the `SpdMetric()` for UMAP / t-SNE!\n",
        "# !! This is a competition, optimize the hyper parameters of each step for best accuracy.\n",
        "# !! No use of Neural Net based classifiers!\n",
        "# !! If you use t-SNE, make sure to make it non random (See `init` and `random_state`).\n",
        "# !! Pay attention to the run time limitation of the whole notebook.\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "d_umap = 4\n",
        "algorithms = {\n",
        "      'UMAP': lambda : UMAP(n_components=d_umap, negative_sample_rate=2, min_dist=1, metric=SpdMetric, random_state=seedNum)\n",
        "}\n",
        "\n",
        "algorithms_accuracy = {}\n",
        "\n",
        "classifier = SVC(random_state=seedNum, gamma='auto')\n",
        "pipeline = Pipeline([('classifier', classifier)])\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "for algorithm_name, algorithm in algorithms.items():\n",
        "    reduction = algorithm()\n",
        "    mZ = reduction.fit_transform(mC)\n",
        "    y_pred = cross_val_predict(pipeline, mZ, vY, cv=loo)\n",
        "    algorithms_accuracy[algorithm_name] = accuracy_score(vY, y_pred)\n",
        "\n",
        "    confusion_matrix_tsne = confusion_matrix(vY, y_pred, labels = np.unique(vY))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(confusion_matrix_tsne, cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix - {algorithm_name}')\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "#===============================================================#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycV-3ibSq42I",
        "outputId": "4d3a0e97-a788-46c5-872c-24d9a9500f3c"
      },
      "outputs": [],
      "source": [
        "print(f\"Accuracy UMAP = {algorithms_accuracy['UMAP'] * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWj5UT2QqD-B",
        "outputId": "8b59d881-ee29-41b3-e14d-286a361c6c51"
      },
      "outputs": [],
      "source": [
        "end = time.time()\n",
        "print(end - start)\n",
        "assert(end - start <= 90)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Royi: Great idea to check this! I will embrace."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TeWpXs1Yq4sR"
      },
      "source": [
        "#### 9.3.1. Question\n",
        "\n",
        "In the above we had the whole data for the _dimensionality reduction_ step.  \n",
        "Assume we use a method without _out of sample extension_.  \n",
        "In the case above, how would you handle a new data point? Explain the pipeline."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5LEZnxDq4sR"
      },
      "source": [
        "#### 9.3.1. Solution\n",
        "\n",
        "To handle a new data point in the case where we use a dimensionality reduction method without out-of-sample extension, we can follow the following pipeline:\n",
        "\n",
        "Preprocess the new data point:\n",
        "\n",
        "Use dimensionality reduction algorithm to transform the known data points into the reduced-dimensional space.\n",
        "This involves applying the learned mapping or transformation function from the original feature space to the reduced-dimensional space.\n",
        "\n",
        "Train a NN model that will get as input data point in the original space, and predicte data point after dimensionality reduction.\n",
        "\n",
        "We should train this model by the training set we generate by the dimensionality reduction algorithm.\n",
        "Classify the new data point:\n",
        "\n",
        "By following this pipeline, we can incorporate the dimensionality reduction technique to handle dimensionality reduction for new data points.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
