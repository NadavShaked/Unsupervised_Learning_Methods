{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 003 - Part I\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 14/05/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0002Part001.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
    " - Answer all questions / tasks within the Jupyter Notebook.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Verify the rendering on VS Code.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for questions.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    "<font color='red'>Got 5 days submission extension by Adi's reserved duty</font>\n",
    "\n",
    "- Nadav_Talmon_203663950\n",
    "- Nadav_Shaked_312494925\n",
    "- Adi_Rosenthal_316550797"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principle Component Analysis (PCA)\n",
    "\n",
    "Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a diagonalizable matrix, that is $\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}$ where $\\boldsymbol{\\Lambda}$ is a diagonal matrix.\n",
    "\n",
    "### 1.1. Question\n",
    "\n",
    "Prove the following:\n",
    "\n",
    "$$ \\operatorname{Tr} \\left( A \\right) = \\sum_{i = 1}^{d} {\\lambda}_{i} \\left( A \\right) $$\n",
    "\n",
    "Where $\\operatorname{Tr} \\left( A \\right) = \\sum_{i = 1}^{d} {A}_{ii}$ and ${\\lambda}_{i} \\left( \\boldsymbol{A} \\right) = {\\Lambda}_{ii}$ is the $i$ -th eigen value of $\\boldsymbol{A}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Solution\n",
    "\n",
    "Since $A$ is diagonalizeable $\\lambda_i$ exist.\n",
    "\n",
    "Using the fact that $A$ is diagonalizeable we can perform eigen decomposition and write $A$ as $QDQ^{-1}$ where $D$ is diagonal matrix such taht $D_{ii} = \\lambda_i(A)$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$Tr(A) = Tr(QDQ^{-1}) = Tr(DQ^{-1}Q) = Tr(D) = \\Sigma^{d}_{i=1} D_{ii} = \\Sigma^{d}_{i=1} \\lambda_i(A)$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two square matrices $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ and $\\boldsymbol{B} \\in \\mathbb{R}^{d \\times d}$ are called similar, namely $ \\boldsymbol{A} \\sim \\boldsymbol{B}$ if there exist an invertible matrix $\\boldsymbol{P} \\in \\mathbb{R}^{d \\times d}$ such that:\n",
    "\n",
    "$$ \\boldsymbol{B} = \\boldsymbol{P} \\boldsymbol{A} \\boldsymbol{P}^{-1} $$\n",
    "\n",
    "* <font color='brown'>(**#**)</font> It means all matrices which are diagonalizable are similar to their respective diagonal matrix.\n",
    "\n",
    "### 1.2. Question\n",
    "\n",
    "Prove that if $\\boldsymbol{A}$ is diagonalizable and $\\boldsymbol{A} \\sim \\boldsymbol{B}$ then $\\boldsymbol{A}$ and $\\boldsymbol{B}$ share the same eigen values, namely:\n",
    "\n",
    "$$ \\boldsymbol{A} \\sim \\boldsymbol{B} \\implies {\\left\\{ {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) \\right\\}}_{i = 1}^{d} = {\\left\\{ {\\lambda}_{i} \\left( \\boldsymbol{B} \\right) \\right\\}}_{i = 1}^{d} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solution\n",
    "\n",
    "Since $\\mathrm{A}$ is diagonalizable matrix, we know that $A \\sim D$, its diagonal matrix, by definition.\n",
    "\n",
    "Following that, we know that $A \\sim B$ therefore, since similarity is closed under equivalence then $B \\sim D$.\n",
    "\n",
    "Thus, $B$ is also diagonalizable. \n",
    "\n",
    "So,\n",
    "\n",
    "$A = P D P^{-1}$\n",
    "\n",
    "$B = P A P^{-1} = PQDQ^{-1}P^{-1}$\n",
    "<br>\n",
    "Let $Z=PQ$ <br> therefore, $B = Z D Z^{-1}$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$\\left\\{\\lambda_i(B)\\right\\}_{i = 1}^d = \\left\\{\\lambda_i(D)\\right\\}_{i = 1}^d = \\left\\{\\lambda_i(A)\\right\\}_{i = 1}^d$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A symmetric matrix $\\boldsymbol{A} = \\boldsymbol{A}^{T}$ is called a _Symmetric Positive Definite_ (SPD) matrix if either:\n",
    "\n",
    "* All eigen values are positive: $\\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0$.\n",
    "* Any quadratic form is positive: $\\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$.\n",
    "\n",
    "Namely $\\boldsymbol{A} \\succ 0$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> There is also a _Symmetric Semi Positive Definite Matrix_ (SPSD) which obeys the above with weak inequality.\n",
    "\n",
    "### 1.3. Question\n",
    "\n",
    " 1. Prove the equivalency of the 2 properties, namely:\n",
    "\n",
    "$$ \\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0 \\iff  \\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$$\n",
    "\n",
    " 2. Prove or disprove: $\\boldsymbol{A}^{-1}$ is a _Symmetric Positive Definite_ matrix.\n",
    " 3. Prove or disprove: The SVD of $\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T}$ is also an eigen decomposition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Solution\n",
    "\n",
    "1.\n",
    "    *   $\\forall i: \\lambda_i(A) > 0 \\rightarrow \\forall v \\neq 0, v^T A v > 0$\n",
    "\n",
    "    Since all eigenvalues are positive, we can conclude that $A$ is SPD. Therefore, $A$ is diagonalizable.\n",
    "\n",
    "    Let's write $A$ as $P D P^T$\n",
    "    \n",
    "    Thus,\n",
    "\n",
    "    $v^T A v = v^T P^T D P v = v^T P^T \\sqrt{D}^T \\sqrt{D} P v$\n",
    "\n",
    "    Denote $Z = v P \\sqrt{D}$, Thus\n",
    "\n",
    "    $v^T A v = Z^T Z > 0$\n",
    "    \n",
    "    So $\\forall v \\neq 0, v^T A v>0$\n",
    "\n",
    "    *   $\\forall v \\neq 0, v^T A v > 0 \\rightarrow \\forall i: \\lambda_i(A) > 0$\n",
    "\n",
    "    Since $\\forall v \\neq 0, v^T A v > 0 \\rightarrow A$ is SPD, Therefore, it is diagonalizable.\n",
    "\n",
    "    Let's write $A$ as $P D P^T$ where $D$ is a diagonal matrix with $A$'s eigenvalues on its diagonal.\n",
    "    \n",
    "    Denote $Z = v P$\n",
    "    \n",
    "    Therefore,\n",
    "    \n",
    "    $v^T P^T D P v = Z^T D Z = \\sum_{i=1}^d \\lambda_i z_i^2$\n",
    "    \n",
    "    So $\\lambda_i$ must be strictly positive for $A$ to be SPD.\n",
    "\n",
    "\n",
    "2.\n",
    "    $A^{-1}$ is symmetric since $A$ is symmetric, by, $A^{-1} = (A^T)^{-1} = (A^{-1})^T$\n",
    "\n",
    "    Zero is not an eigen value of $A$ Therefore $A x=0$ has a non-trivial solution, so, $A$ is invertible.\n",
    "\n",
    "    All of $A^{-1}$ eigen values are also positive since they are the reciprocals of the eigen values of $A$.\n",
    "    \n",
    "    Thus, $A^{-1}$ is positive definite when $A$ is positive definite.\n",
    "\n",
    "3.\n",
    "    Let's write $A$ as $Q D Q^{-1}$, $Q$ is a matrix whose columns are the eigenvectors of $A$ and $D$ is a diagonal matrix with the eigenvalues of $A$ on its diagonal.\n",
    "\n",
    "    Given the SVD, $A = U \\Sigma V^T$, $U$ consists of the eigenvectors of $A A^T$ and $V$ consists of the eigenvectors of $A^T A$, since $A$ is SPD then $A A^T = A^T A$ will have positive definite structures, so $A A^T = A^T A = U \\Sigma^2 U^T$.\n",
    "\n",
    "    The matrix $\\Sigma$ consists of the square root of the eigen values of $A$ on its diagonal.\n",
    "    \n",
    "    Therefore, the SVD is an eigenvalue decomposition.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Consider the data $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ with mean $\\boldsymbol{\\mu}_{x} \\in \\mathbb{R}^{D}$ and covariance $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{D \\times D}$.\n",
    " * Let $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{T}$ be the eigen decomposition of $\\boldsymbol{\\Sigma}_{x}$.\n",
    " * Let $\\boldsymbol{z}_{i} = \\boldsymbol{U}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$.\n",
    "\n",
    "\n",
    "### 1.4. Question\n",
    "\n",
    "Prove the following:\n",
    "\n",
    " 1. The mean of the set $\\mathcal{Z} = \\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{D} \\right\\}$ is zero, that is, $\\boldsymbol{\\mu}_{z} = \\frac{1}{N} \\sum_{i = 1}^{N} \\boldsymbol{z}_{i} = 0$.\n",
    " 2. The covariance of $\\mathcal{Z}$ is diagonal, that is, $\\boldsymbol{\\Sigma}_{z}$ is diagonal.\n",
    " 3. The pair wise distance is equal, that is, $\\forall i, j: \\; {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Solution\n",
    "\n",
    "1. The mean of the set $\\mathcal{Z} = \\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{D} \\right\\}$ is zero, that is, $\\boldsymbol{\\mu}_{z} = \\frac{1}{N} \\sum_{i = 1}^{N} \\boldsymbol{z}_{i} = 0$.\n",
    "\n",
    "First notice that\n",
    "\n",
    "$\\frac{1}{N} \\Sigma^N_{i = 1} x_i = \\mu_x \\rightarrow \\Sigma^N_{i = 1} x_i = N \\cdot \\mu_x$\n",
    "\n",
    "Thus\n",
    "\n",
    " $\\mu_z = \\frac{1}{N} \\Sigma^N_{i = 1} z_i = \\frac{1}{N} \\Sigma^N_{i = 1} U^T (x_i - \\mu_x) = \\frac{1}{N} U^T \\Sigma^N_{i = 1} (x_i - \\mu_x) = \\frac{1}{N} U^T \\Sigma^N_{i = 1} x_i - \\frac{1}{N} U^T \\Sigma^N_{i = 1} \\mu_x = \\frac{1}{N} U^T (N \\cdot \\mu_x) - \\frac{1}{N} U^T (N \\cdot \\mu_x) = U^T \\mu_x - U^T \\mu_x = 0$\n",
    "\n",
    "2. The covariance of $\\mathcal{Z}$ is diagonal, that is, $\\boldsymbol{\\Sigma}_{z}$ is diagonal.\n",
    "\n",
    "By 1. we know that $\\mu_Z = 0$, thus\n",
    "\n",
    "$\\Sigma_Z = E[(z_i - \\mu_Z)(z_i - \\mu_Z)^T] = E[z_i z_i^T] = E[(U^T (x_i - \\mu_x)) (U^T (x_i - \\mu_x))^T] = E[U^T (x_i - \\mu_x) (x_i - \\mu_x)^T U] = U^T \\Sigma_X U$\n",
    "\n",
    "Since $\\Sigma_X = U \\Lambda U^T$, and by definition $U$ is ortogonal matrix and $\\Lambda$ is diagonal matrix.\n",
    "\n",
    "$U^T \\Sigma_X U = U^T U \\Lambda U^T U = \\Lambda$\n",
    "\n",
    "So\n",
    "\n",
    "$\\Sigma_Z = \\Lambda$\n",
    "\n",
    "and $\\Sigma_Z$ is diagonal.\n",
    "\n",
    "3. The pair wise distance is equal, that is, $\\forall i, j: \\; {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}$.\n",
    "\n",
    "$||z_i - z_j||^2_2 = ||U^T (x_i - \\mu_x) - U^T (x_j - \\mu_x)||^2_2 = ||U^T (x_i - \\mu_x - x_j + \\mu_x)||^2_2 = ||U^T (x_i - x_j)||^2_2 = (U^T (x_i - x_j))^T (U^T (x_i - x_j)) = (x_i - x_j)^T U U^T (x_i - x_j)$\n",
    "\n",
    "Like 2. by definition $U$ is ortogonal matrix, thus\n",
    "\n",
    "$(x_i - x_j)^T U U^T (x_i - x_j) = (x_i - x_j)^T (x_i - x_j) = ||x_i - x_j||^2_2$\n",
    "\n",
    "by definition $||\\cdot||_2$ is positive, so\n",
    "\n",
    "$||z_i - z_j||_2 = ||x_i - x_j||_2$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}$ be a full rank matrix with $d \\leq D$. \n",
    "\n",
    " * <font color='brown'>(**#**)</font> If a matrix has rank $d$ it means its SVD  has $d$ non zero singular values.\n",
    "\n",
    "\n",
    "### 1.5. Question\n",
    "\n",
    "Show that exists an invertible matrix $\\boldsymbol{M} \\in \\mathbb{R}^{d \\times d}$ such that $\\boldsymbol{O} = \\boldsymbol{U}_{d} \\boldsymbol{M} \\in \\mathbb{R}^{D \\times d}$ is semi orthogonal, that is $\\boldsymbol{O}^{T} \\boldsymbol{O} = \\boldsymbol{I}_{d}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Solution\n",
    "\n",
    "Let the columns of $U_d=\\left[u_1, u_2 \\ldots u_d\\right]$ since it has full rank we know that its columns are linearly independent, so,\n",
    "\n",
    "$$\\forall i \\neq j: u_i . u_j = 0$$\n",
    "\n",
    "$O^T O=M^T U_d^T U_d M$\n",
    "\n",
    "Denote diagonal matrix $A$\n",
    "\n",
    "$A = U_d^T U_d = \\left\\{\\begin{array}{l}u_i \\cdot u_j = 0  \\quad i \\neq j \\\\ u_i . u_j = u_{i j}^2  i = j\\end{array} \\right.$\n",
    "\n",
    "So,\n",
    "\n",
    "$O^T O = M^T A M$\n",
    "\n",
    "Since all singular values are non-zero we can choose $M$ to be $A^{-\\frac{1}{2}}$, that is, the inverse square root of $A$ and therefore, there exists an invertible matrix $M$ such that $O^T O=I_d$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Consider the data $\\boldsymbol{X} \\in \\mathbb{R}^{D \\times N}$.\n",
    " * Assume it has zero mean $\\boldsymbol{\\mu}_{x} = \\boldsymbol{X} \\boldsymbol{1}_{N} = \\boldsymbol{0}$.\n",
    " * Its covariance is given by $\\boldsymbol{\\Sigma}_{x} = \\frac{1}{N} \\boldsymbol{X} \\boldsymbol{X}^{T}$.\n",
    "\n",
    "\n",
    "### 1.6. Question (Bonus 2 Points)\n",
    "\n",
    "Prove the following 2 optimization problem are equivalent.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}^{T} \\boldsymbol{U} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\max_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & \\operatorname{Tr} \\left( \\boldsymbol{U}_{d}^{T} \\boldsymbol{\\Sigma}_{x} \\boldsymbol{U}_{d} \\right) \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}^{T} \\boldsymbol{U} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " * <font color='brown'>(**#**)</font> Equivalent means the have the same solution, in the case above, same minimizer.\n",
    " * <font color='brown'>(**#**)</font> The above shows the equivalence between the trace form, which decorrelates the data, and the encoder decoder form, which minimized the reconstruction error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Solution\n",
    "\n",
    "\n",
    "First lets find equivalent expression for $||X - U_d U^T_d X||^2_F$ subject to $U^T U = I_d$:\n",
    "\n",
    "\n",
    "$\\frac{1}{N} ||X - U_d U^T_d X||^2_F = \\frac{1}{N} Tr((X - U_d U^T_d X)^T (X - U_d U^T_d X)) = \\frac{1}{N} Tr(X^T X - X^T U_d U^T_d X - (U^T_d X)^T U^T_d X + (U^T_d X)^T U^T_d U_d (U^T_d X)) = \\frac{1}{N} (Tr(X^T X) - Tr(X^T U_d U^T_d X) - Tr(X^T U_d U^T_d X) + Tr(X^T U_d U^T_d U_d U^T_d X)) = \\frac{1}{N} (Tr(X^T X) - 2 \\cdot Tr(X^T U_d U^T_d X) + Tr(X^T U_d U^T_d X)) = \\frac{1}{N} (Tr(X^T X) - Tr(X^T U_d U^T_d X)) = \\frac{1}{N} Tr(\\Sigma_X) - \\frac{1}{N} Tr(U^T_d X X^T U_d) = \\frac{1}{N} Tr(\\Sigma_X) - \\frac{1}{N} Tr(U^T_d \\Sigma_X U_d)$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}_{d}^{T} \\boldsymbol{U}_{d} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "=\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & Tr(\\Sigma_X) - Tr(U^T_d \\Sigma_X U_d) \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}_{d}^{T} \\boldsymbol{U}_{d} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\Sigma_X$ is constant when optimizing with respect to $U_d$, so\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & Tr(\\Sigma_X) - Tr(U^T_d \\Sigma_X U_d) \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}_{d}^{T} \\boldsymbol{U} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "=\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & - Tr(U^T_d \\Sigma_X U_d) \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}_{d}^{T} \\boldsymbol{U}_{d} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "=\n",
    "\\begin{align*}\n",
    "\\arg \\max_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & Tr(U^T_d \\Sigma_X U_d) \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}_{d}^{T} \\boldsymbol{U}_{d} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore the two objective are equivalent\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U_{d}}^{T} \\boldsymbol{U}_{d} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "=\n",
    "\\begin{align*}\n",
    "\\arg \\max_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & \\operatorname{Tr} \\left( \\boldsymbol{U}_{d}^{T} \\boldsymbol{\\Sigma}_{x} \\boldsymbol{U}_{d} \\right) \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}_{d}^{T} \\boldsymbol{U}_{d} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> Royi: <font color='green'>✓ +2 Points (Bonus)</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Consider the data $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ with mean $\\boldsymbol{\\mu}_{x} \\in \\mathbb{R}^{D}$ and covariance $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{D \\times D}$.\n",
    " * Let $\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}$ be a semi orthogonal matrix, that is, $\\boldsymbol{U}_{d}^{T} \\boldsymbol{U}_{d} = \\boldsymbol{I}$.\n",
    " * Where $\\boldsymbol{U}_{d}$ be the $d$ eigen vectors corressponding to the $d$ largest eigen values of $\\boldsymbol{\\Sigma}_{x}$.\n",
    " * Let $\\boldsymbol{z}_{i} = \\boldsymbol{U}_{d}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$.\n",
    " * Let $\\hat{\\boldsymbol{x}}_{i} = \\boldsymbol{U}_{d} \\boldsymbol{z}_{i} + \\boldsymbol{\\mu}_{x}$.\n",
    " * Let $\\boldsymbol{\\epsilon}_{i} = \\hat{\\boldsymbol{x}}_{i} - \\boldsymbol{x}_{i}$.\n",
    "\n",
    "\n",
    "### 1.7. Question\n",
    "\n",
    "Prove the following:\n",
    "\n",
    " 1. $\\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{x} \\right) = \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{z} \\right) + \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right)$.\n",
    " 2. $\\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right) = \\sum_{i = d + 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)$.\n",
    "\n",
    "Where\n",
    "\n",
    " * $\\boldsymbol{\\Sigma}_{z} \\in \\mathbb{R}^{d \\times d}$ is the covariance of $\\left\\{ \\boldsymbol{z}_{i} \\right\\}_{i = 1}^{N}$.\n",
    " * $\\boldsymbol{\\Sigma}_{\\epsilon} \\in \\mathbb{R}^{D \\times D}$ is the covariance of $\\left\\{ \\boldsymbol{\\epsilon}_{i} \\right\\}_{i = 1}^{N}$.\n",
    "\n",
    "</br>\n",
    "\n",
    " * <font color='brown'>(**#**)</font> The idea is to prove the total variance / energy is kept. Part of it in the low dimensionality data and the other is in the error (Lost information).\n",
    " * <font color='brown'>(**#**)</font> To make calculations easier, think how $\\boldsymbol{\\mu}_{x}$ effects the variance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "    $x_i = \\hat{x}_i - \\epsilon_i = U_d z_i + \\mu_x - \\epsilon_i$\n",
    "\n",
    "    $\\mu_z = U_d^T E \\left[x_i - \\mu_x \\right] = U_d^T E \\left[x_i\\right]  - \\mu_x = 0$\n",
    "\n",
    "    $\\mu_{\\epsilon} = E \\left[\\hat{x}_i - x_i \\right] = E \\left[U_d z_i + \\mu_x - x_i \\right] = E \\left[U_d z_i \\right] + \\mu_x - \\mu_x = 0$\n",
    "\n",
    "    $\\Sigma_z = E \\left[\\left(z_i - \\mu_z \\right)\\left(z_i - \\mu_z \\right)^T \\right] = E\\left[z_i z_i^T \\right] = E \\left[U_d^T \\left(x_i - \\mu_x \\right)\\left(x_i - \\mu_x \\right)^T U_d \\right] = U_d^T \\Sigma_X U_d$\n",
    "\n",
    "    $\\Sigma_{\\epsilon} = E \\left[\\left(\\epsilon_i - \\mu_{\\epsilon} \\right) \\left(\\epsilon_i - \\mu_{\\epsilon} \\right)^T \\right] = E \\left[\\epsilon_i \\epsilon_i^T\\right]$\n",
    "\n",
    "    $\\Sigma_X = E \\left[\\left(x_i - \\mu_X \\right)\\left(x_i - \\mu_X \\right)^T \\right] = E \\left[\\left(\\hat{x}i - \\epsilon_i - \\mu_X \\right) \\left(\\hat{x}i - \\epsilon_i - \\mu_X \\right)^T \\right] = E \\left[\\left(U_d z_i + \\mu_X - \\epsilon_i - \\right.\\right. \\left.\\left. \\mu_X \\right) \\left(U_d z_i + \\mu_X - \\epsilon_i - \\mu_X \\right)^T \\right] = E\\left[\\left(U_d z_i - \\epsilon_i \\right) \\left(U_d z_i - \\epsilon_i \\right)^T \\right] = E\\left[U_d z_i z_i^T U_d^T \\right] + E \\left[E_i E_i^T \\right] - 2 E \\left[U_d z_i \\epsilon_i^T \\right] = U_d \\Sigma_z U_d^T + \\Sigma_{\\epsilon} - 2 E \\left[U_d z_i \\epsilon_i^T \\right]$\n",
    "\n",
    "    Let's focus on $E \\left[U_d z_i \\epsilon_i^T \\right]$:\n",
    "\n",
    "    $E \\left[U_d z_i \\epsilon_i^T \\right] = E \\left[\\left(\\hat{x}_i - \\mu_X \\right) \\left(\\hat{x}_i - x_i \\right)^T \\right] = E\\left[U_d U_d^T \\left(x_i - \\mu_X \\right) \\left(U_d U_d^T \\left(x_i - \\mu_X \\right) + \\mu_X - x_i \\right)^T \\right] = E \\left[U_d U_d^T \\left(x_i - \\mu_X \\right) \\left(x_i - \\mu_X \\right)^T U_d U_d^T \\right] + E\\left[U_d U_d^T \\left(x_i - \\mu_X \\right) \\left(\\mu_X^T - x_i^T \\right) \\right] = U_d U_d^T \\Sigma_X U_d U_d^T - E \\left[U_d U_d^T \\left(x_i - \\mu_X \\right)\\left(x_i - \\mu_X \\right)^T \\right] = U_d U_d^T \\Sigma_X U_d U_d^T-U_d U_d^T \\Sigma_X$\n",
    "\n",
    "    Thus,\n",
    "\n",
    "    $\\Sigma_X = U_d \\Sigma_z U_d^T + \\Sigma_{\\epsilon}- 2 \\left(U_d U_d^T \\Sigma_X U_d U_d^T - U_d U_d^T \\Sigma_X \\right)$\n",
    "\n",
    "    So,\n",
    "\n",
    "    $Tr(\\Sigma_X) = Tr(U_d \\Sigma_z U_d^T) + Tr(\\Sigma_{\\epsilon}) - 2 Tr(U_d U_d^T \\Sigma_X U_d U_d^T) + Tr(U_d U^T_d \\Sigma_X) = Tr(\\Sigma_z) + Tr(\\Sigma_{\\epsilon}) - 2 Tr(U_d U_d^T U_d U_d^T \\Sigma_X) + 2 Tr(U_d U_d^T \\Sigma_X) = Tr(\\Sigma_z) + Tr(\\Sigma_{\\epsilon}) - s Tr(U_d U_d^T \\Sigma_X) + 2 Tr (U_d U_d^T \\Sigma_X) = Tr(\\Sigma_z) + Tr(\\Sigma_{\\epsilon})$\n",
    "\n",
    "2.\n",
    "    $\\Sigma_X$ is clearly a diagonalizable matrix and since we proved in 1.1 that $Tr(\\Sigma_X) = \\Sigma_{i=1}^D \\lambda_i (\\Sigma_X)$\n",
    "    \n",
    "    Following 1.7.1 we know that $\\Sigma_z = U_d^T \\Sigma_X U_d$\n",
    "    \n",
    "    So $Tr(\\Sigma_z) = Tr(U_d^T \\Sigma_X U_d)$ where $U_d^T \\Sigma_X U_d$ is a $d$ x $d$ matrix.\n",
    "    \n",
    "    Since $U_d$ are the eigen vectors corresponding to the $d$ largest eigen values of $\\Sigma_X$ we can say that $Tr(U_d^T \\Sigma_X U_d) = \\Sigma_{i=1}^d \\lambda_i (\\Sigma_X)$\n",
    "    \n",
    "    Therefore,\n",
    "    \n",
    "    $Tr(\\Sigma_E) = \\Sigma_{i=1}^D \\lambda_i (\\Sigma_X) - \\Sigma_{i=1}^d \\lambda_i (\\Sigma_X) = \\sum_{i=d+1}^D \\lambda_i (\\Sigma_X)$\n",
    "    \n",
    "---  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a square matrix.\n",
    " * Let $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\in \\mathbb{R}^{d}$ be eigen vectors with the same eigen value $\\lambda$ such that $\\boldsymbol{A} \\boldsymbol{u}_{1} = \\lambda \\boldsymbol{u}_{1}$ and $\\boldsymbol{A} \\boldsymbol{u}_{2} = \\lambda \\boldsymbol{u}_{2}$.\n",
    " * The vectors $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2}$ are perpendicular, that is, $\\left \\langle \\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\right \\rangle = 0$.\n",
    " * Let $\\boldsymbol{v} = \\alpha \\boldsymbol{u}_{1}$ where $\\alpha \\neq 0$.\n",
    "\n",
    "\n",
    "### 1.10. Question\n",
    "\n",
    " 1. Show $\\boldsymbol{v}$ is an eigen vector of $\\boldsymbol{A}$ and find its corresponding eigen value.\n",
    " 2. Prove or disprove that $\\boldsymbol{u} = \\boldsymbol{u}_{1} + \\boldsymbol{u}_{2}$ is an eigen vector of $A$.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10. Solution\n",
    "\n",
    "1. Show $\\boldsymbol{v}$ is an eigen vector of $\\boldsymbol{A}$ and find its corresponding eigen value.\n",
    "\n",
    "Notice that $\\frac{1}{\\alpha} v = u_1$ and $A u_1 = \\lambda u_1$\n",
    "\n",
    "So\n",
    "\n",
    "$A v = A (\\alpha u_1) = \\alpha A u_1 = \\alpha \\lambda u_1 = \\alpha \\lambda \\frac{1}{\\alpha} v = \\lambda v$\n",
    "\n",
    "Thus the eign value of $v$ is $\\lambda$\n",
    "\n",
    "2. Prove or disprove that $\\boldsymbol{u} = \\boldsymbol{u}_{1} + \\boldsymbol{u}_{2}$ is an eigen vector of $A$.\n",
    "\n",
    "$A u = A (u_1 + u_2) = A u_1 + A u_2 = \\lambda u_1 + \\lambda u_2 = \\lambda (u_1 + u_2) = \\lambda u$\n",
    "\n",
    "Thus the eign value of $u$ is $\\lambda$\n",
    "\n",
    "> Royi: ❌, What about the case $\\boldsymbol{u}_{1} + \\boldsymbol{u}_{2} = \\boldsymbol{0}$? You need to address it.  \n",
    "> Royi: <font color='red'>-1</font>.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a diagonalizable matrix.\n",
    " * Let $\\left\\{ \\left( \\boldsymbol{u}_{i}, {\\lambda}_{i} \\right) \\right\\}_{i = 1}^{d}$ be the set of eigen pairs, that is, $\\boldsymbol{A} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$.\n",
    " * Let $\\boldsymbol{B} = \\boldsymbol{R} \\boldsymbol{A} \\boldsymbol{R}^{T}$ where $\\boldsymbol{R} \\in \\mathbb{R}^{d \\times d}$ is an orthogonal matrix, namely, $\\boldsymbol{R}^{T} \\boldsymbol{R} = \\boldsymbol{R} \\boldsymbol{R}^{T} = \\boldsymbol{I}$.\n",
    "\n",
    "\n",
    "### 1.11. Question\n",
    "\n",
    "Find the set $\\left\\{ \\left( \\boldsymbol{v}_{i}, {\\alpha}_{i} \\right) \\right\\}_{i = 1}^{d}$ of eigen pairs of $\\boldsymbol{B}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11. Solution\n",
    "\n",
    "Lets denote $v_i = R u_i$, we know that $R$ is ortogonal so $u_i = R^T v_i$\n",
    "\n",
    "$B v_i = R A R^T v_i = R A R^T R u_i$\n",
    "\n",
    "$R$ is ortogonal matrix so\n",
    "\n",
    "$R A R^T R u_i = R A u_i = R (\\lambda u_i) = \\lambda R u_i = \\lambda R R^T v_i = \\lambda v_i$\n",
    "\n",
    "So\n",
    "\n",
    "$\\forall v_i, B v_i = \\lambda v_i$\n",
    "\n",
    "Thus the corresponding eigen value $\\alpha_i$ for every eigen vector $v_i$ is $\\lambda_i$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Consider the data $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{10} \\right\\}_{i = 1}^{N}$ and its matrix form $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times N}$.\n",
    " * Let $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{10 \\times 10}$ be the covariance matrix where all of its eigen values are unique.\n",
    " * Let $\\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{3} \\right\\}_{i = 1}^{N}$ be the low dimensional representation obtained by applying PCA from $\\mathbb{R}^{10}$ to $\\mathbb{R}^{3}$\n",
    " * Let $\\left\\{ \\boldsymbol{w}_{i} \\in \\mathbb{R}^{2} \\right\\}_{i = 1}^{N}$ be the low dimensional representation obtained by applying PCA from $\\mathbb{R}^{10}$ to $\\mathbb{R}^{2}$\n",
    "\n",
    "\n",
    "### 1.12. Question\n",
    "\n",
    "Prove or disprove:\n",
    "\n",
    "$$ \\boldsymbol{w}_{i} = \\begin{bmatrix}\n",
    "{\\alpha}_{1} & 0 & 0 \\\\ \n",
    "0 & {\\alpha}_{2} & 0\n",
    "\\end{bmatrix} \\boldsymbol{z}_{i}, \\; {\\alpha}_{1}, {\\alpha}_{2} \\in \\left\\{ -1, 1 \\right\\} $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12. Solution\n",
    "\n",
    "Lets denote $w_i = U^T_{d_2} (x_i - \\mu_x)$ and $z_i = U^T_{d_3} (x_i - \\mu_x)$.\n",
    "\n",
    "As we learned by PCA, $U_{d_2}$, $U_{d_3}$ are the same part of $U_d$ matrix.\n",
    "\n",
    "Therefore, by denoting $\\alpha_1, \\alpha_2 = 1$ we essentially choose the fiest two rows of $z_i$ which corresponds to $w_i$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$ \\boldsymbol{w}_{i} = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\ \n",
    "0 & 1 & 0\n",
    "\\end{bmatrix} \\boldsymbol{z}_{i}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel Principle Component Analysis (K-PCA)\n",
    "\n",
    "Let $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T} \\in \\mathbb{R}^{N \\times N}$ be the centering matrix.\n",
    "\n",
    "### 2.1. Question\n",
    "\n",
    "Prove that $\\boldsymbol{J}$ is an idempotent matrix, that is, $\\boldsymbol{J}^{k} = \\boldsymbol{J}$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> This implies that if we center the data multiple times it will have no effect beyond doing it once."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Solution\n",
    "\n",
    "As we prove in Exercise 1 symetric matrix is idempotent matrix\n",
    "\n",
    "So first let prove that $\\frac{1}{N} 1 1^T$ is symetric\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\left[\\begin{array}{ccc}\n",
    "\\frac{1}{N} & \\cdots & \\frac{1}{N} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{1}{N} & \\cdots & \\frac{1}{N}\n",
    "\\end{array}\\right]_{N x N}\n",
    "$$\n",
    "\n",
    "It is clear that A and I are symetrics, so they are also idempotent.\n",
    "\n",
    "We will show that $J$ is also idempotent\n",
    "\n",
    "$J^2 = (I - A)^2 = I^2 - 2 I A + A^2 = I - 2 A + A = I - A = J$\n",
    "\n",
    "$J^2 = J$, thus $J$ is idempotent. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{X} \\in \\mathbb{R}^{D \\times N}$.\n",
    " * The covariance matrix $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{X} \\boldsymbol{X}^{T} \\in \\mathbb{R}^{D \\times D}$.\n",
    " * The matrix $\\boldsymbol{K}_{x} = \\boldsymbol{X}^{T} \\boldsymbol{X} \\in \\mathbb{R}^{N \\times N}$.\n",
    " * Let $\\left( \\boldsymbol{u}_{i}, {\\lambda}_{i} > 0 \\right)$ be an eigen pair of $\\boldsymbol{\\Sigma}_{x}$, such that, $\\boldsymbol{\\Sigma}_{x} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$.\n",
    "\n",
    "\n",
    "### 2.2. Question\n",
    "\n",
    " 1. Show that ${\\lambda}_{i}$ is an eigen value of $\\boldsymbol{K}_{x}$.\n",
    " 2. Find the corresponding eigen vector $\\boldsymbol{v}_{i}$ such that $\\boldsymbol{K}_{x} \\boldsymbol{v}_{i} = {\\lambda}_{i} \\boldsymbol{v}_{i}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Solution\n",
    "\n",
    "1. Show that ${\\lambda}_{i}$ is an eigen value of $\\boldsymbol{K}_{x}$.\n",
    "\n",
    "Notice that\n",
    "\n",
    "$K_X = X^T X$\n",
    "\n",
    "Multiply by $X^T$ on the right\n",
    "\n",
    "$K_x X^T = X^T X X^T = X^T \\Sigma_X$\n",
    "\n",
    "multiply by $u_i$ on the right\n",
    "\n",
    "$K_x X^T u_i = X^T \\Sigma_X u_i = X^T \\lambda_i u_i = \\lambda_i X^T u_i$\n",
    "\n",
    "So\n",
    "\n",
    "$K_x (X^T u_i) = \\lambda_i (X^T u_i)$\n",
    "\n",
    "Thus\n",
    "\n",
    "$\\lambda_i$ is an eigen value and $(X^T u_i)$ is the corresponding eigen vector of $K_X$\n",
    "\n",
    "2. Find the corresponding eigen vector $\\boldsymbol{v}_{i}$ such that $\\boldsymbol{K}_{x} \\boldsymbol{v}_{i} = {\\lambda}_{i} \\boldsymbol{v}_{i}$.\n",
    "\n",
    "Let $v_i = X^T u_i$\n",
    "\n",
    "As we showed before\n",
    "\n",
    "$K_X v_i = K_x (X^T u_i) = \\lambda_i (X^T u_i) = \\lambda_i v_i$\n",
    "\n",
    "Thus the corresponding eigen vector is $(X^T u_i)$, so $v_i = (X^T u_i)$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Kernel Functions**\n",
    " \n",
    " * Let $k: \\mathbb{R}^{d} \\times \\mathbb{R}^{d} \\to \\mathbb{R}$.\n",
    " * Consider $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{N}$.\n",
    "\n",
    "### 2.3. Question\n",
    "\n",
    "Show that if $k \\left( \\cdot, \\cdot \\right)$ can be written as inner product:\n",
    "\n",
    "$$ k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\left \\langle \\phi \\left( \\boldsymbol{x}_{i} \\right), \\phi \\left( \\boldsymbol{x}_{j} \\right) \\right \\rangle $$\n",
    "\n",
    "for some $\\phi : \\mathbb{R}^{d} \\to \\mathbb{R}^{M} $ then the matrix defined by $\\boldsymbol{K} \\left[ i, j \\right] = k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) $ is _Symmetric Positive Semi Definite_ (SPSD), that is, $\\boldsymbol{K} \\succeq 0$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Solution\n",
    "\n",
    "First we will show that $K$ is symmetric\n",
    "\n",
    "$K_{ij} = k(x_i, x_j) = <\\phi(x_i), \\phi(x_j)> = <\\phi(x_j), \\phi(x_i) = k(x_j, x_i)> = K_{ji}$\n",
    "\n",
    "So $K$ is symmetric\n",
    "\n",
    "Second we will show that $\\forall u \\in R^n, u^T K u \\geq 0$\n",
    "\n",
    "$u^T K u = \\Sigma^d_{i = 1} \\Sigma^d_{j = 1} u^T_i u_j K_{ij} = \\Sigma^d_{i = 1} \\Sigma^d_{j = 1} u^T_i u_j <\\phi(x_i), \\phi(x_j)> = \\Sigma^d_{i = 1} u^T_i \\Sigma^d_{j = 1} u_j <\\phi(x_i), \\phi(x_j)> = \\Sigma^d_{i = 1} u^T_i \\Sigma^d_{j = 1} <\\phi(x_i), u_j \\phi(x_j)> = \\Sigma^d_{i = 1} u^T_i <\\phi(x_i),\\Sigma^d_{j = 1} (u_j \\phi(x_j))> = \\Sigma^d_{i = 1} <u^T_i \\phi(x_i),\\Sigma^d_{j = 1} (u_j \\phi(x_j))> = <\\Sigma^d_{i = 1}  (u^T_i \\phi(x_i)),\\Sigma^d_{j = 1} (u_j \\phi(x_j))> = ||\\Sigma^d_{i = 1}  (u^T_i \\phi(x_i))||^2$\n",
    "\n",
    "The expression is satisfy\n",
    "\n",
    "$||\\Sigma^d_{i = 1}  (u^T_i \\phi(x_i))||^2 \\geq 0$\n",
    "\n",
    "So\n",
    "\n",
    "$u^T K u \\geq 0$\n",
    "\n",
    "Thus the matrix $K$ is SPSD\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{A} \\succ 0 $ be an SPD matrix.\n",
    " * Let $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\boldsymbol{x}_{i}^{T} \\boldsymbol{A} \\boldsymbol{x}_{j} $.\n",
    "\n",
    "### 2.4. Question\n",
    "\n",
    "Prove or disprove that $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ is a kernel function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Solution\n",
    "\n",
    "Since $A$ is SPD, it is also diagonalizable, so it can be rewritten by $A = Q D Q^T$\n",
    "\n",
    "And $A$ SPD so $A$ is symmetric, thus\n",
    "\n",
    "$A = Q D Q^T = Q^T D Q$\n",
    "\n",
    "So\n",
    "\n",
    "$k(x_i, x_j) = x^T_i A x_j = x^T_i Q D Q^T x_j = x^T_i Q^T D Q x_j = (Q x_i)^T D Q x_j = <\\sqrt{D} Q x_i, \\sqrt{D} Q x_j>$\n",
    "\n",
    "Thus, we found a feature mapping function $\\phi (x_i) = \\sqrt{D} Q x_i$\n",
    "\n",
    "$k(x_i, x_j) = <\\sqrt{D} Q x_i, \\sqrt{D} Q x_j> = <\\phi (x_i), \\phi (x_j)>$\n",
    "\n",
    "And by Mercer's theorem we may say that $k(x_i, x_j)$ is a kernel function\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let $k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left( 1 + \\boldsymbol{x}^{T} \\boldsymbol{y} \\right)}^{2}$.\n",
    "\n",
    "### 2.5. Question\n",
    "\n",
    "Prove or disprove that $k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right)$ is a kernel function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Solution\n",
    "\n",
    "$k(x, y) = (1 + x^T y)^2 = 1 + 2 x^T y + (x^T y)^T (x^T y) = 1 + 2 x^T y + y^T x x^T y = 1 + 2 x^T y + y^T x y^T x = 1 + 2 x^T y + y^T y x^T x = 1 + 2 x^T y + x^T x y^T y$\n",
    "\n",
    "So we can express $k(x, y)$ as $<\\phi(x), \\phi(y)>$ where feature map function\n",
    "\n",
    "$$\\phi(z) = \\begin{bmatrix}\n",
    "1 \\\\ \n",
    "\\sqrt{2} z \\\\\n",
    "z^T z\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Thus by Mercer's theorem we can conclude that $k(x, y)$ is a kernel function\n",
    "\n",
    "> Royi: ❌, What is the kernel function? You did nice but not the required step.  \n",
    "> Royi: <font color='red'>-2</font>.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " * Let $\\boldsymbol{K}_{x} \\left[ i, j \\right] = k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ be an **SPD** kernel matrix.\n",
    " * Let $\\tilde{\\boldsymbol{K}}_{x} = \\boldsymbol{J} \\boldsymbol{K}_{x} \\boldsymbol{J}$ be a centered kernel matrix where $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$.\n",
    "\n",
    "### 2.7. Question\n",
    "\n",
    "Prove or disprove: $\\tilde{\\boldsymbol{K}}_{x}$ is an SPD matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Solution\n",
    "\n",
    "Assume by contradiction that $K_x$ is SPD, that is, $\\forall x \\in R^N \\backslash \\{0\\}, x^T K_x x > 0$.\n",
    "\n",
    "For simplicity, let's assume $N = 1$. Let $x = [1] \\in R^1 \\backslash \\{0\\}$, since $N = 1 \\rightarrow J = [0]$ which means $x^T K_x x = x^T J K_x J x = 0$\n",
    "\n",
    "Therefore, there exists a matrix $J$ and a vector $x$ such that $x^T K_x x = 0$.\n",
    "\n",
    "Contradiction.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out of Sample Extension**\n",
    "\n",
    " * Consider the training set $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$.\n",
    " * Let $\\boldsymbol{K}_{x}$ be the kernel matrix obtained by applying the kernel on the training set.\n",
    " * Let $\\boldsymbol{J} \\boldsymbol{K} \\boldsymbol{K} = \\boldsymbol{V} \\boldsymbol{\\Sigma}^{2} \\boldsymbol{V}^{T}$ be the eigen decomposition of the centered kernel matrix.\n",
    " * Let $\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}$ be the low dimensional representation obtained by applying K-PCA, that is, $\\boldsymbol{Z} = \\boldsymbol{\\Sigma}_{d} \\boldsymbol{V}_{d}^{T}$.\n",
    "\n",
    "</br>\n",
    "\n",
    " * <font color='brown'>(**#**)</font> Kernel matrix is also called _Gram Matrix_.\n",
    "\n",
    "### 2.8. Question (Bonus 2 Points)\n",
    "\n",
    "Let $\\boldsymbol{X}^{\\star} \\in \\mathbb{R}^{D \\times M}$ be a set of a new unseen data points. Write an expression, in a matrix form, for $\\boldsymbol{Z}^{\\star} \\in \\mathbb{R}^{d \\times M}$, the K-PCA out of sample extension applied to $\\boldsymbol{X}^{\\star}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Solution\n",
    "\n",
    "To obtain an out-of-sample extension we proceed as in K-PCA: center the new data with respect to the training set, and then apply the projection matrix.\n",
    "\n",
    "The transformation is explicitly unknown, but we can use Kernels to our advantage.\n",
    "\n",
    "Let $\\Phi\\left(X^*\\right)=\\left[\\phi\\left(x_1^*\\right) \\phi\\left(x_2^*\\right) \\ldots \\phi\\left(x_M^*\\right)\\right]^T \\in R^{M x D} ; K_{x^*}=\\left[k_\\alpha\\left(x_i^*, x_j\\right)\\right]$\n",
    "\n",
    "Let $\\tilde{\\phi}\\left(x_i^*\\right)=\\phi\\left(x_i^*\\right)-\\frac{1}{N} \\Sigma_{i=1}^N \\phi\\left(x_i\\right)$ to obtain the matrix $\\tilde{\\Phi}\\left(X^*\\right)=\\left[\\tilde{\\phi}\\left(x_1^*\\right) \\tilde{\\phi}\\left(x_2^*\\right) \\ldots \\widetilde{\\phi}\\left(x_M^*\\right)\\right]^T$\n",
    "\n",
    "By applying the transformation, we get $\\tilde{\\Phi}\\left(X^*\\right) V_{D x k}=\\tilde{\\Phi}\\left(X^*\\right)\\left(\\tilde{\\Phi}^{\\top} \\tilde{\\Phi} V_{D x k} \\Sigma_{k x k}^{-2}\\right)$.\n",
    "\n",
    "Let $\\tilde{K}_{x^*}=\\left[\\tilde{\\phi}\\left(x_i^*\\right)^T \\tilde{\\phi}\\left(x_j\\right)\\right] \\in R^{M x n} \\rightarrow \\tilde{\\Phi}\\left(X^*\\right) V_{D x k}=Z^*=\\tilde{K}_{x^{-}} U_{N x k} \\Sigma_{k x k k}^{-1}$ $J_{M X N}^2 K_\\alpha 1_{N X N}$\n",
    "\n",
    "> Royi: <font color='green'>✓ +2 Points (Bonus)</font>. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
