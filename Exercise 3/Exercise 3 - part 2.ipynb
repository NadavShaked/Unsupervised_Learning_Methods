{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 003 - Part II\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 23/05/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0002Part002.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:07.453435Z",
     "start_time": "2023-06-17T21:42:07.202842Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_openml, load_breast_cancer, load_digits, load_iris, load_wine\n",
    "\n",
    "# Computer Vision\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import math\n",
    "from platform import python_version\n",
    "import random\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:07.472918Z",
     "start_time": "2023-06-17T21:42:07.452460Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "%matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:07.490017Z",
     "start_time": "2023-06-17T21:42:07.474237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FILE_URL   = r'None'\n",
    "DATA_FILE_NAME  = r'None'\n",
    "\n",
    "T_MNIST_IMG_SIZE = (28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:07.490440Z",
     "start_time": "2023-06-17T21:42:07.482197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def BalancedSubSample( dfX: pd.DataFrame, colName: str, numSamples: int ):\n",
    "    \n",
    "    # TODO: Validate the number of samples\n",
    "    # TODO: Validate the column name (Existence and categorical values)\n",
    "    return dfX.groupby(colName, as_index = False, group_keys = False).apply(lambda dfS: dfS.sample(numSamples, replace = False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
    " - Answer all questions / tasks within the Jupyter Notebook.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Verify the rendering on VS Code.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for questions.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The `Import Packages` section above imports most needed tools to apply the work. Please use it.\n",
    "* <font color='brown'>(**#**)</font> You may replace the suggested functions to use with functions from other packages.\n",
    "* <font color='brown'>(**#**)</font> Whatever not said explicitly to implement maybe used by a 3rd party packages.\n",
    "* <font color='brown'>(**#**)</font> The total run time of this notebook must be **lower than 60 [Sec]**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "- Nadav_Talmon_203663950\n",
    "- Nadav_Shaked_312494925\n",
    "- Adi_Rosenthal_316550797"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:07.490547Z",
     "start_time": "2023-06-17T21:42:07.486935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download Data\n",
    "# This section downloads data from the given URL if needed.\n",
    "\n",
    "if (DATA_FILE_NAME != 'None') and (not os.path.exists(DATA_FILE_NAME)):\n",
    "    urllib.request.urlretrieve(DATA_FILE_URL, DATA_FILE_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA\n",
    "\n",
    "### 3.1. PCA Algorithm\n",
    "\n",
    "In this section we'll implement a SciKit Learn API compatible class for the PCA.  \n",
    "The class should implement the following methods:\n",
    "\n",
    "1. `__init____()` - The object constructor by the encoder dimension.  \n",
    "2. `fit()` - Given a data set builds the encoder / decoder.  \n",
    "3. `transform()` - Applies the encoding on the input data.  \n",
    "4. `inverse_transform()` - Applies the decoding on the input data.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> You may use the [SciKit Learn's PCA module](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) as a reference.\n",
    "* <font color='brown'>(**#**)</font> Both encoding and decoding applied as out of sample encoding / decoding.\n",
    "* <font color='brown'>(**#**)</font> Pay attention to data structure (`N x D`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:07.502226Z",
     "start_time": "2023-06-17T21:42:07.499199Z"
    }
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, d: int = 2):\n",
    "        '''\n",
    "        Constructing the object.\n",
    "        Args:\n",
    "            d - Number of dimensions of the encoder output.\n",
    "        '''\n",
    "        # ===========================Fill This===========================#\n",
    "        # 1. Keep the model parameters.\n",
    "\n",
    "        self.d = d\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        # ===============================================================#\n",
    "\n",
    "    def fit(self, mX: np.ndarray):\n",
    "        '''\n",
    "        Fitting model parameters to the input.\n",
    "        Args:\n",
    "            mX - Input data with shape N x D.\n",
    "        Output:\n",
    "            self\n",
    "        '''\n",
    "        # ===========================Fill This===========================#\n",
    "        # 1. Build the model encoder.\n",
    "        # 2. Build the model decoder.\n",
    "        # 3. Optimize calculation by the dimensions of `mX`.\n",
    "        # !! You may find `scipy.sparse.linalg.svds()` useful.\n",
    "        # !! You may find `scipy.sparse.linalg.eigsh()` useful.\n",
    "\n",
    "        N, D = mX.shape\n",
    "\n",
    "        vMean = np.mean(mX, axis=0).reshape(1, -1)\n",
    "        mX_centers = mX - vMean\n",
    "\n",
    "        if D <= N:\n",
    "            eigenvalues, eigenvectors = sp.sparse.linalg.eigsh(mX_centers.T @ mX_centers, k=self.d)\n",
    "            mUd = eigenvectors[:, np.argsort(eigenvalues)[::-1]]\n",
    "        else:\n",
    "            v_d, Σ_x_power2, _ = sp.sparse.linalg.svds(mX_centers @ mX_centers.T, k=self.d)\n",
    "            eigenvectors = (mX_centers.T @ v_d @ (np.linalg.inv(np.sqrt(np.diag(Σ_x_power2)))))\n",
    "            mUd = eigenvectors[:, np.argsort(Σ_x_power2)[::-1]]\n",
    "\n",
    "        self.encode = lambda mX: (mX - vMean) @ mUd\n",
    "        self.decode = lambda mZ: mZ @ mUd.T + vMean\n",
    "\n",
    "        # ===============================================================#\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, mX: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Applies (Out of sample) encoding\n",
    "        Args:\n",
    "            mX - Input data with shape N x D.\n",
    "        Output:\n",
    "            mZ - Low dimensional representation (embeddings) with shape N x d.\n",
    "        '''\n",
    "        # ===========================Fill This===========================#\n",
    "        # 1. Encode data using the model encoder.\n",
    "\n",
    "        mZ = self.encode(mX)\n",
    "\n",
    "        # ===============================================================#\n",
    "\n",
    "        return mZ\n",
    "\n",
    "    def inverse_transform(self, mZ: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Applies (Out of sample) decoding\n",
    "        Args:\n",
    "            mZ - Low dimensional representation (embeddings) with shape N x d.\n",
    "        Output:\n",
    "            mX - Reconstructed data with shape N x D.\n",
    "        '''\n",
    "        # ===========================Fill This===========================#\n",
    "        # 1. Encode data using the model decoder.\n",
    "\n",
    "        mX = self.decode(mZ)\n",
    "\n",
    "        # ===============================================================#\n",
    "\n",
    "        return mX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> In the class we use _out of sample_ encoding / decoding. What if we use the same `mX` for training and the encoding?  \n",
    "Make sure to understand this before proceeding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. PCA Application\n",
    "\n",
    "In this section the PCA (Using the above class) will be applied on several data sets:\n",
    "\n",
    " * Breast Cancer Dataset - Loaded using `load_breast_cancer()`.\n",
    " * Digits Dataset - Loaded using `load_digits()`.\n",
    " * Iris Dataset - Loaded using `load_iris()`.\n",
    " * Wine Dataset - Loaded using `load_wine()`.\n",
    "\n",
    "For each data set:\n",
    "\n",
    "1. Make yourself familiar with the data set:\n",
    "    * How many features are there ($D$).\n",
    "    * How many samples are there ($D$).\n",
    "    * Do all features have the same unit?\n",
    "2. Apply a Pre Process Step  \n",
    "   In ML, usually, if the features do not have the same unit they are normalized.  \n",
    "   Namely, make each feature with zero mean and unit standard deviation.   \n",
    "   Write a function to normalize input data.\n",
    "3. Apply the PCA  \n",
    "   Set `d` to be visualization friendly and apply PCA from $D$ to $d$.  \n",
    "   The obtained the low dimensional data represents $\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}$.\n",
    "4. Plot Low Dimensional Data  \n",
    "   Make a scatter plot of $\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}$ and color the data points according to the data labels.  \n",
    "   For each data set show result with the normalization step and without it.\n",
    "5. Calculate Lost Energy  \n",
    "   For each plot, show the value of ${\\left\\| \\tilde{\\boldsymbol{X}} - \\boldsymbol{X} \\right\\|}_{F}^{2}$.  \n",
    "   Do this by applying `inverse_transform()` on the low dimensional data and calculate the norm.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Pay attention to the difference in dimensions of the data to the derived Math formulations.\n",
    "* <font color='brown'>(**#**)</font> The output should be 2 figures for each data set. You may show them in a single plot using sub plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:07.508094Z",
     "start_time": "2023-06-17T21:42:07.501902Z"
    }
   },
   "outputs": [],
   "source": [
    "def NormalizeData(mX: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Normalize data so each feature has zero mean and unit standard deviation.\n",
    "    Args:\n",
    "        mX  - Input data with shape N x d.\n",
    "    Output:\n",
    "        mY  - Output data with shape N x d.\n",
    "    Remarks:\n",
    "        - Features with zero standard deviation are not scaled (Only centered).\n",
    "    '''\n",
    "\n",
    "    stds = np.std(mX, axis=0, ddof=0)\n",
    "    stds[stds == 0] = 1\n",
    "    mean = np.mean(mX, axis=0)\n",
    "    mY = (mX - mean) / stds\n",
    "\n",
    "    return mY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:10.335913Z",
     "start_time": "2023-06-17T21:42:07.508495Z"
    }
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Set parameter `d`.\n",
    "# 2. Load each data set.\n",
    "# 3. Apply PCA to each data set with and without normalization.\n",
    "# 4. Display results as scatter data.\n",
    "\n",
    "def apply_pca(data, d, data_name):\n",
    "    mX = data.data\n",
    "    mX_normalized = NormalizeData(mX)\n",
    "\n",
    "    pca_norm = PCA(d=d)\n",
    "    pca_norm.fit(mX_normalized)\n",
    "    mZ_norm = pca_norm.transform(mX_normalized)\n",
    "    mX_reconstructed_norm = pca_norm.inverse_transform(mZ_norm)\n",
    "    lost_energy_norm = np.linalg.norm(mX_reconstructed_norm - mX_normalized, ord='fro') ** 2\n",
    "\n",
    "    pca_no_norm = PCA(d=d)\n",
    "    pca_no_norm.fit(mX)\n",
    "    mZ_no_norm = pca_no_norm.transform(mX)\n",
    "    mX_reconstructed = pca_no_norm.inverse_transform(mZ_no_norm)\n",
    "    lost_energy_no_norm = np.linalg.norm(mX_reconstructed - mX, ord='fro') ** 2\n",
    "\n",
    "    # Plot the low-dimensional data\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    fig.suptitle(f\"dataset: {data_name}\")\n",
    "\n",
    "    # Without normalization\n",
    "    axes[0].scatter(mZ_norm[:, 0], mZ_norm[:, 1], c=data.target)\n",
    "    axes[0].set_title(f'With Normalization, Lost Energy: {lost_energy_norm}')\n",
    "    axes[0].set_xlabel('PC1')\n",
    "    axes[0].set_ylabel('PC2')\n",
    "\n",
    "    # With normalization\n",
    "    axes[1].scatter(mZ_no_norm[:, 0], mZ_no_norm[:, 1], c=data.target)\n",
    "    axes[1].set_title(f'Without Normalization, Lost Energy: {lost_energy_no_norm}')\n",
    "    axes[1].set_xlabel('PC1')\n",
    "    axes[1].set_ylabel('PC2')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "d = 2\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "digits = load_digits()\n",
    "iris = load_iris()\n",
    "wine = load_wine()\n",
    "\n",
    "# Apply PCA to each data set with and without normalization\n",
    "apply_pca(breast_cancer, d, \"breast_cancer\")\n",
    "apply_pca(digits, d, \"digits\")\n",
    "apply_pca(iris, d, \"iris\")\n",
    "apply_pca(wine, d, \"wine\")\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Question\n",
    "\n",
    "In the above, why does the results of the normalized and non normalized data are different?  \n",
    "Address the geometry of the results and the value of the reconstruction error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Solution\n",
    "\n",
    "Normalization ensures that each feature contributes equally to PCA by equalizing their contributions through centering and scaling. This allows PCA to accurately identify the directions of maximum variance in the data.\n",
    "\n",
    "When data is not normalized, features with larger variances can overshadow others, leading to distorted results and a biased representation of the data's geometry.\n",
    "\n",
    "Additionally, normalization reduces the reconstruction error in PCA. Normalized data allows the PCA model to reconstruct the original data in a better way since it captures the dominant patterns more accurately. In contrast, non-normalized data with varying scales and means can result in higher reconstruction errors because the model may struggle to represent the original data structure effectively.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Denoising\n",
    "\n",
    "In this section the PCA algorithm will be used for denoising images from the [MNIST Dataset](https://en.wikipedia.org/wiki/MNIST_database).  \n",
    "In this section:\n",
    "\n",
    " 1. Load Data  \n",
    "    Load the MNIST data set and sub sample it.  \n",
    "    We'll have a perfectly balanced data set.\n",
    "    The data will be in `mX` and labels in `vY`.  \n",
    "    This is already implemented.\n",
    " 2. Add Noise  \n",
    "    We'll add noise to the data.  \n",
    "    The noise of the data will be modeled as a Poisson Noise (Also known as [_Shot Noise_](https://en.wikipedia.org/wiki/Shot_noise)).  \n",
    "    The _Shot Noise_ is a classic model of noise gathered by imaging sensors.\n",
    " 3. Analyze the Data  \n",
    "    Analyze the spectrum of the data and choose an appropriate ste of parameters for denoising.\n",
    " 3. Apply Denoising  \n",
    "    Apply denoising by utilizing the PCA algorithm.\n",
    " 4. Analyze Result  \n",
    "    Show the results as a function of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:10.336210Z",
     "start_time": "2023-06-17T21:42:10.335642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "numSamplesClass = 600\n",
    "λ               = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:52.931738Z",
     "start_time": "2023-06-17T21:42:10.336115Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dfX, dfY = fetch_openml(name = 'mnist_784', version = 1, return_X_y = True, as_frame = True)#, parser = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:53.714804Z",
     "start_time": "2023-06-17T21:42:52.935335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sub Sample Data\n",
    "dfData = pd.concat((dfX, dfY), axis = 1)\n",
    "\n",
    "# Balanced Sub Sample\n",
    "# End Result: 'numSamplesClass' samples per digit\n",
    "dfData = BalancedSubSample(dfData, 'class', numSamplesClass)\n",
    "vY = dfData['class'].to_numpy(dtype = np.uint8)\n",
    "mX = dfData.drop(columns = ['class']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:54.064280Z",
     "start_time": "2023-06-17T21:42:53.716577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Poisson Noise\n",
    "mN = np.random.poisson(λ, size = mX.shape) #<! Noise samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:54.143068Z",
     "start_time": "2023-06-17T21:42:54.066832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Noise\n",
    "# Make sure values are in {0, 1, 2, ..., 255} range\n",
    "mXRef = mX.copy() #<! Reference with no noise\n",
    "mXRef = mXRef / 255\n",
    "\n",
    "mX += mN\n",
    "mX = np.minimum(mX, 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:54.907896Z",
     "start_time": "2023-06-17T21:42:54.143514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show Samples\n",
    "\n",
    "lIdx = [np.flatnonzero(vY == ii)[0] for ii in range(10)]\n",
    "\n",
    "_, mHA = plt.subplots(1, 10, figsize = (16, 4))\n",
    "for ii, hA in enumerate(mHA.flat):\n",
    "    idx = lIdx[ii]\n",
    "    mI  = np.reshape(mX[idx], T_MNIST_IMG_SIZE)\n",
    "    # mI  = np.clip(mI, 0, 1)\n",
    "    hA.imshow(mI, cmap = 'gray')\n",
    "    hA.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. The Data Spectrum\n",
    "\n",
    "In this section:\n",
    "\n",
    " 1. Pre Process the data (Optional).  \n",
    "    Do this step if oyu think it is needed.\n",
    " 2. Plot the Spectrum of the Eigen Values of the data.\n",
    " 3. Choose **a range** (5 values) of `d` for the low dimensionality reduction.\n",
    " 4. For each `d` value, calculate the **relative energy loss**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:54.910774Z",
     "start_time": "2023-06-17T21:42:54.906948Z"
    }
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Pre Process Data (Optional).\n",
    "# !! Make sure to keep the name of the data `mX`.\n",
    "# !! Don't change the order of the data so it matches `vY`.\n",
    "\n",
    "mX = mX / 255\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:55.152124Z",
     "start_time": "2023-06-17T21:42:54.912335Z"
    }
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the spectrum of the Eigen Values of the data.\n",
    "\n",
    "eigenvalues = np.linalg.eigvals(np.cov(mX.T))\n",
    "eigenvalues_norm = eigenvalues / np.sum(eigenvalues)\n",
    "sorted_eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "energy = [eigenvalues_norm[0]]\n",
    "for e in range(len(eigenvalues_norm) - 1):\n",
    "    energy.append(energy[-1] + eigenvalues_norm[e + 1])\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:55.629075Z",
     "start_time": "2023-06-17T21:42:55.159170Z"
    }
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Display the Spectrum.\n",
    "# !! You may show both the spectrum and the relative energy.\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(sorted_eigenvalues, marker='o', linestyle='-', color='b', label=\"spectrum\")\n",
    "plt.xlabel('Eigenvalue Index')\n",
    "plt.ylabel('Normalized Eigenvalue')\n",
    "plt.title('Spectrum of Eigenvalues')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(energy, marker='+', linestyle='-', color='g', label=\"relative energy\")\n",
    "plt.xlabel('Eigenvalue Index')\n",
    "plt.ylabel('relative energy')\n",
    "plt.title('relative energy of Eigenvalues')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:55.899282Z",
     "start_time": "2023-06-17T21:42:55.669089Z"
    }
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Choose a range of `d` values.\n",
    "# 2. Per `d` plot / display the relative energy loss.\n",
    "# !! Don't choose too many, keep running time and visualization reasonable.\n",
    "# !! The choice should be in order to show the effect of `d` on the results and not only the optimal `d`.\n",
    "\n",
    "d_values = [5, 20, 50, 100, 200, 400]\n",
    "relative_energy_loss = []\n",
    "\n",
    "for d in d_values:\n",
    "    energy_loss = np.sum(sorted_eigenvalues[d:]) / np.sum(sorted_eigenvalues)\n",
    "    relative_energy_loss.append(energy_loss)\n",
    "\n",
    "# Display the relative energy loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(d_values, relative_energy_loss, marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('Relative Energy Loss')\n",
    "plt.title('Relative Energy Loss for Different d Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. PCA Based Denoising\n",
    "\n",
    "In this section, per `d` value:\n",
    "\n",
    " 1. Build the _Encoder_ and _Decoder_. \n",
    " 2. Denoise the images listed in the index list `lIdx`.\n",
    " 3. Show results per `d`\n",
    "      * For each image show the reconstruction error vs. the noisy sample (`mX`).\n",
    "      * For each image show the estimation error vs. the non noisy sample (`mXRef`).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Make sure when you use the whole data (`mX`) and when the sub set to analyze.\n",
    "* <font color='brown'>(**#**)</font> For the PCA you may only use `mX`.\n",
    "* <font color='brown'>(**#**)</font> The output should be the 10 images per row where the number of rows is the number of `d` values + 2 (For the reference / noisy images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:42:56.990552Z",
     "start_time": "2023-06-17T21:42:55.900900Z"
    }
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Build the encoder / decoder using the `PCA` class above.\n",
    "# 2. Per `d` denoise the images in `lIdx`.\n",
    "# !! Only use `mX` for the PCA step.\n",
    "\n",
    "denoised_images = []\n",
    "reconstruction_errors = []\n",
    "estimation_errors = []\n",
    "\n",
    "for d in d_values:\n",
    "    pca = PCA(d=d)\n",
    "    pca.fit(mX)\n",
    "    mX_lIdx = mX[lIdx]\n",
    "    mZ = pca.transform(mX_lIdx)\n",
    "    mX_reconstructed = pca.inverse_transform(mZ)\n",
    "    denoised_images.append(mX_reconstructed.copy())\n",
    "    reconstruction_error = np.linalg.norm(mX_reconstructed - mX[lIdx],axis=1)\n",
    "    estimation_error = np.linalg.norm(mX_reconstructed -mXRef[lIdx],axis=1)\n",
    "    reconstruction_errors.append(reconstruction_error.copy())\n",
    "    estimation_errors.append(estimation_error.copy())\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:43:09.141008Z",
     "start_time": "2023-06-17T21:42:56.997542Z"
    }
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Create a subplot of `len(d) + 2 x 10` plots.\n",
    "# 2. In the 1st row, show the clean images (`mXRef`).\n",
    "# 3. In the 2nd row, show the noisy images (`mX`).\n",
    "# 4. In the next rows show the sample per different `d`.  \n",
    "#    Per row, show `d`.\n",
    "\n",
    "num_images = len(lIdx)\n",
    "num_rows = len(d_values) + 2\n",
    "\n",
    "fig = plt.figure(tight_layout=True, figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(num_rows, num_images)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "ax.set_title(f'clean images (`mXRef`)')\n",
    "ax.axis('off')\n",
    "\n",
    "ax = fig.add_subplot(gs[1, :])\n",
    "ax.set_title(f'noisy images (`mX`)')\n",
    "ax.axis('off')\n",
    "\n",
    "for i in range(len(d_values)):\n",
    "    ax = fig.add_subplot(gs[i + 2, :])\n",
    "    ax.set_title(f'd = {d_values[i]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "for i, idx in enumerate(lIdx):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.imshow(mXRef[idx].reshape(T_MNIST_IMG_SIZE), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "for i, idx in enumerate(lIdx):\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    ax.imshow(mX[idx].reshape(T_MNIST_IMG_SIZE), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Plot denoised images for each d\n",
    "for row in range(2, num_rows):\n",
    "    for i, idx in enumerate(lIdx):\n",
    "        d = d_values[row - 2]\n",
    "        ax = fig.add_subplot(gs[row, i])\n",
    "        ax.imshow(denoised_images[row - 2][i].reshape(T_MNIST_IMG_SIZE), cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:43:09.657056Z",
     "start_time": "2023-06-17T21:43:09.155060Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Create 2 sub plots where the `x` is the image index {0, 1, ..., 9}.\n",
    "# 2. The 1st plot, per `d`, shows the reconstruction error.\n",
    "# 3. The 2nd plot, per `d`, shows the estimation error.\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, sharey=True,figsize=(10, 10))\n",
    "bar_width = 0.15\n",
    "bar_positions = np.arange(num_images)\n",
    "for i in range(len(d_values)):\n",
    "    axs[0].bar(bar_positions + i * bar_width, reconstruction_errors[i], bar_width, label=f'd={d_values[i]}')\n",
    "\n",
    "for i in range(len(d_values)):\n",
    "    axs[1].bar(bar_positions + i * bar_width, estimation_errors[i], bar_width, label=f'd={d_values[i]}')\n",
    "\n",
    "axs[1].set_xlabel('Image Index')\n",
    "axs[0].set_ylabel('Reconstruction Error')\n",
    "axs[1].set_ylabel('Estimation Error')\n",
    "\n",
    "axs[0].legend(title='d values',loc='right',  bbox_to_anchor=(1.15, 1))\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "plt.show()\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Question\n",
    "\n",
    "Address the following remarks:\n",
    "\n",
    " - How does the noise model effect the performance of the denoising?  \n",
    "   Specifically, if the noise model was Gaussian with the same variance, what would change?\n",
    " - Would you use the reconstruction error as an estimation of the estimation error?\n",
    " - Explain the idea behind the PCA denoising.  \n",
    "   Specifically address the trade off between small and large values of `d`.\n",
    " - If the data was 1D, would you expect it to perform better?  \n",
    "   Think if the model has any knowledge about the data being 2D."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Solution\n",
    "\n",
    "The noise model can affect the performance of denoising. If the noise model was Gaussian with the same variance, the performance of denoising using PCA would still be affected. However, PCA assumes that the noise is additive and uncorrelated, which is similar to the assumptions of Gaussian noise. Therefore, the performance may still be reasonably good, but it would depend on the specific characteristics of the data and noise.\n",
    "\n",
    "The reconstruction error cannot be used as an estimation of the estimation error. The reconstruction error measures the dissimilarity between the original image and the denoised image, while the estimation error measures the dissimilarity between the denoised image and the ground truth (non-noisy) image. The reconstruction error does not account for the noise in the original image, so it cannot accurately estimate the performance of denoising.\n",
    "\n",
    "The idea behind PCA denoising is to project the noisy data onto a lower-dimensional subspace that captures the most important variations in the data. By reducing the dimensionality, PCA can effectively denoise the data by removing the noise components in the higher-dimensional space. The trade-off between small and large values of d (the number of principal components) lies in the balance between noise removal and preserving important information. Small values of d may not capture enough variations in the data, leading to incomplete denoising and loss of important details. On the other hand, large values of d may capture more noise components and lead to overfitting, resulting in less effective denoising. The optimal value of d depends on the specific characteristics of the data and the noise.\n",
    "\n",
    "If the data was 1D, the performance of PCA denoising would likely be similar or even better compared to 2D data. PCA does not require knowledge about the data being 2D. It is a dimensionality reduction technique that can be applied to data of any dimensionality. However, in the case of 2D data, such as images, PCA can capture both row-wise and column-wise variations, which can be beneficial for denoising. In 1D data, PCA would capture variations along a single dimension, which may still be effective for denoising, especially if the noise is additive and uncorrelated.\n",
    "\n",
    "> Royi: In `4.1.` Think about the geometry of the data.  \n",
    "> Royi: In `4.2.` The idea was about measureing the error for images. Reconstruction is ${L}_{2}$ error.  \n",
    "> Royi: In `4.4.` On the contrary.  \n",
    "> Royi: ❌, See notes above.  \n",
    "> Royi: <font color='red'>-2</font>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. PCA Denoising with Labels\n",
    "\n",
    "In the above we used no knowledge on the label of the image.  \n",
    "In this section you should use the labels information in order to improve results.\n",
    "\n",
    " 1. Write a code which take advantage of the labels `vY` (Be creative).\n",
    " 2. Show the plots of the reconstruction and estimation error.\n",
    " 3. Explain, in words, your idea.\n",
    " 4. Explain, in words, the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:43:17.478761Z",
     "start_time": "2023-06-17T21:43:09.660299Z"
    }
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Choose the maximum `d` used in the previous section.\n",
    "# 2. Apply PCA Denoising on the list of images.\n",
    "\n",
    "reconstruction_errors_with_labels = []\n",
    "estimation_errors_with_labels = []\n",
    "\n",
    "labels = range(10)\n",
    "d = d_values[-1]\n",
    "\n",
    "for label in labels:\n",
    "    pca = PCA(d=d)\n",
    "    pca.fit(mX[vY == label])\n",
    "    mX_lIdx = mX[lIdx[label]]\n",
    "    mZ = pca.transform(mX_lIdx)\n",
    "    mX_reconstructed = pca.inverse_transform(mZ)\n",
    "\n",
    "    reconstruction_error = np.linalg.norm(mX_reconstructed - mX[lIdx[label]])\n",
    "    estimation_error = np.linalg.norm(mX_reconstructed - mXRef[lIdx[label]])\n",
    "    reconstruction_errors_with_labels.append(reconstruction_error.copy())\n",
    "    estimation_errors_with_labels.append(estimation_error.copy())\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T21:43:18.200349Z",
     "start_time": "2023-06-17T21:43:17.478988Z"
    }
   },
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Display the reconstruction and estimation error per image.\n",
    "# 2. Compare to the previous result for the same `d`.\n",
    "reconstruction_errors_no_label = reconstruction_errors[-1]\n",
    "estimation_errors_no_label = estimation_errors[-1]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(10, 10))\n",
    "fig.suptitle(f\"reconstruction and estimation error for d = {d}\")\n",
    "bar_width = 0.15\n",
    "bar_positions = np.arange(num_images)\n",
    "\n",
    "axs[0].bar(bar_positions, reconstruction_errors_no_label, bar_width, label=f'Without label')\n",
    "axs[0].bar(bar_positions + bar_width, reconstruction_errors_with_labels, bar_width, label=f'With label')\n",
    "\n",
    "axs[1].bar(bar_positions, estimation_errors_no_label, bar_width, label=f'Without label')\n",
    "axs[1].bar(bar_positions + bar_width, estimation_errors_with_labels, bar_width, label=f'With label')\n",
    "\n",
    "axs[1].set_xlabel('Image Index')\n",
    "axs[0].set_ylabel('Reconstruction Error')\n",
    "axs[1].set_ylabel('Estimation Error')\n",
    "\n",
    "axs[0].legend(title='With \\ Without label',loc='upper right')\n",
    "axs[1].legend(title='With \\ Without label',loc='upper right')\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3. Solution\n",
    "\n",
    "The idea is to utilize the labels vY associated with the images to enhance the denoising process by running PCA for each label seperatly. \n",
    "By considering the labels associated with the images, we can take advantage of label-dependent characteristics that may exist in the data. These characteristics can capture specific patterns or variations present in images of different classes.\n",
    "By incorporating this information into the denoising process, we can potentially enhance the reconstruction and estimation of image features that are relevant to each class.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4. Solution\n",
    "\n",
    "In our results, we observed that for high-dimensional data (in our case, d=400), the estimation error remained the same while the reconstruction error decreased as we incorporated label information into the denoising process. This means that considering the labels associated with the images helped improve the accuracy of reconstructing the original image while maintaining the same level of estimation error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
