{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0Rvf2Jhen61X"
   },
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 001 - Part I\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.001 | 02/04/2023 | Royi Avital | Fixed a typo in question `0.1.`                                    |\n",
    "| 0.1.000 | 12/03/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GZf3ZGWxn61Z"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0001Part001.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "knK5e3Stn61Z"
   },
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AXn1GLjbn61Z"
   },
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names of the team memebers in the `Team Members` section.\n",
    " - Answer all questions within the Jupyter Notebook.\n",
    " - Open questions are in part I of the exercise.\n",
    " - Coding based questions are in the subsequent notebooks.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for question.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5vkEjUNSn61a"
   },
   "source": [
    "## Team Members\n",
    "\n",
    " - Nadav_Talmon_203663950.\n",
    " - Nadav_Shaked_312494925.\n",
    " - Adi_Rosenthal_316550797."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c3URV4Gen61a"
   },
   "source": [
    "## 0. Linear Algebra\n",
    "\n",
    "A matrix $ P $ is called an orthogonal projection operator if, and only if it is idempotent and symmetric.\n",
    "\n",
    "**Remark**: Idempotent matrix means $ \\forall n \\in \\mathcal{N} \\; {P}^{n} = P $.\n",
    "\n",
    "### 0.1. Question\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ where $ m \\geq n $ and $ \\operatorname{Rank} \\left( A \\right) = n $.  \n",
    "Given the linear least squares problem:\n",
    "\n",
    "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "With the solution in the form $\\hat{\\boldsymbol{x}} = R \\boldsymbol{y}$, show that $P = A R$ is an orthogonal projection operator.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "1. Derive the solution to the Least Squares above in the form of $ \\hat{\\boldsymbol{x}} = R \\boldsymbol{y} $.\n",
    "2. Show the $ P $ matrix is symmetric.\n",
    "3. Show the $ P $ matrix is idempotent.\n",
    "4. Conclude the matrix is an orthogonal projection operator.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The matrix $P$ is the Orthogonal Projection onto the range (Columns space) of $ A $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuFmxuUHn61a"
   },
   "source": [
    "### 0.1. Solution\n",
    "Derive the solution to the Least Squares above in the form of  𝑥̂ =𝑅𝑦:\n",
    "\n",
    "$$ \\frac{1}{2} \\cdot 2 \\cdot A^T(Ax-y)= 0$$\n",
    "\n",
    "$$ A^T(ARy-y)= 0$$\n",
    "$$ A^TARy=A^Ty$$\n",
    "$$ Ry = (A^T A)^{-1} A^T y$$\n",
    "The solution to the normal equation is given by:\n",
    "$$x = (A^T A)^{-1} A^T y = Ry$$\n",
    "Thus\n",
    "$$R = (A^T A)^{-1} A^T$$\n",
    "$$P = A (A^T A)^{-1} A^T $$\n",
    "\n",
    "To show that $P$ is symmetric, we have to show that $P^T = P$\n",
    "\n",
    "$$P^T = (A (A^T A)^{-1} A^T)^T = (A^T)^T ((A^T A)^{-1})^T A^T = A ((A^T A)^T)^{-1} A^T = A (A^T A)^{-1} A^T = P$$\n",
    "\n",
    "To show that $P$ is idempotent, we have to show that $P^2 = P$\n",
    "$$P^2 = A (A^T A)^{-1} A^T \\cdot A (A^T A)^ {-1}A^T = A (A^T A)^{-1} (A^T A) (A^T A)^{-1} A^T = A (A^T A)^{-1} A^T = P$$\n",
    "\n",
    "Hence $P$ is orthogonal projection operator\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cCOE07h6n61a"
   },
   "source": [
    "## 1. Convexity\n",
    "\n",
    "**Convex Set**  \n",
    "\n",
    "Let:\n",
    "\n",
    "$$ \\mathbb{R}_{\\geq 0}^{d} = \\left\\{ \\boldsymbol{x} \\in\\mathbb{R}^{d} \\, \\bigg| \\, \\min_{i} {x}_{i} \\geq 0 \\right\\} $$\n",
    "\n",
    "Where $\\boldsymbol{x} = \\begin{bmatrix} {x}_{1} \\\\ {x}_{2} \\\\ \\vdots \\\\ {x}_{d} \\end{bmatrix}$\n",
    "\n",
    "### 1.1. Question\n",
    "\n",
    "Prove or disprove that $\\mathbb{R}_{\\geq 0}^{d}$ is convex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "z3vOPOo5n61b"
   },
   "source": [
    "### 1.1. Solution\n",
    "\n",
    "Convex Set Definition: $A$ set $C \\subseteq R^n$ is convex if $\\forall x, y \\in C, C$ contains the line segment between $\\mathrm{x}$ and $y$.\n",
    "\n",
    "Proof\n",
    "\n",
    "Denote $\\mathbb{R}_{\\geq 0}^d$ as $H$, that is, $H=\\left\\{x \\in \\mathbb{R}^d \\mid \\min _i x_i \\geq 0\\right\\}$.\n",
    "\n",
    "Let $x, y \\in H$ and consider $z=(1-\\lambda) x+\\lambda y$ for some $\\lambda \\in[0,1]$.\n",
    "\n",
    "Since $x, y \\in H$ it follows that $\\min_i x_i \\geq 0$ and $\\min _i y_i \\geq 0$. Also, by definition $\\lambda,(1-\\lambda) \\geq 0$.\n",
    "\n",
    "Therefore, $\\min _i z_i \\geq 0$ which means $min_i z_i \\geq 0$ so $z \\in H$.\n",
    "\n",
    "Thus, $H$ is a convex set.\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7-xtJ4Z1n61b"
   },
   "source": [
    "**Convex Combination** \n",
    "\n",
    "Let $\\mathcal{C} \\subseteq \\mathbb{R}^{d} $ be a convex set and consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\} _{i=1}^{N}$.\n",
    "\n",
    "### 1.2. Question\n",
    "\n",
    "Prove that for any $N \\in \\mathbb{N}$: \n",
    "\n",
    "$$ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $$\n",
    "\n",
    "Where $\\alpha_{i}$ are such that: \n",
    "\n",
    " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
    " - $\\sum_{i = 1}^{N} \\alpha_{i} = 1$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The properties of ${\\alpha}_{i}$ above means it is sampled from the Unit Probability Simplex.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ILVkt0pin61b"
   },
   "source": [
    "### 1.2. Solution\n",
    "\n",
    "Proof\n",
    "\n",
    "\n",
    "We will use reduction in this proof:\n",
    "\n",
    "Base case - n = 2:\n",
    "\n",
    "from the defenition of convex set we know:\n",
    "$$\\alpha_1 = a, \\alpha_2 = 1-a$$\n",
    "$$\\alpha_1 +\\alpha_2=1$$\n",
    "$$ \\forall x,y \\in C: ax+(1+a)y \\in C$$\n",
    "\n",
    "Inductive step - Assume the formula holds for n = N-1 and now proof for n = N:\n",
    "\n",
    "\n",
    "It is given that $\\left\\{x_i \\in C\\right\\}_{i=1}^N$ and $C$ is a convex set.\n",
    "\n",
    "$$\\sum_{i=1}^N \\alpha_i x_i=\\alpha_1 x_1+\\alpha_2 x_2+\\cdots+\\alpha_N x_N=(\\sum_{i=1}^{N-1} \\alpha_i) \\cdot \\left(\\frac{\\alpha_1}{\\sum_{i=1}^{N-1} \\alpha_i} x_1+\\frac{\\alpha_2}{\\sum_{i=1}^{N-1} \\alpha_i} x_2+\\cdots+\\frac{\\alpha_{N-1}}{\\sum_{i=1}^{N-1} \\alpha_i} x_{N-1}\\right)+\\alpha_N x_N$$\n",
    "\n",
    "Let $z=\\frac{\\alpha_1}{\\Sigma_{i=1}^{N-1} \\alpha_i} x_1+\\frac{\\alpha_2}{\\sum_{i=1}^{N-1} \\alpha_i} x_2+\\cdots+\\frac{\\alpha_{N-1}}{\\sum_{i=1}^{N-1} \\alpha_i} x_{N-1}$.\n",
    "\n",
    "we know that $\\frac{\\alpha_1}{\\Sigma_{i=1}^{N-1} \\alpha_i} +\\frac{\\alpha_2}{\\sum_{i=1}^{N-1} \\alpha_i}+\\cdots+\\frac{\\alpha_{N-1}}{\\sum_{i=1}^{N-1} \\alpha_i}  = \\frac{\\Sigma_{i=1}^{N-1} \\alpha_i}{\\Sigma_{i=1}^{N-1} \\alpha_i}= 1$ and from the assumption statement above we know is also in $C$ and therefore $z \\in C$\n",
    "\n",
    "Since $\\Sigma_{i=1}^N \\alpha_i=1$ we can say that $\\sum_{i=1}^{N-1} \\alpha_i=1-\\alpha_N$.\n",
    "\n",
    "$\\sum_{i=1}^N \\alpha_i x_i=\\left(1-\\alpha_N\\right) z+\\alpha_N x_N$ which is clearly in $C$ for the same reasoning as above.\n",
    "\n",
    "Since we have shown that the formula holds for the base case n=2, and that it holds for n+1 whenever it holds for n, we can conclude that the formula holds for all natural numbers n.\n",
    "\n",
    "Thus, $\\Sigma_{i=1}^N \\alpha_i x_i \\in C$ for any $N \\in \\mathbb{N}$.\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-dbqx6Pan61c"
   },
   "source": [
    "### 1.3. Question\n",
    "\n",
    "Prove or disprove the following assertion:\n",
    "\n",
    "Necessarily, any point $\\boldsymbol{y} \\in \\mathcal{C}$ can be represented as a convex combination of $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KA9bxBtan61c"
   },
   "source": [
    "### 1.3. Solution\n",
    "\n",
    "Let denote $C = \\{ (a, b) | a \\geq 0, a \\geq b \\}$, $C$ is a convex set\n",
    "\n",
    "Consider set of points $\\left\\{ \\boldsymbol{(i, 0)} \\in \\mathcal{C} \\right\\}_{i=1}^{10}$.\n",
    "\n",
    "notice that $(1, 1) \\in C$ but there is no convex combination of $\\left\\{ (i, 0) \\in \\mathcal{C} \\right\\}_{i=1}^{10}$ such that $\\sum\\limits_{i=1}^{10} \\alpha_i x_i$ because the second cooridinate is $0$ for any $x_i$, so for any $\\alpha$ the second cooridinate will be equals to $0$\n",
    "\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MXV61DIHn61c"
   },
   "source": [
    "## 2. The Gradient\n",
    "\n",
    "**Remark**: Assume all functions in this section are differentiable.\n",
    "\n",
    "\n",
    "**Directional Derivative**\n",
    "\n",
    "Let $f : \\mathbb{R}^{d} \\to \\mathbb{R}$ and let $\\boldsymbol{x}_{0} \\in \\mathbb{R}^{d}$. \n",
    "\n",
    "### 2.1. Question\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$ \\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\nabla f \\left( \\boldsymbol{x}_{0} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle \\implies \\boldsymbol{g}_{0} = \\nabla f \\left( \\boldsymbol{x}_{0} \\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wic94Pcgn61c"
   },
   "source": [
    "### 2.1. Solution\n",
    "\n",
    "Given that $$\\forall h\\in R^d:\\nabla f(x_0)[h] = <g_0, h>$$\n",
    "By definition we know that\n",
    "$$<g_0, h> = \\lim_{t \\to 0}\\frac{f(x_0 + th)-f(x_0)}{t}$$\n",
    "Denote $k=th$, thus\n",
    "$$<g_0, h> = \\lim_{k \\to 0}\\frac{f(x_0 + k)-f(x_0)}{k}h = <\\nabla f(x_0), h> $$\n",
    "Hence\n",
    "$$\\nabla f(x_0)[h] \\Rightarrow g_0 = \\nabla f(x_0)$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5kXUMdjHn61c"
   },
   "source": [
    "**Definition**\n",
    "\n",
    "$f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ is said to be **linear** if:\n",
    "\n",
    "$$ f \\left( \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y} \\right) = \\alpha f \\left( \\boldsymbol{x} \\right) + \\beta f \\left( \\boldsymbol{y} \\right) $$\n",
    "\n",
    "For all $\\alpha, \\beta \\in \\mathbb{R}$ and for all $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{{d}_{1}}$.\n",
    "\n",
    "\n",
    "\n",
    "Let $f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ be a linear function.\n",
    "\n",
    "### 2.2. Question\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = f \\left( \\boldsymbol{h} \\right) $$\n",
    "\n",
    "For all $\\boldsymbol{x}, \\boldsymbol{h} \\in \\mathbb{R}^{{d}_{1}}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "95IRJfvkn61d"
   },
   "source": [
    "### 2.2. Solution\n",
    "\n",
    "By defenition\n",
    "$$\\nabla f(x_0)[h] = \\lim_{t \\to 0}\\frac{f(x_0+th)-f(x_0)}{t}$$\n",
    "From linearity of $f$\n",
    "$$\\nabla f(x_0)[h] = \\lim_{t \\to 0}\\frac{f(x_0)+f(th)-f(x_0)}{t} = \\lim_{t \\to 0}\\frac{tf(h)}{t} = f(h)$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dfXvXn6Tn61d"
   },
   "source": [
    "### 2.3. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wk7Jp8eDn61d"
   },
   "source": [
    "### 2.3. Solution\n",
    "As we leaned in class:\n",
    "\n",
    "i.\n",
    "$$\\nabla <f(x), g(x)>[h] = <\\nabla f(x)[h], g(x)> + <f(x), \\nabla g(x)[h]>$$\n",
    "ii.\n",
    "$$<A𝑥,ℎ> = <𝑥,𝐴^𝑇ℎ>$$\n",
    "iii.\n",
    "$$\\nabla f(x)[h] = <\\nabla f(x), h>$$\n",
    "$$f is linear ⇒ \\nabla f(x) [h] = f(h)$$\n",
    "\n",
    "answer:\n",
    "$$f(x) = x^TAx = <x, Ax>$$\n",
    "by i, ii, iii\n",
    "\n",
    "$$\\nabla f(x)[h]=<x, A x>=<f(x), g(x)>=<\\nabla f(x)[h], g(x)>+<f(x), \\nabla g(x)[h]>=<h, A x>+<x, A h>=<A x, h>+<A^T x, h>=<\\left(A+A^T\\right) x, h>$$\n",
    "\n",
    "Thus\n",
    "$$\\nabla f(x) = (A + A^T) x$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTMpdUkQn61d"
   },
   "source": [
    "### 2.4. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{A} \\boldsymbol{X} \\right\\} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5hRO4FV2n61d"
   },
   "source": [
    "### 2.4. Solution\n",
    "\n",
    "$$f(X) = Tr\\{X^T AX\\} = <X, AX>$$\n",
    "\n",
    "$$\\nabla <X, AX>[H] = <H, AX> + <X, A H> = <AX, H> + <A^T X, H> = <(A + A^T) X, H>$$\n",
    "Thus\n",
    "$$\\nabla f(X) = (A + A^T) X$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "omT666b2n61d"
   },
   "source": [
    "### 2.5. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {\\left\\| \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x} \\right\\|}_{2}^{2} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP2z0Ko5n61d"
   },
   "source": [
    "### 2.5. Solution\n",
    "\n",
    "$$f(x) = ||y - Ax||^2_2 = <y - Ax, y - Ax>$$\n",
    "\n",
    "\\begin{aligned}\n",
    "& f(x)=\\|y-A x\\|_2^2=<y-A x, y-A x>\\rightarrow \\nabla f(x)[h]=\\nabla<f(x), g(x)>=<\\nabla f(x)[h], g(x)>+< \\\\\n",
    "& f(x), \\nabla g(x)[h]>=<-A h, y-A x>+<y-A x,-A h>=<2(y-A x),-A h>=< \\\\\n",
    "& -2 A^T(y-A x), h>\\rightarrow \\nabla f(x)=-2 A^T(y-A x)\n",
    "\\end{aligned}\n",
    "\n",
    "Thus\n",
    "$$\\nabla f(x) = -2 A^T (y - Ax)$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RluvYKSOn61e"
   },
   "source": [
    "### 2.6. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X} \\right\\|}_{F}^{2} $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{D \\times d}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$.\n",
    " - ${\\left\\| \\cdot \\right\\|}_{F}^{2}$ is the squared [Frobenius Norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm), that is, ${\\left\\| \\boldsymbol{X} \\right\\|}_{F}^{2} = \\left\\langle \\boldsymbol{X}, \\boldsymbol{X} \\right\\rangle = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{X} \\right\\}$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EiWVPbNyn61e"
   },
   "source": [
    "### 2.6. Solution\n",
    "\n",
    "$$f(X) = ||Y - AX||^2_F = <Y - AX, Y - AX>$$\n",
    "\n",
    "$$\\nabla <Y - AX, Y - AX>[H] = <-AH, Y - AX> + <Y - AX, -AH> = 2<Y - AX, -AH> = -2<A^T (Y - AX), H>  = <-2 A^T (Y - AX), H>$$\n",
    "Thus\n",
    "$$\\nabla f(x) = -2 A^T (Y - AX)$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qIUEmRlPn61e"
   },
   "source": [
    "### 2.7. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{X}^{T} \\boldsymbol{A}, \\boldsymbol{Y}^{T} \\right\\rangle $$\n",
    "\n",
    "Where $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times D}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MlVDOMsin61e"
   },
   "source": [
    "### 2.7. Solution\n",
    "\n",
    "$$f(X) = <X^T A, Y^T> = Tr\\{(X^T A)^T Y^T\\} = Tr\\{A^T X Y^T\\} = Tr\\{Y^T A^T X\\} = <AY, X>$$\n",
    "by i, ii, iii\n",
    "$$\\nabla <AY, X>[H] = <0, X> + <AY, H> = <AY, H> = Y^TA^TH$$\n",
    "Thus\n",
    "$$\\nabla f(X) = AY$$\n",
    "\n",
    "\n",
    "As we leaned in class:\n",
    "\n",
    "i.\n",
    "$$\\nabla <f(X), g(X)>[H] = <\\nabla f(X)[H], g(X)> + <f(X), \\nabla g(X)[H]>$$\n",
    "ii.\n",
    "$$g(X) = X$$\n",
    "$$\\nabla g(X)[H] = H$$\n",
    "iii.\n",
    "$$g(X) = Y$$\n",
    "$$\\nabla g(X)[H] = 0$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri75AQBWn61e"
   },
   "source": [
    "### 2.8. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) $$\n",
    "\n",
    "Where $g \\left( \\cdot \\right)$ is an element wise function $g \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RjgYmDaon61e"
   },
   "source": [
    "### 2.8. Solution\n",
    "\n",
    "let denote $g' \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix} g' \\left( {x}_{1} \\right) \\\\ g' \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g' \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$\n",
    "\n",
    "$$f(x) = a^T g(x)$$\n",
    "\n",
    "$$\\nabla f(x) = \\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial x} = a^T g'(x)$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\\nabla f(x)[h] = <\\nabla f(x), h> = <a^T g'(x), h> = g'(x)^T ah$$\n",
    "\n",
    "> Royi: ❌, The gradient of yours is a scalar while it should be a vector.  \n",
    "> Royi: <font color='red'>-3</font>.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "V7xDRF7an61e"
   },
   "source": [
    "### 2.9. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{A}, \\log \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\log \\left( \\cdot \\right)$ is the element wise $\\log$ function: $\\boldsymbol{M} = \\log \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\log \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ix8-7-10n61e"
   },
   "source": [
    "### 2.9. Solution\n",
    "\n",
    "Denote $X^- = \\nabla log(X) \\implies \\boldsymbol{X^-} \\left[ i, j \\right] = \\frac{\\boldsymbol{1}}{\\boldsymbol{X} \\left[ i, j\\right]}$.\n",
    "\n",
    "By i\n",
    "\n",
    "$$f(X)[H] = <0, \\log (X)> + <A, X^- H> = <(X^-)^T A, H>$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\\nabla f(X) = (X^-)^T A$$\n",
    "\n",
    "As we leaned in class:\n",
    "\n",
    "i.\n",
    "$$\\nabla <f(X), g(X)>[H] = <\\nabla f(X)[H], g(X)> + <f(X), \\nabla g(X)[H]>$$\n",
    "\n",
    "> Royi: ❌, Think of the case $ X, A \\in \\mathbb{R}^{m \\times n}$. The definition of the function will hold, yet will your gradient have the correct dimensions?  \n",
    "> Royi: <font color='red'>-3</font>.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uX6KBMJRn61f"
   },
   "source": [
    "### 2.10. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\operatorname{Diag} \\left( \\cdot \\right) : \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d} $ returns the diagonal of a matrix, that is, $\\boldsymbol{b} = \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i \\right] = \\left( \\boldsymbol{X} \\left[ i, i\\right] \\right)$.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tiscSdTFn61f"
   },
   "source": [
    "### 2.10. Solution\n",
    "\n",
    " Let $\\operatorname{diag}(x)=\\left\\{\\begin{array}{ll}x & i=j \\\\ 0 & \\text { else }\\end{array}\\right.$ such that it returns $R^d \\rightarrow R^{d x d}$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\nabla f(X)[h]=\\nabla<f(X), g(X)>=<\\nabla f(x)[h], g(x)>+<f(x),\\nabla g(x)[H]>=<0,\\operatorname{Diag}(X)>+<a, \\operatorname{Diag}(H)>=<\\operatorname{diag}(a), H>\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "daQq7z8Ln61f"
   },
   "source": [
    "## 3. Constraint Optimization\n",
    "\n",
    "**MinMax**  \n",
    "\n",
    "Let $G \\left( x, y \\right) = \\sin \\left( x + y \\right)$.\n",
    "\n",
    "### 3.1. Question\n",
    "\n",
    "Show that:\n",
    "\n",
    " - $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$.\n",
    " - $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5wcfWcDmn61f"
   },
   "source": [
    "### 3.1. Solution\n",
    "\n",
    "For\n",
    "$$\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$$\n",
    "\n",
    "Let calculate the derivative by $y$\n",
    "$$\\frac{\\partial f}{\\partial y} = \\cos(x + y)$$\n",
    "For find the optimum points we compare to $0$\n",
    "$$\\frac{\\partial f}{\\partial y} = \\cos(x + y) = 0$$\n",
    "Thus\n",
    "$$x + y = \\frac{\\pi}{2} + \\pi k, \\forall k \\in Z$$\n",
    "$$y = \\frac{\\pi}{2} + \\pi k - x, \\forall k \\in Z$$\n",
    "For $k = 0$, $\\sin (x + y) = \\sin (\\frac {\\pi}{2}) = 1$, and this is the maximum value\n",
    "Lets minimize the equation\n",
    "$$\\underset{x}{\\min} \\sin (\\frac{\\pi}{2})$$\n",
    "The equation doesnt depends on $x$, thus\n",
    "$$\\underset{x}{\\min} \\sin (\\frac{\\pi}{2}) = 1$$\n",
    "So\n",
    "$$\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$$\n",
    "\n",
    "---\n",
    "\n",
    "For\n",
    "$$\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$$\n",
    " \n",
    "Let calculate the derivative by $x$\n",
    "$$\\frac{\\partial f}{\\partial x} = \\cos(x + y)$$\n",
    "For find the optimum points we compare to $0$\n",
    "$$\\frac{\\partial f}{\\partial x} = \\cos(x + y) = 0$$\n",
    "Thus\n",
    "$$x + y = \\frac{\\pi}{2} + \\pi k, \\forall k \\in Z$$\n",
    "$$y = \\frac{\\pi}{2} + \\pi k - x, \\forall k \\in Z$$\n",
    "For $k = -1$, $\\sin (x + y) = \\sin (-\\frac {\\pi}{2}) = -1$, and this is the minimum value\n",
    "Lets minimize the equation\n",
    "$$\\underset{y}{\\max} \\sin (-\\frac{\\pi}{2})$$\n",
    "The equation doesnt depends on $y$, thus\n",
    "$$\\underset{y}{\\max} \\sin (-\\frac{\\pi}{2}) = -1$$\n",
    "So\n",
    "$$\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "> Royi: There is no need to calculate the derivative.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HtU7uJfPn61f"
   },
   "source": [
    "**Rayleigh Quotient**  \n",
    "\n",
    "The _Rayleigh Quotient_ is defined by:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = \\frac{ \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} $$\n",
    "\n",
    "For some symmetric matrix $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "### 3.2. Question\n",
    "\n",
    "Follow the given steps:\n",
    "\n",
    " - Show that $ {\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) = \\begin{cases} {\\min}_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\\\ \\text{ s.t. } {\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1 \\end{cases} $.\n",
    " - Write the Lagrangian of the constraint objective $\\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right)$.\n",
    " - Show that ${\\nabla}_{\\boldsymbol{x}} \\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right) = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x}$.  \n",
    "   In other words, the stationary points $\\left( \\boldsymbol{x}, \\lambda \\right)$ are the eigenvectors and eigenvalues of $\\boldsymbol{A}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2qhkNC9Mn61f"
   },
   "source": [
    "### 3.2. Solution\n",
    "\n",
    "Let $g(x)=x^T A x$\n",
    "First Direction:\n",
    "$$\n",
    "\\min _x f(x) \\rightarrow\\left\\{\\begin{array}{c}\n",
    "\\min _x x^T A x \\\\\n",
    "\\text { s.t. }\\|x\\|_2^2=1\n",
    "\\end{array}:\\right.\n",
    "$$\n",
    "Since $f(x)=f(a x)$ where $a$ is a scalar, we can choose $\\|x\\|$ to be however we like it. Specifically, $\\|x\\|_2=1 \\rightarrow\\|x\\|_2^2=1$. Thus, minimizing $f(x)$ is equivalent to minimizing $g(x)$ such that $\\|x\\|_2^2=1$.\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{array}{c}\n",
    "\\min _x x^T A x \\\\\n",
    "\\text { s.t. }\\|x\\|_2^2=1\n",
    "\\end{array} \\rightarrow \\min _x f(x): \\text { if }\\|x\\|_2^2=1 \\text { we can rewrite } g(x)=x^T A x \\text { as } g(x)=\\frac{x^T A x}{\\|x\\|_2^2}=\\frac{x^T A x}{x^T x}=f(x)\\right.\n",
    "$$\n",
    "and therefore minimizing $g(x)$ subject to $\\|x\\|_2^2=1$ is the same as minimizing $f(x)$.\n",
    "\n",
    "\n",
    "Let’s rewrite the constraint $‖x‖_2^2=1$:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\left\\{\\begin{array}{c}\n",
    "-\\left(\\|x\\|_2^2-1\\right) \\leq 0 \\\\\n",
    "\\|x\\|_2^2-1 \\leq 0\n",
    "\\end{array}\\right. \\\\\n",
    "& \\underline{\\nabla \\mathcal{L}(x, \\lambda)=0 \\rightarrow \\mathrm{Ax}=\\lambda \\mathrm{x} ,  λ >0}\\\\\n",
    "& \\mathcal{L}(x, \\lambda)=x^\\tau A x-\\lambda\\left(\\|x\\|_2^2-1\\right) \\rightarrow \\nabla \\mathcal{L}(x, \\lambda)=0 \\rightarrow 2 A x-2 \\lambda x=0 \\rightarrow A x=\\lambda x \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> Royi: <font color='green'>✔</font>.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_edr2GPcn61f"
   },
   "source": [
    "<img src=\"https://i.imgur.com/qIP5xPv.png\" height=\"700\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "knK5e3Stn61Z",
    "AXn1GLjbn61Z",
    "5vkEjUNSn61a",
    "ZuFmxuUHn61a",
    "z3vOPOo5n61b",
    "7-xtJ4Z1n61b",
    "ILVkt0pin61b",
    "-dbqx6Pan61c",
    "KA9bxBtan61c",
    "Wic94Pcgn61c",
    "5kXUMdjHn61c",
    "95IRJfvkn61d",
    "dfXvXn6Tn61d",
    "wk7Jp8eDn61d",
    "ZTMpdUkQn61d",
    "5hRO4FV2n61d",
    "omT666b2n61d",
    "ZP2z0Ko5n61d",
    "RluvYKSOn61e",
    "EiWVPbNyn61e",
    "qIUEmRlPn61e",
    "MlVDOMsin61e",
    "Ri75AQBWn61e",
    "RjgYmDaon61e",
    "V7xDRF7an61e",
    "ix8-7-10n61e",
    "uX6KBMJRn61f",
    "tiscSdTFn61f",
    "5wcfWcDmn61f",
    "HtU7uJfPn61f"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
