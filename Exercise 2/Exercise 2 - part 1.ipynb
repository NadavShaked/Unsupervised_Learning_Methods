{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 002 - Part I\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.001 | 04/05/2023 | Royi Avital | Fixed the missing squares in `1.1`                                 |\n",
    "| 0.1.000 | 31/03/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0002Part001.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
    " - Answer all questions / tasks within the Jupyter Notebook.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Verify the rendering on VS Code.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for questions.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    "- `Nadav_Talmon_203663950`.\n",
    "- `Nadav_Shaked_312494925`.\n",
    "- `Adi_Rosenthal_316550797`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Royi: <font color='green'>‚úì +2 Points (Early Submission)</font>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Algebra\n",
    "\n",
    "The Frobenius Norm ${\\left\\| \\cdot \\right\\|}_{F} : \\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{+}$ is defined as ${\\left\\| A \\right\\|}_{F} = \\sqrt{\\sum_{i} \\sum_{j} {A}_{ij}^{2}}$.\n",
    "\n",
    "### 1.1. Question\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ and ${\\lambda}_{i} \\left( M \\right)$ is the $i$ -th eigen value of the matrix $M$ (Assuming $M$ has a valid eigen decomposition).  \n",
    "Prove that ${\\left\\| A \\right\\|}_{F}^{2} = \\sum_{i = 1}^{n} {\\lambda}_{i} \\left( {A}^{T} A \\right)$.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> Make sure to show why ${\\lambda}_{i} \\left( {A}^{T} A \\right)$ exists."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Solution\n",
    "\n",
    "The Frobenius norm ${\\left| \\cdot \\right|}{F}$ of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as ${\\left\\| A \\right\\|}_{F} = \\sqrt{\\sum_{i} \\sum_{j} {A}_{ij}^{2}} = \\operatorname{Tr}(A^TA).$\n",
    "\n",
    "To prove ${\\left\\| A \\right\\|}_{F}^{2} = \\sum_{i = 1}^{n} {\\lambda}_{i} \\left( {A}^{T} A \\right)$, we first compute the eigenvalues of the symmetric positive semidefinite matrix ${A}^{T} A$.\n",
    "\n",
    "Since ${A}^{T} A$ is symmetric, it has an eigen decomposition of the form ${A}^{T} A = Q \\Lambda Q^{-1}$, where $Q$ is an orthonormal matrix whose columns are the eigenvectors of ${A}^{T} A$, and $\\Lambda = \\mathrm{diag}({\\lambda}{1}, {\\lambda}{2}, \\dots, {\\lambda}_{n})$ is a diagonal matrix whose entries are the corresponding eigenvalues. Hence, ${\\lambda}_{i} \\left( {A}^{T} A \\right)$ exists and is non-negative for all $i$.\n",
    "\n",
    "Using this, we have\n",
    "$ \\operatorname{Tr}(A^TA) = \\operatorname{Tr}(Q \\Lambda Q^{-1})$. Since trace is invariant under cyclic permutations, we can rearrange the multiplication as follows: $ \\operatorname{Tr}( \\Lambda Q^{-1} Q)$.\n",
    "The product $Q^{-1}Q$ is the identity matrix, so we have:\n",
    "$\\operatorname{Tr}(A^TA) = \\operatorname{Tr}(\\Lambda) = \\lambda_1 + \\lambda_2 + ... + \\lambda_n$.\n",
    "\n",
    "Therefore, we can conclude that ${\\left\\| A \\right\\|}_{F}$ is equal to the sum of the eigenvalues of $\\mathbf{A}^\\top\\mathbf{A}$. \n",
    "\n",
    "> Royi: Actually since it is a symmetric matrix its eigen deocmposition by orthogonal matrix can be written as $Q \\Lambda {Q}^{T}$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix $Q \\in \\mathbb{R}^{n \\times n}$ is called a positive definite matrix if and only if $\\forall \\boldsymbol{x} \\in \\mathbb{R}^{n} \\setminus \\left\\{ \\boldsymbol{0} \\right\\}, \\; \\boldsymbol{x}^{T} Q \\boldsymbol{x} > \\boldsymbol{0}$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The following are the common notations:\n",
    "    * $\\boldsymbol{S}^{N}      = \\left\\{ X \\in \\mathbb{R}^{n \\times n} \\mid X = {X}^{T} \\right\\}$ (Symmetric matrices).\n",
    "    * $\\boldsymbol{S}^{N}_{+}  = \\left\\{ X \\in \\mathbb{R}^{n \\times n} \\mid X \\succeq 0 \\right\\}$ (Positive semi definite matrices which are symmetric).\n",
    "    * $\\boldsymbol{S}^{N}_{++} = \\left\\{ X \\in \\mathbb{R}^{n \\times n} \\mid X \\succ 0 \\right\\}$ (Positive definite matrices which are symmetric).\n",
    "\n",
    "### 1.2. Question\n",
    "\n",
    "Let $Q \\in \\mathbb{R}^{n \\times n}$ be a positive definite matrix. Show that ${\\left\\| \\boldsymbol{x} \\right\\|}_{Q} = \\sqrt{\\boldsymbol{x}^{T} Q \\boldsymbol{x}}$ is a norm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solution\n",
    "\n",
    "\n",
    "We will show that ${\\left\\| \\boldsymbol{x} \\right\\|}_{Q} = \\sqrt{\\boldsymbol{x}^{T} Q \\boldsymbol{x}}$ is a norm.\n",
    "<br>\n",
    "By definition norm must satisfy:\n",
    "<br>\n",
    "1. $‚àÄx \\ne 0, ||x||_Q > 0, x = 0, ||x||_Q = 0$\n",
    "2. $||Œª x||_Q = |Œª|||x||_Q$\n",
    "3. $||x + y||_Q ‚â§ ||x||_Q + ||y||_Q$\n",
    "\n",
    "Proof:\n",
    "1. $‚àÄx \\ne 0, ||x||_Q > 0, x = 0, ||x||_Q = 0$\n",
    "<br>\n",
    "By definition Q is positive matrix so $‚àÄx \\ne 0, ||x||_Q > 0$, and for $x=0$, easy to see that $||x||_Q = 0$\n",
    "<br><br>\n",
    "2. $||Œªx||_Q = \\sqrt{(Œªx)^TQŒªx} = \\sqrt{Œªx^TQŒªx} = \\sqrt{Œª^2x^TQx} = |Œª|\\sqrt{x^TQx} = |Œª|||x||_Q$\n",
    "<br><br>\n",
    "3. We will show that $||x + y||_Q ‚â§ ||x||_Q + ||y||_Q$:\n",
    "<br>\n",
    "$(||x + y||_Q)^2 ‚â§ (||x||_Q + ||y||_Q)^2$\n",
    "<br>\n",
    "$(x + y)^TQ(x + y) ‚â§ ||x||_Q^2 + 2||x||_Q||y||_Q + ||y||_Q^2$\n",
    "<br>\n",
    "$(x^TQ + y^TQ)(x + y) ‚â§ x^TQx + 2\\sqrt{x^TQx} \\sqrt{y^TQy} + y^TQy$\n",
    "<br>\n",
    "$x^TQx + x^TQy + y^TQx + y^TQy ‚â§ x^TQx + 2\\sqrt{x^TQx} \\sqrt{y^TQy} + y^TQy$\n",
    "<br><br>\n",
    "ùëÑ  is symmetric, so $\\boldsymbol{x}^{T} Q \\boldsymbol{y} = (\\boldsymbol{x}^{T} Q \\boldsymbol{y})^{T} = \\boldsymbol{y}^{T} Q \\boldsymbol{x}$.\n",
    "<br>\n",
    "$2x^TQy ‚â§ 2\\sqrt{x^TQx} \\sqrt{y^TQy}$\n",
    "<br>\n",
    "$x^TQy ‚â§ \\sqrt{x^TQx} \\sqrt{y^TQy}$\n",
    "<br><br>\n",
    "The Cauchy-Schwarz inequality states that for any vectors $u$ and $v$ in an inner product space, the absolute value of their inner product is less than or equal to the product of their norms: $|u^Tv| \\leq \\|u\\| \\|v\\|$\n",
    "<br>\n",
    "Let's apply the Cauchy-Schwarz inequality to the vectors $u = \\sqrt{Q}x$ and $v = \\sqrt{Q}y$:\n",
    "<br>\n",
    "$|(\\sqrt{Q}x)^T \\sqrt{Q}y| \\leq \\|\\sqrt{Q}x\\| \\|\\sqrt{Q}y\\|$\n",
    "<br>\n",
    "Simplifying the left-hand side: $|\\sqrt{Q}x^T \\sqrt{Q}y| = |x^T Q y|$\n",
    "<br>\n",
    "And the norms on the right-hand side:\n",
    "<br>\n",
    "$\\|\\sqrt{Q}x\\| = \\sqrt{x^T Q x}, \\|\\sqrt{Q}y\\| = \\sqrt{y^T Q y}$\n",
    "<br>\n",
    "Putting it all together, we have:\n",
    "<br>\n",
    "$x^T Q y \\leq \\sqrt{x^T Q x} \\sqrt{y^T Q y}$\n",
    "<br>\n",
    "Therefore, we have shown that $||x + y||_Q ‚â§ ||x||_Q + ||y||_Q$ for a positive definite matrix $Q$.\n",
    "\n",
    "thus ${\\left\\| \\boldsymbol{x} \\right\\|}_{Q} = \\sqrt{\\boldsymbol{x}^{T} Q \\boldsymbol{x}}$ is a norm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix $U \\in \\mathbb{R}^{n \\times n}$ is called an _orthogonal matrix_ if and only if ${U}^{T} U = U {U}^{T} = {U}^{-1} U = U {U}^{-1} = I$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> For matrices over $\\mathbb{C}$ we call such matrices a unitary matrices.\n",
    "\n",
    "### 1.3. Question\n",
    "\n",
    "Show that for any orthogonal matrix $U$ is an isometry with respect to the euclidean norm, that is ${\\left\\| U \\boldsymbol{x} \\right\\|}_{2} = {\\left\\| x \\right\\|}_{2}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Solution\n",
    "\n",
    "$||Ux||_2 = \\sqrt{(Ux)^T Ux} = \\sqrt{x^T U^T U x} = \\sqrt{x^T I x} = \\sqrt{x^T x} = ||x||_2$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $R = \\begin{bmatrix*}[r] \\cos \\left( \\theta \\right) & - \\sin \\left( \\theta \\right) \\\\ \\sin \\left( \\theta \\right) & \\cos \\left( \\theta \\right) \\end{bmatrix*}$ is called a rotation matrix.\n",
    "\n",
    "### 1.4. Question\n",
    "\n",
    "For the set of matrices of size $2 \\times 2$, Prove or disprove:\n",
    "\n",
    " - The matrix $R$ is a orthogonal matrix.\n",
    " - Any orthogonal matrix is a rotation matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Solution\n",
    "\n",
    "The matrix $R$ is a orthogonal matrix\n",
    "<br><br>\n",
    "Proof:\n",
    "<br>\n",
    "$R = \\begin{bmatrix*}[r] \\cos \\left( \\theta \\right) & - \\sin \\left( \\theta \\right) \\\\ \\sin \\left( \\theta \\right) & \\cos \\left( \\theta \\right) \\end{bmatrix*}$\n",
    "<br>\n",
    "$R^T = \\begin{bmatrix*}[r] \\cos \\left( \\theta \\right) & \\sin \\left( \\theta \\right) \\\\ - \\sin \\left( \\theta \\right) & \\cos \\left( \\theta \\right) \\end{bmatrix*}$\n",
    "<br>\n",
    "<br>\n",
    "$R^T R = \\begin{bmatrix*}[r] \\cos \\left( \\theta \\right) & - \\sin \\left( \\theta \\right) \\\\ \\sin \\left( \\theta \\right) & \\cos \\left( \\theta \\right) \\end{bmatrix*} \\begin{bmatrix*}[r] \\cos \\left( \\theta \\right) & \\sin \\left( \\theta \\right) \\\\ - \\sin \\left( \\theta \\right) & \\cos \\left( \\theta \\right) \\end{bmatrix*} = \\begin{bmatrix*}[r] \\cos^2 \\left( \\theta \\right) + \\sin^2 \\left( \\theta \\right) & -\\cos \\left( \\theta \\right) \\sin \\left( \\theta \\right) + \\cos \\left( \\theta \\right) \\sin \\left( \\theta \\right) \\\\ -\\cos \\left( \\theta \\right) \\sin \\left( \\theta \\right) + \\cos \\left( \\theta \\right) \\sin \\left( \\theta \\right) & \\cos^2 \\left( \\theta \\right) + \\sin^2 \\left( \\theta \\right) \\end{bmatrix*} = \\begin{bmatrix*}[r] 1 & 0 \\\\ 0 & 1 \\end{bmatrix*} = I$\n",
    "<br>\n",
    "<br>\n",
    "We saw $R^{-1} = R^T$ So $R$ is orthogonal matrix.\n",
    "\n",
    "Any orthogonal matrix is a rotation matrix:\n",
    "<br><br>\n",
    "Disproof:\n",
    "<br><br>\n",
    "Let $A=\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right], A^{-1}=A=A^T$\n",
    "<br><br>\n",
    "we need to show it's orthogonal:\n",
    "<br><br>\n",
    "$\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right]\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & -1\\end{array}\\right]=\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & 1\\end{array}\\right]=I$.\n",
    "<br><br>\n",
    "Thus, $A$ is orthogonal.\n",
    "<br>\n",
    "Square matrix $R$ will be called rotation $m$ atrix iff $R^T=R^{-1}$ and $\\operatorname{det}(\\mathrm{R})=1$\n",
    "In our case, $A^T=A^{-1}$, but $\\operatorname{det}(A)=-1 \\neq 1$. Thus, $A$ is orthogonal but not a rotation matrix.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimization\n",
    "\n",
    "A twice differentiable function is convex if and only if ${\\nabla}^{2} f \\left( \\boldsymbol{x} \\right) \\succeq 0$. \n",
    "\n",
    "### 2.1. Question (Bonus 4%)\n",
    "\n",
    "Consider the function $f \\left( \\boldsymbol{x} \\right) : \\mathbb{R}^{n}_{++} \\to \\mathbb{R}_{-}$ where $f \\left( \\boldsymbol{x} \\right) = -\\sqrt[^n]{ {x}_{1} {x}_{2} \\ldots {x}_{n} }$.  \n",
    "Show that $ f\\left( \\boldsymbol{x} \\right)$ is a convex function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Solution\n",
    "\n",
    "First we will prove that Geometric Mean is concave.\n",
    "\n",
    "Define the Grometric mean function $g(x) = - f(x)$\n",
    "\n",
    "To show that $g(\\boldsymbol{x})$ is concave, we need to show that for any two points $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{n}_{++}$ and any $\\lambda \\in [0,1]$, we have:\n",
    "\n",
    "$g(\\lambda \\boldsymbol{x} + (1 - \\lambda)\\boldsymbol{y}) \\geq \\lambda g(\\boldsymbol{x}) + (1 - \\lambda) g(\\boldsymbol{y})$\n",
    "\n",
    "Let $\\boldsymbol{x} = (x_1, x_2, \\ldots, x_n)$ and $\\boldsymbol{y} = (y_1, y_2, \\ldots, y_n)$. Then we have:\n",
    "\n",
    "By the AM-GM inequality $\\frac{a_1 + a_2 + \\cdots + a_n}{n} \\geq\\left(a_1 a_2 \\cdots a_n \\right)^{\\frac{1}{n}}$\n",
    "\n",
    "Thus, applying AM-GM for $a_i = \\frac{x_i}{\\lambda x_i + (1 - \\lambda) y_i}$ then for $a_i = \\frac{y_i}{\\lambda x_i + (1 - \\lambda) y_i}$ we get:\n",
    "\n",
    "For x\n",
    "\n",
    "1. $\\frac{g(x)}{(g(\\lambda x + (1 - \\lambda) y)} = \\left(\\prod_{i = 1}^n \\frac{x_i}{\\lambda x_i + (1 - \\lambda) y_i} \\right)^{\\frac{1}{n}} \\leq \\frac{1}{n}\\left(\\sum_{i = 1}^n \\frac{x_i}{\\lambda x_i + (1 - \\lambda) y_i} \\right)$\n",
    "\n",
    "For y\n",
    "\n",
    "2. $\\frac{g(y)}{(g(\\lambda x + (1 - \\lambda) y)} = \\left(\\prod_{i = 1}^n \\frac{y_i}{\\lambda x_i + (1 - \\lambda) y_i} \\right)^{\\frac{1}{n}} \\leq \\frac{1}{n}\\left(\\sum_{i = 1}^n \\frac{y_i}{\\lambda x_i + (1 - \\lambda) y_i}\\right)$\n",
    "\n",
    "Multiplying 1. By $\\lambda$ and the second by $1 - \\lambda$ and then do $1 + 2$ we get\n",
    "\n",
    "$\\lambda \\cdot \\frac{1}{n}\\left(\\sum_{i = 1}^n \\frac{x_i}{\\lambda x_i + (1 - \\lambda) y_i}\\right) + (1 - \\lambda) \\cdot \\frac{1}{n}\\left(\\sum_{i = 1}^n \\frac{y_i}{\\lambda x_i + (1 - \\lambda) y_i}\\right) = \\frac{1}{n} \\left( \\sum_{i = 1}^n \\frac{\\lambda x_i + (1 - \\lambda) y_i}{\\lambda x_i + (1 - \\lambda) y_i} \\right) = 1$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$\\frac{\\lambda g(x)+1-\\lambda g(y)}{g(\\lambda x+(1-\\lambda) y)} \\leq 1 \\rightarrow g(x)$ is concave.\n",
    "\n",
    "We have shown that $g(x)$ is concave, thus $f(\\boldsymbol{x})$ is a convex function.\n",
    "\n",
    "> Royi: <font color='green'>‚úì +4 Points (Bonus)</font>.  \n",
    "> Royi: This is really nice!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Question\n",
    "\n",
    "Find the global minimum and maximum points of the linear function $f \\left( x, y \\right) = 7 x + 12 y$ over the set $\\mathcal{S} = \\left\\{ \\left( x, y \\right) \\mid 2 {x}^{2} + 6 x y + 9 {y}^{2} - 2 x - 6 y \\leq 24 \\right\\}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Solution\n",
    "\n",
    "Rewrite the constraint equation $\\mathcal{S} = \\left\\{ \\left( x, y \\right) \\mid 2 {x}^{2} + 6 x y + 9 {y}^{2} - 2 x - 6 y - 24 \\leq 0 \\right\\}$.\n",
    "<br>\n",
    "\n",
    "Define the Lagrangian function as $\\mathcal{L}(x, y, \\lambda) = f(x, y) + \\lambda \\cdot g(x, y)$, where $g(x, y) = 2x^2 + 6xy + 9y^2 - 2x - 6y - 24$, the constraint equation.\n",
    "<br>\n",
    "\n",
    "$\\mathcal{L}(x, y, \\lambda) = 7x + 12y + \\lambda \\cdot (2x^2 + 6xy + 9y^2 - 2x - 6y - 24)$\n",
    "<br>\n",
    "\n",
    "Calculate the partial derivatives of $\\mathcal{L}$ and equal to zero:\n",
    "<br>\n",
    "\n",
    "1. $\\frac{\\partial \\mathcal{L}}{\\partial x}$: $7 + 4\\lambda x + 6\\lambda y - 2\\lambda = 0$\n",
    "<br>\n",
    "\n",
    "2. $\\frac{\\partial \\mathcal{L}}{\\partial y}$: $12 + 6\\lambda x + 18\\lambda y - 6\\lambda = 0$\n",
    "<br>\n",
    "\n",
    "3. $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}$: $2x^2 + 6xy + 9y^2 - 2x - 6y - 24 = 0$\n",
    "<br>\n",
    "\n",
    "Let's solve these equations\n",
    "<br>\n",
    "Multiply by 3 the first equation and then subtract equation 2.\n",
    "<br>\n",
    "\n",
    "$21 + 6 \\lambda x + 6 \\lambda x + 18 y \\lambda -6 \\lambda -(12 + 6 \\lambda x + 18 y \\lambda -6 \\lambda) = 0$\n",
    "<br>\n",
    "\n",
    "$9 + 6 \\lambda x=0$\n",
    "<br>\n",
    "\n",
    "$x = - \\frac{3}{2 \\lambda} ;$  assuming $\\lambda \\neq 0$\n",
    "<br><br>\n",
    "\n",
    "Now Let's consider equation 2\n",
    "<br>\n",
    "\n",
    "$12 + \\lambda (6 x + 18 y - 6)$\n",
    "<br>\n",
    "\n",
    "$\\lambda (x + 3 y) = 6 \\lambda - 12$\n",
    "<br>\n",
    "\n",
    "$x + 3 y=\\frac{\\lambda - 2}{\\lambda} \\rightarrow y = \\frac{2 \\lambda - 1}{6 \\lambda}$\n",
    "\n",
    "Finally, let's rearrange equation 3 and find $\\lambda$\n",
    "<br>\n",
    "\n",
    "$2 x^2 + 6 x y + 9 y^2 - 2 x - 6 y - 24$\n",
    "<br>\n",
    "\n",
    "$(x + 3 y)^2 + x^2 - 2 (x + 3 y) - 24 = 0$\n",
    "$\\frac{\\lambda^2 - 4 \\lambda + 4}{\\lambda^2} + \\frac{9}{4 \\lambda^2} + \\frac{-2 \\lambda + 4}{\\lambda} - 24=0$\n",
    "<br>\n",
    "\n",
    "$4 \\lambda^2 - 16 \\lambda + 16 + 9 - 8 \\lambda^2 + 16 \\lambda - 96 \\lambda^2 = 0$\n",
    "<br>\n",
    "\n",
    "$100 \\lambda^2=25 \\rightarrow \\lambda= \\pm \\frac{1}{2}$\n",
    "<br>\n",
    "\n",
    "$\\lambda=\\frac{1}{2} \\rightarrow x=-3, y=0$\n",
    "<br>\n",
    "\n",
    "$\\lambda=-\\frac{1}{2} \\rightarrow x=3, y=\\frac{2}{3}$\n",
    "<br><br>\n",
    "\n",
    "Therefore\n",
    "<br>\n",
    "\n",
    "$f(-3, 0) = -21$\n",
    "<br>\n",
    "\n",
    "$f(3, \\frac{2}{3}) = 29$\n",
    "<br>\n",
    "\n",
    "So the global minimum is -21 for point $x = -3, y = 0$\n",
    "<br>\n",
    "\n",
    "and the globel maximum is 29 for point $x = 3, y = \\frac{2}{3}$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Means\n",
    "\n",
    "The K-Means objective is given by:\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "### 3.1. Question\n",
    "\n",
    "Show that the following 2 objectives are equivalent to the K-Means objectives:\n",
    "\n",
    "1. As a function of the clusters:\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\} } \\sum_{k = 1}^{K} \\frac{1}{\\left| \\mathcal{D}_{k} \\right|} \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "2. As a function of the centroids:\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Solution\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\} } \\sum_{k = 1}^{K} \\frac{1}{\\left| \\mathcal{D}_{k} \\right|} \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} $$\n",
    "<br>\n",
    "\n",
    "First as we learned in class the optimal centroids $\\mu_k$ for K-means objective function is:\n",
    "<br>\n",
    "\n",
    "$\\boldsymbol{\\mu}_k = \\frac{1}{|\\mathcal{D}_k|}\\sum_{\\boldsymbol{x}_i \\in \\mathcal{D}_k} \\boldsymbol{x}_i$\n",
    "<br>\n",
    "\n",
    "To prove the equivalation we need to calculate $\\mu_k^2$:\n",
    "<br>\n",
    "\n",
    "$\\mu_k^2 = \\|\\boldsymbol{\\mu}_k\\|_2^2$\n",
    "<br>\n",
    "\n",
    "$= \\left\\|\\frac{1}{|\\mathcal{D}_k|}\\sum_{\\boldsymbol{x}_i \\in \\mathcal{D}_k} \\boldsymbol{x}_i\\right\\|_2^2$\n",
    "<br>\n",
    "\n",
    "$= \\left(\\frac{1}{|\\mathcal{D}_k|}\\sum_{\\boldsymbol{x}_i \\in \\mathcal{D}_k} \\boldsymbol{x}_i\\right) \\cdot \\left(\\frac{1}{|\\mathcal{D}_k|}\\sum_{\\boldsymbol{x}_j \\in \\mathcal{D}_k} \\boldsymbol{x}_j\\right)$\n",
    "<br>\n",
    "\n",
    "$= \\frac{1}{|\\mathcal{D}_k|^2} \\sum_{\\boldsymbol{x}_i \\in \\mathcal{D}_k}\\sum_{\\boldsymbol{x}_j \\in \\mathcal{D}_k} \\boldsymbol{x}_j^T \\cdot \\boldsymbol{x}_i = \\frac{1}{|\\mathcal{D}_k|^2} \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\sum_{ \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } <\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}>$\n",
    "<br><br>\n",
    "\n",
    "Next step is find equivalent equation for $\\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2}$\n",
    "<br>\n",
    "\n",
    "$\\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} = \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } \\left( {\\left\\| \\boldsymbol{x}_{i}\\right\\|^2 + \\left\\| \\boldsymbol{x}_{j}\\right\\|^2 - 2 < \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}>} \\right) = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\sum_{ \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } \\left( {\\left\\| \\boldsymbol{x}_{i}\\right\\|^2 + \\left\\| \\boldsymbol{x}_{j}\\right\\|^2 - 2 < \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}>} \\right) = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\sum_{ \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 + \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\sum_{ \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{j}\\right\\|^2 - 2 \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\sum_{ \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } <\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}>$\n",
    "<br>\n",
    "\n",
    "$ = |\\mathcal{D}_k| \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 + |\\mathcal{D}_k| \\sum_{ \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{j}\\right\\|^2 - 2 \\cdot |\\mathcal{D}_k|^2 \\cdot \\|\\boldsymbol{\\mu}_k\\|_2^2 = 2 \\cdot |\\mathcal{D}_k| \\left(\\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 - |\\mathcal{D}_k| \\|\\boldsymbol{\\mu}_k\\|_2^2 \\right)$\n",
    "<br><br>\n",
    "\n",
    "Then we will show an equivalent equation for $\\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2}$\n",
    "<br>\n",
    "\n",
    "$\\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left( \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 + \\left\\| \\boldsymbol{\\mu}_{k}\\right\\|^2 - 2 <\\boldsymbol{x}_{i}, \\boldsymbol{\\mu}_{k}> \\right) = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 + \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{\\mu}_{k}\\right\\|^2 - 2 \\cdot \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } <\\boldsymbol{x}_{i}, \\boldsymbol{\\mu}_{k}> = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 + |\\mathcal{D}_k| \\left\\| \\boldsymbol{\\mu}_{k}\\right\\|^2 - 2 \\cdot {\\mu}_k^T \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\boldsymbol{x}_{i}$\n",
    "<br>\n",
    "\n",
    "$\\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 + |\\mathcal{D}_k| \\left\\| \\boldsymbol{\\mu}_{k}\\right\\|^2 - 2 \\cdot |\\mathcal{D}_k|  {\\mu}_k^T {\\mu}_k = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 + |\\mathcal{D}_k| \\left\\| \\boldsymbol{\\mu}_{k}\\right\\|_2^2 - 2 \\cdot |\\mathcal{D}_k| \\|\\boldsymbol{\\mu}_k\\|_2^2 = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 - |\\mathcal{D}_k| \\left\\| \\boldsymbol{\\mu}_{k}\\right\\|_2^2$\n",
    "<br><br>\n",
    "\n",
    "So we get that\n",
    "<br>\n",
    "\n",
    "$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\left( \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 - |\\mathcal{D}_k| \\left\\| \\boldsymbol{\\mu}_{k}\\right\\|_2^2 \\right)$\n",
    "<br>\n",
    "\n",
    "$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\} } \\sum_{k = 1}^{K} \\frac{1}{\\left| \\mathcal{D}_{k} \\right|} \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} = \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ \\mathcal{\\mu}_{k} \\right\\} } \\sum_{k = 1}^{K} 2 \\cdot \\left( \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 - |\\mathcal{D}_k| \\left\\| \\boldsymbol{\\mu}_{k}\\right\\|_2^2 \\right) = \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ \\mathcal{\\mu}_{k} \\right\\} } \\sum_{k = 1}^{K} \\left( \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\left\\| \\boldsymbol{x}_{i}\\right\\|^2 - |\\mathcal{D}_k| \\left\\| \\boldsymbol{\\mu}_{k}\\right\\|_2^2 \\right)$\n",
    "<br>\n",
    "\n",
    "Therefore both equations are equivalent\n",
    "\n",
    "---\n",
    "\n",
    "$$ \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
    "\n",
    "First we will define new definition:\n",
    "<br>\n",
    "\n",
    "$x_i \\in N(\\boldsymbol{\\mu}_{k}) \\Leftrightarrow \\arg \\min_{ \\left\\{{j} \\right\\}} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{j} \\right\\|}_{2}^{2} = k$\n",
    "<br>\n",
    "\n",
    "Mean that $x_i \\in N(\\boldsymbol{\\mu}_{k})$ if and only if $\\boldsymbol{\\mu}_{k}$ is the closest centroid to $x_i$\n",
    "<br>\n",
    "\n",
    "So:\n",
    "<br>\n",
    "\n",
    "$ \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\sum_{k = 1}^{K} \\boldsymbol{\\Iota} [x_i \\in N(\\boldsymbol{\\mu}_{k})] \\cdot {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2}$\n",
    "<br>\n",
    "\n",
    "$\\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{i = 1}^{N} \\boldsymbol{\\Iota} [x_i \\in N(\\boldsymbol{\\mu}_{k})] \\cdot {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{x_i \\in N(\\boldsymbol{\\mu}_{k})} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2}$\n",
    "<br>\n",
    "<br>\n",
    "we can see that\n",
    "<br>\n",
    "\n",
    "$\\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{x_i \\in N(\\boldsymbol{\\mu}_{k})} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2}$ is equivalent to $ \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Question\n",
    "\n",
    "Prove or disprove: The K-Means algorithm **always** converge to a global minimum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Solution\n",
    "\n",
    "Disprove:\n",
    "<br>\n",
    "K-Means algorithm converge is depends on the centroids initialization.\n",
    "<br>\n",
    "For example, set { 0, 3, 6, 9 } and k=2\n",
    "<br>\n",
    "<br>\n",
    "K-Means initialization is $C_1 = 0$ and $C_2 = 5$\n",
    "<br>\n",
    "So the clusters will be:\n",
    "<br>\n",
    "Cluster1 = { 3, 6, 9 }\n",
    "<br>\n",
    "Cluster2 = { 0 }\n",
    "<br>\n",
    "The algorithm will improve the centers and reach to the local minimum, $C_1 = 0$ and $C_2 = 6$\n",
    "<br>\n",
    "The objective will be equals to $18$\n",
    "<br>\n",
    "the algorithm riched to saddle point and won't improve the centers.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "But if K-Means initialization is $C_1 = 1$ and $C_2 = 6$\n",
    "<br>\n",
    "the algorithm will improve the centers and reach to the global minimum, $C_1 = 1.5$ and $C_2 = 7.5$\n",
    "<br>\n",
    "and the clusters will be:\n",
    "<br>\n",
    "Cluster1 = { 0, 3 }\n",
    "<br>\n",
    "Cluster2 = { 6, 9 }\n",
    "<br>\n",
    "and the objective will be equals to 9\n",
    "<br><br>\n",
    "Therefore K-Means algorithm not always converge to the global minimum\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gaussian Mixture Model\n",
    "\n",
    " * Let $\\underline{X} \\sim \\mathcal{N}_{d} \\left( \\boldsymbol{\\mu}_{x}, {\\Sigma}_{x} \\right)$ be a Gaussian Radom Vector.\n",
    " * Let $Y = {\\boldsymbol{a}}^{T} \\underline{X} + b$ be a random variable.\n",
    "\n",
    "\n",
    "### 4.1. Question\n",
    "\n",
    "Find ${f}_{Y} \\left( y \\right)$, the Probability Density Function (PDF) of $Y$ as a function of $\\boldsymbol{\\mu}_{x}, {\\Sigma}_{x}, \\boldsymbol{a}, b$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Solution\n",
    "\n",
    "To find the PDF of Y, we can use the properties of miltivatiate Gaussian distributions and linear transformations\n",
    "<br>\n",
    "\n",
    "First we will calculate the mean of $Y$\n",
    "<br>\n",
    "\n",
    "${\\mu}_Y = E[Y] = E[a^T X + b] = E[a^T X] + E[b] = E[a^T] \\cdot E[X] + E[b] = a^T \\cdot E[X] + b = a^T \\cdot {\\mu}_X + b$\n",
    "<br>\n",
    "\n",
    "Then we will calculate the variance of $Y$\n",
    "<br>\n",
    "\n",
    "${\\sigma}_{Y}^{2} = E[\\left(Y - E[Y] \\right)^2] = E[\\left(a^T X + b - a^T {\\mu}_X -b \\right)^2] = E[\\left( a^T X - a^T {\\mu}_X \\right)^2] = E[\\left(a^T \\left( X - {\\mu}_X \\right) \\right)^2] = E[a^T \\left( X - {\\mu}_X \\right)^2 a] = a^T E[\\left( X - {\\mu}_X \\right)^2] a = a^T Var(X) a = a^T {\\Sigma}_X a$\n",
    "<br>\n",
    "\n",
    "Thus we can say that $Y$ distribution is\n",
    "<br>\n",
    "\n",
    "$Y \\sim \\mathcal{N} \\left(a^T {\\mu}_X + b, a^T {\\Sigma}_X a \\right)$\n",
    "<br>\n",
    "\n",
    "So, the PDF of Y, denote as $f_Y(y)$, is given by the expression\n",
    "<br>\n",
    "\n",
    "$f_Y(y) = \\frac{1}{\\sqrt{2 \\pi \\cdot a^T {\\Sigma}_X a}} \\cdot e^{- \\frac{\\left( y - a^T {\\mu}_X - b \\right)^2}{2 \\cdot a^T {\\Sigma}_X a}}$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix $A \\in \\mathbb{R}^{d \\times d}$ is called Symmetric Positive Semi Definite (SPSD) if ${A}^{T} = A$ and for any $\\boldsymbol{v} \\in \\mathbb{R}^{d}$:\n",
    "\n",
    "$$ \\boldsymbol{v}^{T} A \\boldsymbol{v} \\geq 0 $$\n",
    "\n",
    "### 4.2. Question\n",
    "\n",
    "Let $\\underline{X}$ be a random vector with covariance matrix ${\\Sigma}_{x}$. Show that ${\\Sigma}_{x}$ is an SPSD matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Sigma_x^T=E\\left[\\left(X X^T\\right)^T\\right]=E\\left[X X^T\\right]=\\Sigma_x \\rightarrow \\Sigma_x$ is symmetric.\n",
    "<br>\n",
    "\n",
    "Let $u_i$ be a non-trivial eigen vector corresponding to $\\lambda_i\\left(\\Sigma_x\\right)$, and let $Z=X^T u_i$\n",
    "<br>\n",
    "\n",
    "$u_i^T \\Sigma_x u_i=\\lambda_i u_i^T u_i=\\lambda_i\\left\\|u_i\\right\\|_2^2 ; u_i^T \\Sigma_x u_i=u_i^T E\\left[X X^T\\right] u_i=E\\left[u_i^T X X^T u_i\\right]=E\\left[Z^2\\right] \\geq 0 \\rightarrow \\lambda_i\\left\\|u_i\\right\\|_2^2 \\geq 0$\n",
    "<br>\n",
    "\n",
    "Therefore, $\\Sigma_x$ is SPSD.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Clustering\n",
    "\n",
    "The _complete linkage distance_ between the 2 clusters $\\mathcal{C}_{1} = {\\left\\{ \\boldsymbol{x}_{i} \\right\\}}_{i = 1}^{{N}_{1}}$ and $\\mathcal{C}_{2} = {\\left\\{ \\boldsymbol{x}_{j} \\right\\}}_{j = 1}^{{N}_{2}}$:\n",
    "\n",
    "$$ {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) = \\begin{cases}\n",
    "0 & \\text{ if } \\mathcal{C}_{1} = \\mathcal{C}_{2} \\\\ \n",
    "\\max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| & \\text{ if } \\mathcal{C}_{1} \\neq \\mathcal{C}_{2}\n",
    "\\end{cases} $$\n",
    "\n",
    "### 5.1. Question\n",
    "\n",
    "Prove that the complete linkage is indeed a metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Solution\n",
    "\n",
    "Metric definition:\n",
    "<br>\n",
    "Symmetric: $\\rho(x, x') = \\rho(x', x)$\n",
    "<br>\n",
    "\n",
    "Positive: $\\rho(x, x') \\geq 0, \\rho(x, x') = 0 \\Leftrightarrow x = x'$\n",
    "<br>\n",
    "\n",
    "Triangle inequality: $\\rho(x, x'') \\le \\rho(x, x') + \\rho(x', x'')$\n",
    "<br><br>\n",
    "\n",
    "Proof:\n",
    "<br><br>\n",
    "Symmetric:\n",
    "<br>\n",
    "From definition\n",
    "<br>\n",
    "\n",
    "${d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) = {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{2}, \\mathcal{C}_{1} \\right)$\n",
    "<br><br>\n",
    "Positive:\n",
    "<br>\n",
    "From definition, and the norm definition\n",
    "<br>\n",
    "\n",
    "$\\forall C_1, C_2 s.t. C_1 \\ne C_2, {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) \\geq 0$\n",
    "<br>\n",
    "\n",
    "$\\forall C, {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}, \\mathcal{C} \\right) = 0$\n",
    "<br><br>\n",
    "Triangle inequality:\n",
    "<br>\n",
    "We will prove that $\\forall C_1, C_2, C_3, \\rho(C_1, C_3) \\le \\rho(C_1, C_2) + \\rho(C_2, C_3)$\n",
    "<br>\n",
    "Denote $x_{C_1}, x_{C_3}$ the $\\max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|$\n",
    "<br>\n",
    "Denote $x_{C_1^*}, x_{C_2^*}$ the $\\max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|$\n",
    "<br>\n",
    "Denote $x_{C_2^{**}}, x_{C_3^{**}}$ the $\\max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{2}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|$\n",
    "<br><br>\n",
    "Therefore\n",
    "<br>\n",
    "\n",
    "$\\rho (C_1, C_3) = ||x_{C_1} - x_{C_3}||$\n",
    "<br>\n",
    "\n",
    "$||x_{C_1} - x_{C_3}|| = ||x_{C_1} - x_{C_2^*} + x_{C_2^*} - x_{C_3}||$\n",
    "<br>\n",
    "By the norm triangle inequality\n",
    "<br>\n",
    "\n",
    "$||x_{C_1} - x_{C_2^*} + x_{C_2^*} - x_{C_3}|| \\leq ||x_{C_1} - x_{C_2^*}|| + ||x_{C_2^*} - x_{C_3}||$\n",
    "<br>\n",
    "\n",
    "By defenition $||x_{C_1} - x_{C_2^*}|| \\leq ||x_{C_1^*} - x_{C_2^*}||$ and $||x_{C_2^*} - x_{C_3}|| \\leq ||x_{C_2^{**}} - x_{C_3^{**}}||$\n",
    "<br>\n",
    "Thus\n",
    "<br>\n",
    "\n",
    "$||x_{C_1} - x_{C_2^*} + x_{C_2^*} - x_{C_3}|| \\leq ||x_{C_1} - x_{C_2^*}|| + ||x_{C_2^*} - x_{C_3}|| \\leq ||x_{C_1^*} - x_{C_2^*}|| + ||x_{C_2^{**}} - x_{C_3^{**}}||$\n",
    "<br>\n",
    "\n",
    "So\n",
    "<br>\n",
    "\n",
    "$\\rho(C_1, C_3) \\leq ||x_{C_1^*} - x_{C_2^*}|| + ||x_{C_2^{**}} - x_{C_3^{**}}|| = \\rho(C_1, C_2) + \\rho(C_2, C_3)$\n",
    "<br><br>\n",
    "Therefore complete linkage indeed a metric\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * The _single linkage dissimilarity_ between the 2 clusters $\\mathcal{C}_{1} = {\\left\\{ \\boldsymbol{x}_{i} \\right\\}}_{i = 1}^{{N}_{1}}$ and $\\mathcal{C}_{2} = {\\left\\{ \\boldsymbol{x}_{j} \\right\\}}_{j = 1}^{{N}_{2}}$:\n",
    "\n",
    "$$ {d}^{2}_{\\text{Single Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| $$\n",
    "\n",
    " * The _Lance Williams_ update rule is given by: ${D}_{\\overline{ij}, k} = {\\alpha}_{i} {D}_{i, k} + {\\alpha}_{j} {D}_{j, k} + \\beta {D}_{i, j} + \\gamma \\left| {D}_{i, k} - {D}_{j, k} \\right|$.\n",
    "\n",
    "### 5.2. Question\n",
    "\n",
    "Consider 3 clusters $\\mathcal{C}_{1}, \\mathcal{C}_{2}, \\mathcal{C}_{3}$ with ${D}_{i, j} = {d}_{\\text{Single Link}} \\left( \\mathcal{C}_{i}, \\mathcal{C}_{j} \\right)$.  \n",
    "Prove that:\n",
    "\n",
    "$$ {D}_{\\overline{12}, 3} = {d}_{\\text{Single Link}} \\left( \\mathcal{C}_{1} \\cup \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) $$\n",
    "\n",
    "In other words, show that the _Lance Williams_ algorithm is correct for the _single linkage dissimilarity_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Solution\n",
    "\n",
    "${d}_{\\text{Single Link}} \\left( \\mathcal{C}_{1} \\cup \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1} ÷ø\\cup \\mathcal{C}_{2}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|$\n",
    "<br>\n",
    "\n",
    "So, there are to cases to satisfy the equation\n",
    "<br>\n",
    "\n",
    "${d}_{\\text{Single Link}} \\left( \\mathcal{C}_{1} \\cup \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1} ÷ø\\cup \\mathcal{C}_{2}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|$\n",
    "<br>\n",
    "or\n",
    "<br>\n",
    "\n",
    "${d}_{\\text{Single Link}} \\left( \\mathcal{C}_{1} \\cup \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1} ÷ø\\cup \\mathcal{C}_{2}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{2}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|$\n",
    "<br>\n",
    "\n",
    "Which mean that the minimum value is given by $\\boldsymbol{x}_{i}$ is from $\\mathcal{C}_{1}$ or $\\mathcal{C}_{2}$.\n",
    "<br><br>\n",
    "\n",
    "W.l.o.g the minimum value that given by $\\boldsymbol{x}_{i}$ is from $\\mathcal{C}_{1}$, so\n",
    "<br>\n",
    "\n",
    "* ${d}_{\\text{Single Link}} \\left( \\mathcal{C}_{1} \\cup \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1} ÷ø\\cup \\mathcal{C}_{2}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| = {D}_{1, 3}$\n",
    "<br>\n",
    "\n",
    "Thus\n",
    "\n",
    "$\\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| \\leq \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{2}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|$\n",
    "<br>\n",
    "\n",
    "Therefore\n",
    "<br>\n",
    "\n",
    "$\\left| {D}_{1, 3} - {D}_{2, 3} \\right| = {D}_{2, 3} - {D}_{1, 3}$\n",
    "<br>\n",
    "\n",
    "Place the coefficients ${\\alpha}_{i} = \\frac{1}{2}$, ${\\alpha}_{j} = \\frac{1}{2}$, $\\beta = 0$, $\\gamma = - \\frac{1}{2}$ in the _Lance Williams_ update rule, and than we get \n",
    "<br>\n",
    "\n",
    "* ${D}_{\\overline{12}, 3} = {\\alpha}_{i} {D}_{1, 3} + {\\alpha}_{j} {D}_{2, 3} + \\beta {D}_{1, 2} + \\gamma \\left| {D}_{1, 3} - {D}_{2, 3} \\right| = \\frac{1}{2} {D}_{1, 3} + \\frac{1}{2} {D}_{2, 3} - \\frac{1}{2} \\left| {D}_{1, 3} - {D}_{2, 3} \\right| = \\frac{1}{2} {D}_{1, 3} + \\frac{1}{2} {D}_{2, 3} - \\frac{1}{2} \\left( {D}_{2, 3} - {D}_{1, 3} \\right) = \\frac{1}{2} {D}_{1, 3} + \\frac{1}{2} {D}_{2, 3} - \\frac{1}{2} {D}_{2, 3} + \\frac{1}{2} {D}_{1, 3} = {D}_{1, 3}$\n",
    "<br>\n",
    "\n",
    "So for both points it shown that\n",
    "<br>\n",
    "\n",
    "${D}_{\\overline{12}, 3} = {d}_{\\text{Single Link}} \\left( \\mathcal{C}_{1} \\cup \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right)$\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
